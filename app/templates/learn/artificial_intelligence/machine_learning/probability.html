Random sampling: each member of a population has equal chance of being selected.

Systematic sampling: we choose one sample every 2nd, 3rd, etc. record. For example going through patient records, we select every third patient.

Stratified sampling: make sure we get enough samples for each category. For example, get a random sample of males and females. It's random but we want to make sure we get enough males and females. In this case male and female are stratum.

Sample variance is the average of squared differences from the mean.

Standard deviation is the measure of the dispersion of a set of data from its mean.

s = \sqrt{\frac{1}{N-1} \sum_{i=1}^N (x_i - \overline{x})^2} .

Entropy measures the impurity or uncertainty present in the data.

Information gain indicates how much information a particular feature/variable gives us about the final outcome.

Confusion matrix is the type I/II error matrix.

Probability density function is equation describing a continuous probability distribution. Has three properties: graph of PDF will be continuous over a range, the area bounded by the curve density function and the x-axis is equal to 1, and the probability that a random variable assumes a value between a & b is equal to the area under the PDF bounded by a & b.

Graph of normal distribution depends on mean and standard deviation.

Central limit theorem: let's say you have statues of heights 1 to 10 ft. If you take a random sample from those statues, let's say you get 1, 6, and 5, so the mean is 4. Keep repeating enough samples, and the means of each sample will eventually make a normal distribution. Why is this important?

Naive Bayes
Imagine we receive wanted emails but also spam.
We make a histogram of all the words that occur in normal messages and calculate the probabilities of seeing each word, given that it was a normal message.
eg. p(Dear|Normal) = 8/17 and p(Dear|Spam) = 0.29
Now let's say we get a new message with Dear Friend and want to determine probability of it being spam
First assume it is a spam message and give it a score
First make a prior probability, which is just # spam/total messages
p(S) x p(Dear|S) x p(Friend|S) = Score if the message is spam = 0.01
Do the same thing for normal
p(N) x p(Dear|N) x p(Friend|N) = 0.09
Since 0.09 > 0.01, we determine this email is a normal message, not spam.
However, if we get a message with a word we don't have in our training data, eg. Lunch, then our calculation for our score is 0. People add 1 count represented by a black box, referred to with alpha
The thing that makes naive bayes so naive is that it treats all word orders the same

Inferential statistics

Point estimation is concerned with the use of sample data to measure a single value which serves as an approximate value or best estimate of an unknown population parameter.

Method of moments: estimates are found out by equating the first k sample moments to the corresponding k population moments

Maximum of likelihood: uessa  model and the values in the model to maximize a likelihood function

Bayes' estimators: minimizes the average risk

Interval estimate: an interval or range of values used to estimate a population parameter

Margin of error: greatest possible distance between the point estimate and the value of the parameter it is estimating

R-square = coefficient of determination = coefficient of multiple determination

Random forest
Problem with decision tree is that it is inaccurate. They work great with data used to create them but are not flexible for classifying new samples.
First make a bootstrapped data set, which is just sampling from the initial set.
Then make a decision tree but only use a random subset of variables or columns at each step.
To make the root and subsequent nodes of the decision tree, select two random variables as candidates (can change number of variables per step to create a more accurate forest)
Now go back and make a new tree for multiple bootstrapped datasets
This variety makes it more effective than decision trees
Once we get new data, we run that data through all the trees and keep track of how many yeses and nos, and then pick the one with the most votes
Typically, about 1/3 of original data does not end up in the bootstrapped dataset, called the out-of-bag dataset
We can use the out-of-bag data to test if the tree works
Proportion of out-of-bag samples that were incorrectly classified is out-of-bag error

K-nearest neighbors algorithm
We use PCA to graph a group of cells that we know the specifics of (which ones are stem cells, which ones are not aka training data)
Then we have a new cell, see which cells it is closest to
K=1 means using the ONE nearest neighbor

Principal component analysis (PCA) using singular value decomposition (SVD)
Say you have 6 mice each with 2 genes at different levels
x-axis becomes gene 1, y-axis becomes gene 2, graph the 6 mice on the x-y graph
Calculate the center and shift the graph to make the center of the graph (0,0)
Now fit a line. Draw a line that goes through the origin, and then rotate the line until it fits the data best
PCA chooses the best fit by maximizing the sum of the squared distances from the projected points to the origin
This line is called PC1
The ratio of x to y of the line tells you how important x is versus y in describing how the data are spread out
Calculate/scale to 1 unit long vector, called the singular vector or the eigenvector for PC1
PC2 is simply the line perpendicular to PC1
To draw the final PCA plot, just rotate so that PC1 is horizontal
Calculate variation for PC1 with SS(distances for PC1)/(n-1)
For 3 principal components, you find PC2 by finding next best fitting line that goes through origin and is perpendicular to PC1
Continue finding principal components by finding best fitting lines that are perpendicular to the other PCs
Scree plot plots the variation as a bar chart
To convert 3D graph into 2D PCA graph, just strip away everything except for PC1 and PC2

Support vector machine (SVM) is a supervised classification method that separates data using hyperplanes
