{% extends 'base.html' %}

{% block title %}Computer science - {% endblock %}

{% block content %}
<style>
  .first {
    font-size: 30px;
    margin: 30px 0 15px;
    color: #333333;
  }

  .second {
    font-size: 20px;
    margin: 20px 0 10px;
    text-decoration: underline;
    color: #333333;
  }

  .third {
    font-size: 100%;
    color: #333333;
    font-weight: 600;
  }

  .size {
    font-size: 130%;
  }

  p,
  button {
    font-size: 100%;
  }

  #integer1_remove {
    display: none;
  }

  iframe {
    display: block;
    margin: 0 auto;
  }

  .sidebar {
    position: fixed;
    margin-left: -375px;
    border-left: 8px solid #c8dce3;
    background-color: #fff;
    overflow-y: scroll;
  }

  .sidebar {
    width: 350px;
    height: 85vh;
  }

  .core {
    padding: 7px;
    width: 100%;
    background-color: beige;
    font-size: larger;
    margin-top: 15px;
    margin-bottom: 10px;
  }

  .core:first-child {
    margin-top: 0px;
  }

  .topic {
    padding-left: 15px;
  }

  .topic a {
    color: #333333;
  }

  .topic a:hover {
    text-decoration: underline;
  }

  a.anchor {
    display: block;
    position: relative;
    top: -150px;
    visibility: hidden;
  }

  .sidebar::-webkit-scrollbar {
    width: 8px;
    /* width of the entire scrollbar */
  }

  .sidebar::-webkit-scrollbar-track {
    background: beige;
    /* color of the tracking area */
  }

  .sidebar::-webkit-scrollbar-thumb {
    background-color: #c8dce3;
    /* color of the scroll thumb */
    /* roundness of the scroll thumb */
    border: 3px solid #c8dce3;
    /* creates padding around scroll thumb */
  }
</style>

<div class="sidebar">
  <div class="core">Software developmental fundamentals</div>
  <div class="topic"><a href="#algorithm_design">Algorithms and design</a></div>
  <div class="topic"><a href="#fundamental_programming">Fundamental programming concepts</a></div>
  <div class="topic"><a href="#fundamental_data">Fundamental data structures</a></div>
  <div class="topic"><a href="#development_methods">Development methods</a></div>
  <div class="core">Discrete structures</div>
  <div class="topic"><a href="#sets">Sets, relations, and functions</a></div>
  <div class="topic"><a href="#sets">Basic logic</a></div>
  <div class="topic"><a href="#sets">Proof techniques</a></div>
  <div class="topic"><a href="#sets">Basics of counting</a></div>
  <div class="topic"><a href="#sets">Graphs and trees</a></div>
  <div class="topic"><a href="#sets">Discrete probability</a></div>
  <div class="core">Algorithms and complexity</div>
  <div class="topic"><a href="#sets">Basic analysis</a></div>
  <div class="topic"><a href="#sets">Algorithmic strategies</a></div>
  <div class="topic"><a href="#sets">Fundamental data structures and algorithms</a></div>
  <div class="topic"><a href="#sets">Basic automata, computability and complexity</a></div>
  <div class="topic"><a href="#sets">Advanced computational complexity</a></div>
  <div class="topic"><a href="#sets">Advanced automata theory and computability</a></div>
  <div class="topic"><a href="#sets">Advanced data structures, algorithms, and analysis</a></div>
  <div class="core">Software engineering</div>
  <div class="topic"><a href="#software_requirements">Software requirements</a></div>
  <div class="topic"><a href="#software_design">Software design</a></div>
  <div class="topic"><a href="#software_construction">Software construction</a></div>
  <div class="topic"><a href="#software_testing">Software testing</a></div>
  <div class="topic"><a href="#software_maintenance">Software maintenance</a></div>
  <div class="topic"><a href="#software_configuration_management">Software configuration management</a></div>
  <div class="topic"><a href="#software_engineering_management">Software engineering management</a></div>
  <div class="topic"><a href="#software_engineering_process">Software engineering process</a></div>
  <div class="topic"><a href="#software_engineering_models_and_methods">Software engineering models and methods</a>
  </div>
  <div class="topic"><a href="#software_quality">Software quality</a></div>
  <div class="topic"><a href="#software_engineering_professional_practice">Software engineering professional
      practice</a>
  </div>
  <div class="topic"><a href="#software_engineering_economics">Software engineering economics</a></div>
  <div class="topic"><a href="#computing_foundations">Computing foundations</a></div>
  <div class="topic"><a href="#mathematical_foundations">Mathematical foundations</a></div>
  <div class="topic"><a href="#engineering_foundations">Engineering foundations</a></div>
  <div class="core">Systems fundamentals</div>
  <div class="topic"><a href="#sets">Computational paradigms</a></div>
  <div class="topic"><a href="#sets">Cross-layer communications</a></div>
  <div class="topic"><a href="#sets">State and state machines</a></div>
  <div class="topic"><a href="#sets">Parallelism</a></div>
  <div class="topic"><a href="#sets">Evaluation</a></div>
  <div class="topic"><a href="#sets">Resource allocation and scheduling</a></div>
  <div class="topic"><a href="#sets">Proximity</a></div>
  <div class="topic"><a href="#sets">Virtualization and isolation</a></div>
  <div class="topic"><a href="#sets">Reliability through redundancy</a></div>
  <div class="topic"><a href="#sets">Quantitative evaluation</a></div>
  <div class="core">Architecture and organization</div>
  <div class="topic"><a href="#sets">Digital logic and digital systems</a></div>
  <div class="topic"><a href="#sets">Machine level representation of data</a></div>
  <div class="topic"><a href="#sets">Assembly level machine organization</a></div>
  <div class="topic"><a href="#sets">Memory system organization and architecture</a></div>
  <div class="topic"><a href="#sets">Interfacing and communication</a></div>
  <div class="topic"><a href="#sets">Functional organization</a></div>
  <div class="topic"><a href="#sets">Multiprocessing and alternative architectures</a></div>
  <div class="topic"><a href="#sets">Performance enhancements</a></div>
  <div class="core">Computational science</div>
  <div class="topic"><a href="#sets">Introduction to modeling and simulation</a></div>
  <div class="topic"><a href="#sets">Modeling and simulation</a></div>
  <div class="topic"><a href="#sets">Processing</a></div>
  <div class="topic"><a href="#sets">Interactive visualization</a></div>
  <div class="topic"><a href="#sets">Data, information, and knowledge</a></div>
  <div class="topic"><a href="#sets">Numerical analysis</a></div>
  <div class="core">Graphics and visualization</div>
  <div class="topic"><a href="#sets">Fundamental concepts</a></div>
  <div class="topic"><a href="#sets">Basic rendering</a></div>
  <div class="topic"><a href="#sets">Geometric modeling</a></div>
  <div class="topic"><a href="#sets">Advanced rendering</a></div>
  <div class="topic"><a href="#sets">Computer animation</a></div>
  <div class="topic"><a href="#sets">Visualization</a></div>
  <div class="core">Human-computer interaction</div>
  <div class="topic"><a href="#sets">Foundations</a></div>
  <div class="topic"><a href="#sets">Designing interaction</a></div>
  <div class="topic"><a href="#sets">Programming interactive systems</a></div>
  <div class="topic"><a href="#sets">User-centered design & testing</a></div>
  <div class="topic"><a href="#sets">New interactive technologies</a></div>
  <div class="topic"><a href="#sets">Collaboration & communication</a></div>
  <div class="topic"><a href="#sets">Statistical methods for HCI</a></div>
  <div class="topic"><a href="#sets">Human factors & security</a></div>
  <div class="topic"><a href="#sets">Design-oriented HCI</a></div>
  <div class="topic"><a href="#sets">Mixed, augmented and virtual reality</a></div>
  <div class="core">Information assurance and security</div>
  <div class="topic"><a href="#sets">Foundational concepts in security</a></div>
  <div class="topic"><a href="#sets">Principles of secure design</a></div>
  <div class="topic"><a href="#sets">Defensive programming</a></div>
  <div class="topic"><a href="#sets">Threats and attacks</a></div>
  <div class="topic"><a href="#sets">Network security</a></div>
  <div class="topic"><a href="#sets">Cryptography</a></div>
  <div class="topic"><a href="#sets">Web security</a></div>
  <div class="topic"><a href="#sets">Platform security</a></div>
  <div class="topic"><a href="#sets">Security policy and governance</a></div>
  <div class="topic"><a href="#sets">Digital forensics</a></div>
  <div class="topic"><a href="#sets">Secure software engineering</a></div>
  <div class="core">Information management</div>
  <div class="topic"><a href="#sets">Information management concepts</a></div>
  <div class="topic"><a href="#sets">Database systems</a></div>
  <div class="topic"><a href="#sets">Data modeling</a></div>
  <div class="topic"><a href="#sets">Indexing</a></div>
  <div class="topic"><a href="#sets">Relational databases</a></div>
  <div class="topic"><a href="#sets">Query languages</a></div>
  <div class="topic"><a href="#sets">Transaction processing</a></div>
  <div class="topic"><a href="#sets">Distributed databases</a></div>
  <div class="topic"><a href="#sets">Physical database design</a></div>
  <div class="topic"><a href="#sets">Data mining</a></div>
  <div class="topic"><a href="#sets">Information storage and retrieval</a></div>
  <div class="topic"><a href="#sets">Multimedia systems</a></div>
  <div class="core">Intelligent systems</div>
  <div class="topic"><a href="#sets">Fundamental issues</a></div>
  <div class="topic"><a href="#sets">Basic search strategies</a></div>
  <div class="topic"><a href="#sets">Basic knowledge representation and reasoning</a></div>
  <div class="topic"><a href="#sets">Basic machine learning</a></div>
  <div class="topic"><a href="#sets">Advanced search</a></div>
  <div class="topic"><a href="#sets">Advanced representation and reasoning</a></div>
  <div class="topic"><a href="#sets">Reasoning under uncertainty</a></div>
  <div class="topic"><a href="#sets">Agents</a></div>
  <div class="topic"><a href="#sets">Natural language processing</a></div>
  <div class="topic"><a href="#sets">Advanced machine learning</a></div>
  <div class="topic"><a href="#sets">Robotics</a></div>
  <div class="topic"><a href="#sets">Perception and computer vision</a></div>
  <div class="core">Networking and communication</div>
  <div class="topic"><a href="#sets">Introduction</a></div>
  <div class="topic"><a href="#sets">Networked applications</a></div>
  <div class="topic"><a href="#sets">Reliable data delivery</a></div>
  <div class="topic"><a href="#sets">Routing and forwarding</a></div>
  <div class="topic"><a href="#sets">Local area networks</a></div>
  <div class="topic"><a href="#sets">Resource allocation</a></div>
  <div class="topic"><a href="#sets">Mobility</a></div>
  <div class="topic"><a href="#sets">Social networking</a></div>
  <div class="core">Operating systems</div>
  <div class="topic"><a href="#overview_operating">Overview of operating systems</a></div>
  <div class="topic"><a href="#sets">Operating system principles</a></div>
  <div class="topic"><a href="#sets">Concurrency</a></div>
  <div class="topic"><a href="#sets">Scheduling and dispatch</a></div>
  <div class="topic"><a href="#sets">Memory management</a></div>
  <div class="topic"><a href="#sets">Security and protection</a></div>
  <div class="topic"><a href="#sets">Virtual machines</a></div>
  <div class="topic"><a href="#sets">Device management</a></div>
  <div class="topic"><a href="#sets">File systems</a></div>
  <div class="topic"><a href="#sets">Real time and embedded systems</a></div>
  <div class="topic"><a href="#sets">Fault tolerance</a></div>
  <div class="topic"><a href="#sets">System performance evaluation</a></div>
  <div class="core">Platform-based development</div>
  <div class="topic"><a href="#sets">Introduction</a></div>
  <div class="topic"><a href="#sets">Web platforms</a></div>
  <div class="topic"><a href="#sets">Mobile platforms</a></div>
  <div class="topic"><a href="#sets">Industrial platforms</a></div>
  <div class="topic"><a href="#sets">Game platforms</a></div>
  <div class="core">Parallel and distributed computing</div>
  <div class="topic"><a href="#sets">Parallelism fundamentals</a></div>
  <div class="topic"><a href="#sets">Parallel decomposition</a></div>
  <div class="topic"><a href="#sets">Communciation and coordination</a></div>
  <div class="topic"><a href="#sets">Parallel algorithm, analysis, and programming</a></div>
  <div class="topic"><a href="#sets">Parallel architecture</a></div>
  <div class="topic"><a href="#sets">Parallel performance</a></div>
  <div class="topic"><a href="#sets">Distributed systems</a></div>
  <div class="topic"><a href="#sets">Cloud computing</a></div>
  <div class="topic"><a href="#sets">Formal models and semantics</a></div>
  <div class="core">Programming languages</div>
  <div class="topic"><a href="#sets">Object-oriented programming</a></div>
  <div class="topic"><a href="#sets">Functional programming</a></div>
  <div class="topic"><a href="#sets">Event-driven and reactive programming</a></div>
  <div class="topic"><a href="#sets">Basic type systems</a></div>
  <div class="topic"><a href="#sets">Program representation</a></div>
  <div class="topic"><a href="#sets">Language translation and execution</a></div>
  <div class="topic"><a href="#sets">Syntax analysis</a></div>
  <div class="topic"><a href="#sets">Compiler semantic analysis</a></div>
  <div class="topic"><a href="#sets">Code generation</a></div>
  <div class="topic"><a href="#sets">Runtime systems</a></div>
  <div class="topic"><a href="#sets">Static analysis</a></div>
  <div class="topic"><a href="#sets">Advanced programming constructs</a></div>
  <div class="topic"><a href="#sets">Concurrency and parallelism</a></div>
  <div class="topic"><a href="#sets">Type systems</a></div>
  <div class="topic"><a href="#sets">Formal semantics</a></div>
  <div class="topic"><a href="#sets">Language pragmatics</a></div>
  <div class="topic"><a href="#sets">Logic programming</a></div>
  <div class="core">Social issues and professional practice</div>
  <div class="topic"><a href="#sets">Social context</a></div>
  <div class="topic"><a href="#sets">Analytical tools</a></div>
  <div class="topic"><a href="#sets">Professional ethics</a></div>
  <div class="topic"><a href="#sets">Intellectual property</a></div>
  <div class="topic"><a href="#sets">Privacy and civil liberties</a></div>
  <div class="topic"><a href="#sets">Professional communication</a></div>
  <div class="topic"><a href="#sets">Sustainability</a></div>
  <div class="topic"><a href="#sets">History</a></div>
  <div class="topic"><a href="#sets">Economies of computing</a></div>
  <div class="topic"><a href="#sets">Security policies, laws and computer crimes</a></div>
</div>
SWEBOK v3
<div class="first">Software requirements</div><a class="anchor" id="software_requirements"></a>
<div class="second">Software requirements fundamentals</div>
<p>Software engineering is the application of a systematic, disciplined, quantifiable approach to the development,
  operation, and maintenance of software; that is, the application of engineering to software.</p>
<div class="third">Definition of a software requirement</div>
<p>At its most basic, a software requirement is a property that must be exhibited by something in order to solve some
  problem in the real world. An essential property of all software requirements it that they be verifiable as an
  individual feature as a functional requirement or at the system level as a nonfunctional requirement. Requirements
  have other attributes, such as priority ratings to enable tradeoffs in face of finite resources and a status value to
  enable project progress to be monitored. Software projects are critically vulnerable when the requirements-related
  activities are poorly performed. Software
  requirements express the needs and constraints placed on a software product that contribute to the solution of some
  real-world problem. The term "requirements engineering" denotes the systematic handling of requirements.</p>
<div class="third">Product and process requirements</div>
<p>A product requirement is a need or constraint on the software to be developed. A process requirement is essentially a
  constraint on the developt of the software.</p>
<div class="third">Functional and nonfunctional requirements</div>
<p>Functional requirements describe the functions that the software is to execute, also known as capabilities or
  features. A functional requirement can also be described as one for which a finite set of test steps can be written to
  validate its behavior. Nonfunctional requirements are ones that act to constrain the solution, also known as
  constraints or quality requirements. Examples of nonfunctional requirements are performance requirements,
  maintainability requirements, safety, reliability, security, interoperability, etc.</p>
<div class="third">Emergent properties</div>
<p>Emergent properties are requirements that cannot be addressed by a single component but that depend on how all the
  software components interoperate. Emergent properties are crucially dependent on system architecture.</p>
<div class="third">Quantifiable requirements</div>
<p>Software requirements should be stated as clearly and as unambiguously as possible, and, where appropriate,
  quantitatively. This is particularly important for nonfunctional requirements.</p>
<div class="third">System requirements and software requirements</div>
<p>A "system" means an interacting combination of elements to accomplish a defined objective, including hardware,
  software, firmware, people, information, techniques, facilities, services, and other support elements. System
  requirements are the requirements for the system as a whole. Software requirements are derived from system
  requirements. User requirements are the requirements of the system's customers or end users.</p>
<div class="second">Requirements process</div>
<div class="third">Process models</div>
<p>The requirements process is not a discrete front-end activity of the software life cycle, but rather a process
  initiated at the beginning of a project that continues to be refined throughout the life cycle. Software requirements
  are configuration items and are managed using the same software configuratoin management practices as other products
  of the softawre life cycle processes. Requirements need to be adapted to the organizations and project context.</p>
<div class="third">Process actors</div>
<p>The requirements process is fundamentally interdisciplinary, and the requirements specialist needs to mediate between
  the domain of the stakeholder and that of software engineering. THe stakeholders will vary across projects, but will
  always include users/operators and customers. Examples of software stakeholders include users (those who use the
  software), customers (those who have commissioned the software), market analysts (marketing people who establish what
  the market needs are and who act as proxy customers), regulators (regulatory authorities such as for applications in
  banking and public transport), and software engineers (constraints can have major impact on project cost or delivery
  if they fit poorly with the skillset of the engineers). It is the job of the softawre engineer to negotiate tradeoffs
  acceptable to principal stakeholders within budgetary, technical, regulatory, and other constraints. A prerequisite
  for this is that all the stakeholders be identified, the nature of their "stake" analyzed, and their requirements
  elicited.</p>
<div class="third">Process support and management</div>
<div class="third">Process quality and improvement</div>
<div class="second">Requirements elictation</div>
<p>Requirements elictation, or requirements capture, or requirements discovery, or requirements acquisition is concerned
  with the origins of software requirements and how the software engineer can
  collect them. One of the fundamental principles of a good requirements elictation process is that of effective
  communication between various stakeholders, continuing through the entire Software Development Life Cycle (SDLC)
  process with different stakeholders at different points in time. A critical element of requirements elictation is
  informing the project scope, which involves providing a description of the software being specified and its purpose
  and prioritizing the deliverables to ensure the customer's most important business needs are satisfied first.</p>
<div class="third">Requirements sources</div>
<p>Goals: the term goal or business concern or critical success factor refers to the overall, high-level objectives of
  the software. Goals provide the motivation for the software but are often vaguely formulated. A feasibility study is a
  relatively low-cost way of assessing value and cost of goals.</p>
<p>Domain knowledge: the software engineer needs to acquire or have available knowledge about the application domain.
  Domain knowledge provides the background against which all elicited requirements knowledge must be set in ordre to
  understand it.</p>
<p>Stakeholders: the software engineer needs to identify, represent, and manage the "viewpoints" of many different types
  of stakeholders.</p>
<p>Business rules: there are statements that define or constrain some aspect of the structure or the behavior of the
  business itself.</p>
<p>The operational environment: requirements will be derived fro mthe environment in which the software will be executed
  (eg. real-time software or performance constraints in a business environment).</p>
<p>The organizational environment: software is often required to support a business process. In general, new software
  should not force unplanned change on the business process.</p>
<div class="third">Elictation techniques</div>
<p>Software engineer must be sensitized to the fact that users may have difficulty describing their tasks, may leave
  important information unstated, or may be unwilling or unable to cooperate. Elicitation is not a passive activity. A
  number of techniques exist:</p>
<p>Inteviews: interviewing stakeholders is a traditional means of eliciting requirements.</p>
<p>Scenarios: scenarios provide context. The most common type of secnario is the use case description.</p>
<p>Prototypes: helps clarify ambiguous requirements. Act similar to scenarios. There is a wide range of prototyping
  techniques, from paper mockups of screen designs to beta-test versions of software products. Low fidelity prototypes
  are often preferred to avoid stakeholder "anchoring" on minor, incidental characteristics of a higher quality
  prorotype.</p>
<p>Facilitated meetings: acheive a summative effect, whereby a group of people can bring more insight into their
  software requirements than by working individually. Allow conflicting requirements surface early.</p>
<p>Observation: observational techniques such as ethnography (study of cultures) can be expensive but also instructive.
</p>
<p>User stories: commonly used in adaptive methods and refers to short, high-level descriptions of required
  functionality expressed in customer terms.</p>
<div class="second">Requirements analysis</div>
<div class="third">Requirements classification</div>
<p>Is the requirement function or nonfunctional? Is the requirement derived from one or more high-level requirements or
  an emergent property, or is being imposed directly on the software by a stakeholder or some other source? Is the
  requirement on the product or the process? What's the priority of the requirement? What's the scope of the requirement
  (extent to which a requirmeent affects the software and its components)? What's the volatility/stability of the
  requirement?</p>
<div class="third">Conceptual modeling</div>
<p>Models aid in understanding the situation in which the problem occurs, as well as helps depict a solution. Many
  modeling notations are part of the United Modeling Language (UML). The factors that influence the choice of modeling
  notation includes 1) the nature of the problem, 2) the expertise of the softawre engineer, 3) the process
  requirements.</p>
<div class="third">Architectural design and requirements allocation</div>
<p>Architectural design is the point at which the requirements process overlaps with software or systems design.</p>
<div class="third">Requirements negotation</div>
<p>Another term commonly used for this is conflict resolution. In most cases it is unwise for the software engineer to
  make a unilateral decision. It is often important for contractual reasons that such decisions be traeable back to the
  customer. Requirements prioritization is necessary, not only as a means to filter important requirements, but also in
  order to resolve conflicts and plan for staged deliveries, which means making complex decisions that require detailed
  domain knowledge and good estimation skills. In practice, software engineers perform requirement prioritization
  frequently without knowing about all the requirements.</p>
<div class="third">Formal analysis</div>
<p>Formal analysis has made an impact on some application domains, especially high-integrity systems. The formal
  expression of requirements requires a language with formally defined semantics. Formal analysis allows requirements to
  be expressed precisely and unambiguously, and allow them to be reasoned over, permitting desired properties of the
  specified software to be proven. Formal reasoning requires tool support, which fall into theorem provers or model
  checkers. In neither case can proof be full yautomated, and the level of competence in formal reasoning in order to
  use the tools restricts the wider application of formal analysis. It is generally counterproductive to apply
  formalization until the business goals and user requirements have come into sharp focus. Once the requirements have
  been stabilized and have been elaborated to specify concrete properties of the software, it may be beneficial to
  formalize at least the critical requirements.</p>
<div class="second">Requirements specification</div>
<p>The term specification refers to the assignment of numerical values or limits to a product's design goals. In
  software engineering, this typically refers to the production of a document that can be systematically reviewed,
  evaluated, and approved. For complex systems, this may include system definition, system requirements, and software
  requirements.</p>
<div class="third">System definition document</div>
<p>Also known as the user requirements document or concept of operations document, the system definition document
  records the system requirements. It defines the high-level system requirements from the domain perspective. Its
  readership includes representatives of the system users/customers (eg. marketing for market-driven software). The
  document lists the system requirements along with background information about the overall objectives for the system,
  its target environment, and a statement of the constraints, assumptions, and nonfunctional requirements. It may
  include conceptual models designed to illustrate the system context, usage scenarios, and the principal domain
  entities, as well as workflows.</p>
<div class="third">System requirements specification</div>
<p>System requirements specification is a systems engineering activity and falls outside the scope of software
  engineering.</p>
<div class="third">Software requirements specification</div>
<p>Software requirements specification establishes the basis for agreement between customers and contractors or
  suppliers on what the software product is to do as well as what it is not expected to do. Software requirements
  specification permits a rigorous assessment of requirements before design can begin and reduces later redesign. It
  also provides a realistic basis for estimating product costs, risks, and schedules. Software requirements are often
  written in natural language, but may be supplemented by formal or semiformal descriptions. The general rule is that
  notations should be used that allow the requirements to be described as precisely as possible.</p>
<p>Quality indicators for individual software requirements specification statements include imperatives, directives,
  weak phrases, options, and continuances. Indicators for the entire software requirements specification document
  include size, readability, specification, depth, and text structure.</p>
<div class="second">Requirements validation</div>
<p>The requirements documents may be subject to validation and vertification procedures. This ensures that the software
  engineer has understood the requirements and veritifes that a requirements document conforms to company standards and
  that it is understandable, consistent, and complete.</p>
<p>Formal notations offer the important advantage of these properties to be proven (in a restricted sense, at least).
  Different stakeholders should review the documents. Requirements documents are subject to the same configuration
  management practices as the other dleiverables of the software life cycle processes. Requirements validation is
  concerned with the process of examining the requirements document to ensure that it defines the right software.</p>
<div class="third">Requirements reviews</div>
<p>Perhaps the most common means of validation is by inspection or reviews of the requirements documents. A group of
  reviewers is assigned a brief to look for errors, mistaken assumptions, lack of clarity, and deviation from standard
  practice.</p>
<div class="third">Prototyping</div>
<p>Prototyping is commonly a means for validating the software engineer's interpretation of the software requirements,
  as well as for eliciting new requirements. The advantage of prototypes is they can make it easier to interpret the
  software engineer's assumptions and give useful feedback on why they are wrong. Disadvantages to prototyping include
  distraction of users' attention from core underlying functionaly towards quality or cosmetic problems. For this
  reason, some advocate prototypes that avoid software, such as flip-chart-based mockups. Prototypes may be costly to
  develop, but can avoid the wastage of resources caused by trying to satisfy erroneous requirements.</p>
<div class="third">Model validation</div>
<p>It is typically necessary to validate the quality of the models developed during analysis.</p>
<div class="third">Acceptance tests</div>
<p>An essential property of a software requirement is that it should be possible to validate that the finished product
  satisfies it. Requirements that cannot be validated are really just "wishes." An important task is therefore planning
  how to verify each requirement. In most cases, designing acceptance tests does this for how end-users typically
  conduct business using the system. Identifying and designing acceptance tests may be difficult for nonfunctional
  requirements. To be validated, they must first be analyzed and decomposed to the point where they can be expressed
  quantitatively.</p>
<div class="second">Practical considerations</div>
<p>The requirements process spans the whole software life cycle. Change management and the maintenance of the
  requirements in a state that accurately mirrors the software to be built, or that has been built, are key to the
  success of the software engineering process. Not every organization has a culture of documenting and managing
  requirements (common in dynamic start-up companies). Most often, however, as these companies expand, as their customer
  base grows, they discover they need to recover the requirements that motivated product features in order to assess the
  impact of proposed changes. Requirements documentation and change management are key.</p>
<div class="third">Iterative nature of the requirements process</div>
<p>There is general pressure in the softare industry for ever shorter development cycles, which is particularly
  pronounced in highly competitive, market-driven sectors. Most projects are constrained in some way by their
  environment, and many are upgrades to, or revisions of, existing software where the architecture is given. In
  practice, therefore, it is almost always impractical to implement the requirements process as a linear, deterministic
  process in which software requirements are elicited from the stakeholders, baselined, allocated, and handed over to
  the software development team. It is certainly a myth that the requirements for large software projects are ever
  perfectly understood or perfectly specified.</p>
<p>For software products that develop iteratively, a project team may baseline only those requirements needed for the
  current iteration. The requirements specialist can continue to develop requirements for future iterations, while
  developers proceed with design and construction of the current iteration. This approach provides customers with
  business value quickly while minimizing the cost of rework. Perhaps the most crucial point in understanding software
  requirements is that a significant proportion of the requirements WILL change. This can be due to errors in the
  analysis or change in the environment, eg. customer's operating or business environment, regulatory processes imposed
  by authorities, or market into which software must sell. Change has to be managed by ensuring that proposed changes go
  thorugh a defined review and approval process and by applying careful requirements tracing, impact analysis, and
  software configuration management.</p>
<div class="third">Change management</div>
<div class="third">Requirements attributes</div>
<p>Requirements should consist not only of a specification of what is required, but also of ancillary information, which
  helps manage and interpret the requirements. Requirements attributes must be defined, recorded, and updated as the
  software under development or maintenance evolves. This should include the various classification dimensions of the
  requirement and the verification method or relevant acceptance test plan section It may also include additional
  information, such as a summary rationale for each requirement, the source of each requirement, and a change history.
  The most important requirements attribute is an identifier that allows the requirements to be uniquely and
  unambiguously identifeid.</p>
<div class="third">Requirements tracing</div>
<p>Requirements tracing is concerned with recovering the source of requirements and predicting the effects of
  requirements. Tracing is fundamental to performing impact analysis when requirements change. A requirement should be
  traceable backward to the reuqirements and stakeholders that motivated it. Conversely, a requirement should be
  traceable forward into the reuqirements and design entities that satisfy it (eg. on the code modules tha timplement it
  or the test cases related to that code or even the given section on the user manual which describes the actual
  functionality).</p>
<p>The requirements tracing for a typical project will form a complex directed acyclic graph (DAG). Maintaining an
  up-to-date graph or traceability matrix is an activity that must be considered during the whole life cycle of a
  product.</p>
<div class="third">Measuring requirements</div>
<p>Functionalize size measurement (FSM) is a technique for evaluating the size of a body of functional requirements.</p>
<div class="second">Software requirements tools</div>
<p>Tools for dealing with software requirements fall into tools for modeling and tools for managing requirements.
  Requirements management tools usually support documentation, tracing, and change management. Many organizations have
  invested in requirements management tools, although many more manage their requirements in more ad hoc and generally
  less satisfactory ways (eg. using spreadsheets).</p>
<div class="first">Software design</div><a class="anchor" id="software_design"></a>
<p>Design is defined as both "the process of defining the architecture, components, interfaces, and other
  characteristics of a system or component" and "the result of that process." Viewed as a process, software design is
  the software engineering life cycle activity in which software requirements are analyzed in order to produce a
  description of the software's internal structure that will serve as the basis for its construction. A software design
  describes the software architecture-that is, how the softawre is decomposed and organized into components-and the
  interfaces between those components. It also describes the components at a level of detail that enables their
  construction.</p>
<div class="second">Software design fundamentals</div>
<div class="third">General design concepts</div>
<p>Design can be viewed as a form of problem solving. The concept of a wicked problem-a problem with no definitive
  solution- is interesting in understanding the limits of design. Goals, constraints, alternatives, representations, and
  solutions are of interest in understanding design in its general sense.</p>
<div class="third">Context of software design</div>
<div class="third">Software design process</div>
<p>Software design is generally a two step process: architectural design (high-level or top-level design) that describes
  how software is organized into components, and detailed design that describes the desired behavior of these
  components. The output of these two processes is a set of models and artifacts that record the major decisions that
  have been taken, along with an explanation of the rationale for each nontrivial decision.</p>
<div class="third">Software design principles</div>
<p>A principle is a comprehensive and fundamental law, doctrine, or assumption. Software design principles include
  abstraction; coupling and cohesion; decomposition and modularization; encapsulation/information hiding; separation of
  interface and implementation; sufficiency, completeness, and primitiveness; and separation of concerns.</p>
<p>Abstraction is a view of an object that focuses on the information relevant to a particular purpose and ignores the
  remainder of the information. Two key abstraction mechanisms are parametrization and specification. Parametrization
  abstracts from the details of data representations by representing the data as named parameters. Abstraction by
  specification leads to procedural abstraction, data abstraction, and control (iteration) abstraction.</p>
<p>Coupling is defined as a measure of the interdependence among modules in a computer program and cohesion is the
  measure of the strength of association of the elements within a module.</p>
<p>Decomposing and modularizing means that large software is divided into a number of smaller named components having
  well-defined interfaces that describe component interactions. Usually the goal is to place different functionalities
  and responsibilities in different components.</p>
<p>Encapsulation and information hiding is grouping and packaging the internal details of an abstraction and making
  those details inaccessible to external entities.
</p>
<p>The separation of interface and implementation defines a component by specifying a public interface (known to
  clients) that is separate from the details of how the
  component is realized.</p>
<p>Sufficiency and completeness ensures that a software component captures all the important characteristics of an
  abstraction and nothing more.
  Primitiveness means the design should be based on patterns that are easy to implement.</p>
<p>A concern is an "area of interest with respect to a software design." A design concern is an area of design that is
  relevant to one or more of its stakeholders. Separating concerns by views allows stakeholders to focus on a few things
  at a time and offers a means of managing complexity.</p>
<div class="second">Key issues in software design</div>
<p>Key issues include performance, security, reliability, usability, and how to decompose, organize, and package
  software components. Non-key issues that deal with some aspect of software's behavior that is not in the application
  domain but which addresses some of the supporting domains, are sometimes referred to as aspects.</p>
<div class="third">Concurrency</div>
<p>Design for concurrency is concerned with decomposing software into processes, tasks, and threads and dealing with
  related issues of efficiency, atomicity, synchronization, and scheduling.</p>
<div class="third">Control and handling of events</div>
<p>This is concerned with how to organize data and control flow and how to handle reactive and temporal events through
  various mechanisms such as implicit invocation and call-backs.</p>
<div class="third">Data persistence</div>
<p>Handles long-lived data.</p>
<div class="third">Distribution of components</div>
<p>This design issue is concerned with how to distribute the software across the hardware, how the components
  communicate, and how middleware can be used to deal with heterogeneous software.</p>
<div class="third">Error and exception handling and fault tolerance</div>
<p>How to prevent, tolerate, and process errors and deal with exceptional conditions.</p>
<div class="third">Interaction and presentation</div>
<p>Concerned with how to structure and organize interactions with users as well as the presentation of information (eg.
  separation of presentation and business logic using Model-View-Controller approach). Note that this does not specify
  user interface details, which is the task of user interface design.</p>
<div class="third">Security</div>
<p>How to prevent unauthorized disclosure, creation, change, deletion, or denial of access to information and other
  resources. Also concerned with how to tolerate security-related attacks or violations by limiting damage, continuing
  service, speeding repair and recovery, and failing and recovering securely. Access control is a fundamental concept of
  security, and one should also ensure the proper use of cryptology.</p>
<div class="second">Software structure and architecture</div>
<p>A software architecture is "the set of structures needed to reason about the system, which comprise software
  elements, relations among them, and properties of both." These design concepts can be used to design families of
  programs (ie. product lines). These are attempts to describe and thus reuse, design knowledge.</p>
<div class="third">Architectural structures and viewpoints</div>
<p>A view represents a partial aspect of a software architecture athat shows specific properties of a software system.
  Views pertain to distinct issues associated with software design. For example, the logical view (satisfying the
  functional requirements) vs. the process view (concurrency issues) vs. the physical view (distribution issues) vs. the
  development view (how the design is broken down into implementation units with explicit representatio nof the
  dependencies among the units).</p>
<div class="third">Architectural styles</div>
<p>An architectural style is a specialization of element and relation types, together with a set of constraints on how
  they can be used, or a provision of the software's high-level organization. Examples are general structures,
  distributed systems, interactive systems, and adaptable systems.</p>
<div class="third">Design patterns</div>
<p>A pattern is a common solution to a common problem in a given context, and design patterns are used to describe
  details at a lower level than architectural styles. These lower level design patterns include creational patterns (eg.
  builder, factory, prototype, singleton), structural patterns (eg. adapter, bridge, composite, decorator, facade,
  fly-weight, proxy), and behavioral patterns (eg. interpreter, iterator, mediator, memento, observer, state, strategy,
  template, visitor).</p>
<div class="third">Architecture design decisions</div>
<p>Architectural deisgn is a creative process. It is useful to think of the architectural design process from a
  decision-making perspective than from an activity perspective.</p>
<div class="third">Families of programs and frameworks</div>
<p>Software product lines can identify commonalities among members and design reusable and customizable components to
  account for variability among family membersn. In object-oriented (OO) programming, a key related notion is that of a
  framework: a partially completed software system that can be extended by appropriately instantiating specific
  extensions, such as plug-ins.</p>
<div class="second">User interface design</div>
<div class="third">General user interface design principles</div>
<p>User interface design should ensure that interaction between the human and the machine provides for effective
  operation and control of the machine. The user interface should be designed to match the skills, experience, and
  expectations of its anticipated users. This includes learnability (easy to learn), user familiarity (should use terms
  and concepts drawn from the experiences of people who will use the software), consistency (comparable operations are
  activated in the same way), minimal surprise, recoverability (allow users to recover from errors), user guidance (give
  meaningful feedback when errors occur and provide context-related help), and user diversity (allow for diverse types
  of users, eg. blind, poor eyesight, deaf, colorblind, etc.).</p>
<div class="third">User interface design issues</div>
<p>How should the user interact with the software? How should information from the software be presented to the user?
</p>
<div class="third">The design of user interaction modalities</div>
<p>User interaction styles can be classified into the following primary styles: question-answer (user issues a question
  to the software, and the software returns the answer), direct manipulation (users interact with objects on the
  computer screen), menu selection (user selects a command from a menu list of commands), form fill-in (user fills in
  forms), command language (user issues a command and related parameters), and natural language (user issues a command
  in natural language which is parsed and translated into software commands).</p>
<div class="third">The deisgn of information presentation</div>
<p>Information presentation may be textual or graphical in nature. A good design keeps the information presentation
  separate from the information itself. The MVC approach is an efective way to keep information presentation separated
  from information being presented. Response time and feedback are also part of design of information presentation.
  Response time is generally measured from the point at which a user executes a certain control action until the
  software responds with a response. An indication of progress is desirable while the software is preparing the
  response. Feedback can be provided by restating the user's input while processing is being completed. Abstract
  visualizatoins can be used when large amounts of information are to be presented.</p>
<p>Designers can use color to enhance an interface: limit the number of colors used, use color change to show change of
  software status, use color-coding to support the user's task, use color-coding in a thoughtful and consistent way, use
  colors to facilitate access for people with color blindness/deficiency, don't depend on color alone to convey
  important information.</p>
<div class="third">User interface design process</div>
<p>The design process usually involves 1) user analysis (designer analyzes users' tasks and how users interact with
  other people, 2) software prototyping, and 3) interface evaluation.</p>
<div class="third">Localization and internationalization</div>
<p>User interface design often needs to consider internationalization and localization, which are means of adapting
  softawre to the different languages, regional differences, and the technical requirements of a target market.
  Internationalization is the process of designing a software application so that it can be adapted to various languages
  and regions without major engineering changes. Localization is the process of adapting internationalized software for
  a specific region or language by adding locale-specific components and translating the text. Localization and
  internationalization should consider factors such as symbols, numbers, currency, time, and measurement units.</p>
<div class="third">Metaphors and conceptual models</div>
<p>An example of a metaphor is an icon of a trash ca nas a metaphor for the operation delete. Metaphors present
  potential problems with respect to internationalization since not all metaphors are meaningful across cultures.</p>
<div class="second">Software design quality analysis and evaluation</div>
<div class="third">Quality attributes</div>
<p>There is a distinction between qualities discernable at runtmie (performance, security, availability, functionality,
  usability) and those that are not (modifiability, portability, reusability, testability), and those related to
  architecture's intrinsic qualities (eg. conceptual integrity, correctness, completeness).</p>
<div class="third">Quality analysis and evaluation techniques</div>
<p>Software design reviews are informal and formalized techniques to determine the quality of design artifacts. Static
  analysis are formal or semiformal static (nonexecutable) analysis that can be used to evaluate a design (eg.
  fault-tree analysis or automated cross-checking). Design vulnerability analysis can be performed if security is a
  concern. Formal design analysis uses mathematical models that allow designers to predicate the behavior and validate
  the performance of software instead of having to rely entirely on testing. Simulation and prototyping can help analyze
  quality.</p>
<div class="third">Measures</div>
<p>Measures can be classified as function-based (structured) which are obtained by analyzing functional decomposition,
  generally represented using a structure chart aka hierarchical diagram, and object-oriented design measures which are
  usually represented as a class diagram.</p>
<div class="second">Software design notations</div>
<div class="third">Structural descriptions (static view)</div>
<p>Mostly but not always graphical notations that describe and represent the structural aspects of a software
  design-that is, they are used to describe the major components and how they are interconnected: architecture
  description languages (ADLs) are textual, often formal, languages used to describe software architecture in terms of
  components and connectors, class and object diagrams are used to represent a set of classes and objects and their
  interrelationships, component diagrams represent a set of components (physical and replaceable parts of a system that
  conform to and provide the realization of a set of interfaces) and their interrelationships, class responsibility
  collaborator cards (CRCs) denote names, responsibilities, and collaborators of components, deployment diagrams
  represent a set of physical nodes and their relationships, entity-relationship diagrams (ERDs) represent conceptual
  models of data stored in information repositories, interface description languages (IDLs) are programming-like
  languages used to define interfaces of software components, and structure charts are used to describe the calling
  structure of programs.</p>
<div class="third">Behavioral descriptions (dynamic view)</div>
<p>Activity diagrams are used to show control flow from activity to activity. Communication diagrams show interactions
  that occur among a grou pof objects (emphasis on objects, their links, and the messages they exchange on those links).
  Data flow diagrams (DFDs) show data flow among elements and can be used for security analysis as they offer
  identification of possible paths for attack and disclosure of confidential information. Decision tables and diagrams
  represent complex combinations of conditions and ctions. Flowcharts represent the flow of control and the associated
  actions to be performed. Sequence diagrams show interactions among a group of objects, with emphasis on the time
  ordering of messages passed between objects. State transition and state chart diagrams show control flow from state to
  state and how the behavior of a component changes based on its current state in a state machine. Formal specification
  languages are textual languages that use basic notions from mathematics (eg. logic, set, sequence) to rigorously and
  abstractly define software component interfaces and behavior, often in terms of pre- and postconditions. Pseudo code
  and program design languages (PDLs) are structured programming-like languages used to describe the behavior of a
  procedure or method.</p>
<div class="second">Software design strategies and methods</div>
<div class="third">General strategies</div>
<p>Divide-and-conquer and stepwise refinement strategies, top-down vs. bottom-up strategies, and strategies making use
  of heuristics, use of patterns and pattern languages, and use of an iterative and incremental approach.</p>
<div class="third">Function-oriented (structured) design</div>
<p>One of the classical methods of software design, where decomposition centers on identifying the major software
  functions and then elaborating and refining them in a hierarchical top-down manner.</p>
<div class="third">Object-oriented design</div>
<p>Noun = object; verb = method; adjective = attribute, where inheritance and polymorphism play a key role. This has
  evolved since to a field of component-based design, where metainformation can be defined and accessed.</p>
<div class="third">Data structure-centered design</div>
<p>Starts from data structures a program manipulates rather than from the function it performs. The software engineer
  first describes the input and output data structures and then develops the program's control structure based on these
  data structure diagrams.</p>
<div class="third">Component-based design (CBD)</div>
<p>A software component is an independent unit, having well-defined interfaces and dependencies that can be composed and
  deployed independently. Component-based design addresses issues related to providing, developing, and itnegrating such
  componnets in order to improve reuse. Components with a certain degree of trustworthiness should not depend on less
  trustworthy components.</p>
<div class="third">Other methods</div>
<p>Iterative and adaptive methods implement software increments and reduce emphasis on rigorous software requirement and
  design. Aspect-oriented design constructs software using aspects to implement the crosscutting concerns and extensions
  that are identified during the software requirement process. Service-oriented architecture builds distributed software
  using web services executed on distributed computers.</p>
<div class="second">Software design tools</div>
<p>Software design tools help translate requirements model into a design representation, provide support for
  representing functional components and their interfaces, implement heuristics refinement and partitioning, and provide
  guidleines for quality assessment.</p>

<div class="first">Software construction</div><a class="anchor" id="software_construction"></a>
<p>The term software construction refers to the detailed creation of working software through a combination of coding,
  verification, unit testing, integration testing, and debugging.</p>
<div class="second">Software construction fundamentals</div>
<div class="third">Minimizing complexity</div>
<p>Minimizing complexity is motivated by the fact that people are limited in their ability to hold complex structures
  and information in their working memory over long periods of time. Reduced complexity is achieved through emphasizing
  code creation that is simple and readable rather than clever. It is accomplished through use of standards, modular
  design, etc.</p>
<div class="third">Anticipating change</div>
<p>Anticipating change helps software engineers build extensible softawre, which means they can enhance a software
  product without disrupting the underlying structure.</p>
<div class="third">Constructing for verification</div>
<p>Constructing for verification means building software in such a way that faults can be readily found by the software
  engineers writing the software as well as by the testers and users during independent testing and operational
  activities. Specific techniques include following coding standards to support code reviews and unit testing,
  organizing code to support automated testing, and restricting the use of complex or hard-to-understand language
  structures.</p>
<div class="third">Reuse</div>
<p>Reuse refers to using existing assets in solving different problems. These typically include libraries, modules,
  components, source code, and commercial off-the-shelf (COTS) assets. "Construction for reuse" means to create reusable
  software assets, and "construction with reuse" means to reuse software assets in the construction of a new solution.
</p>
<div class="third">Standards in construction</div>
<p>Standards that directly affect construction issues include communication methods (eg. standards for document formats
  and contents), programming languages, coding standards (eg. standards for namign conventions, layouts, and
  indentation), platforms (eg. interface standards for operating system calls), and tools (eg. diagrammatic standards
  for notations like UML). Standards can be external or internal.</p>
<div class="second">Managing construction</div>
<div class="third">Construction in life cycle models</div>
<p>Waterfall and staged-delivery life cycle models treat construction as an activity that occurs only after significant
  prerequisite work has been completed, and emphasize the activities that precede construction (requirements and
  design). The main emphasis in these models may be coding.</p>
<p>Evolutionary prototyping and agile development are more iterative, and tend to treat construction as an activity that
  occurs concurrently with other software development activities. These approaches usually mix design, coding, and
  testing activities as all part of construction.</p>
<p>In general, software construction is mostly coding and debugging, but it also involves construction planning,
  detailed design, unit testing, integration testing, etc.</p>
<div class="third">Construction planning</div>
<p>The choice of construction method affects the extent to which construction prerequisites are performed, the order in
  which they are performed, and the degree to which they should be completed before construction work begins.
  Constructoin planning also defines the order in whcih the components are created and integrated, the integration
  strategy (eg. phased or incremental integration), the software quality management processes, the allocation of task
  assignments to specific software engineers, etc.</p>
<div class="third">Construction measurement</div>
<p>Construction activities and artifacts can be measured, eg. code developed, code modified, code reused, code
  destroyed, code complexity, code inspection statistics, fault-fix and fault-find rates, effort, and scheduling. These
  can be useful for managing construction.</p>
<div class="second">Practical considerations</div>
<div class="third">Construction design</div>
<p>Just as construction workers building a physical structure must make small-scale modifications to account for
  unanticipated gaps in the builder's plans, software construction workers must make modifications on a smaller or
  larger scale to flesh out details of software design during construction.</p>
<div class="third">Construction languages</div>
<p>Construction languages include all forms of communication by which a human can specify an executable problem solution
  to a problem. The simplest type of construction language is a configuration language, in which software engineers
  choose from a limited set of predefined options to create new or custom software installations. The text-based
  configuration files used in both the Windows and Unix operating systems are examples of this.</p>
<p>Toolkit languages are used to build applications out of elements in toolkits. Scripting languages are commonly used
  kinds of application programming languages, also called batch files or macros. Programming languages are the most
  flexible type of construction languages. They also contain the least amount of information about specific application
  areas and development processes, and so require the most training and skill to use effectively. The choice of
  programming language can have a large effect on the likelihood of vulnerabilities being introduced during coding.</p>
<p>There are three general kinds of notation used for programing languages. Linguistic notations (eg. C/C++, Java) use
  textual strings to represent complex software constructions and have a sentence-like syntax. Formal notations (eg.
  Event-B) rely less on intuitive, everyday meanings of words and text strings and more on definitions backed up by
  precise, unambiguous and formal (or mathematical) definitions. Formal construction are at the base of most forms of
  system programming notations, where accuracy, time behavior, and testability are more miportant than ease of mapping
  into natural language. Visual notations (eg. Matlab) rely on direct visual interpretation and placement of visual
  entities that represent the underlying software. Visual construction is limited by the difficulty of making "complex"
  statements using only the arrangement of icons on a display.</p>
<div class="third">Coding</div>
<p>Consider techniques for creating understandable source code (naming conventions, source code layout), use of classes,
  enumerated types, variables, named constants, use of control structures, handling of error conditions, prevention of
  code-level security breaches, resource usage via use of exclusion mechanisms and discipline in accessing serially
  reusable resources, source code organization, code documentation, and code tuning.</p>
<div class="third">Construction testing</div>
<p>Construction involves two forms of testing, often performed by the software engineers who wrote the code: unit
  testing and integration testing. The purpose is to reduce the gap between the time when faults are inserted and the
  time when those faults are detected. Test cases may be written before or after code is written.</p>
<div class="third">Construction for reuse</div>
<p>Construction for reuse creates software that has the potential to be reused in the future, usually based on
  variability analysis and design. To aovid the problem of code clones, it is desired to encapsulate reusable code
  fragments into well-structured libraries or components. The tasks related to reuse include variability implementation
  with mechanisms such as parameterization, conditional compilation, design patterns, etc., variability encapsulation to
  make the software assets easy to configure and customize, testing the variability provided by the reusable software
  assets, and description and publication of reusable software assets.</p>
<div class="third">Construction with reuse</div>
<p>Construction with reuse means to create new software with the reuse of existing software assets. The most popular
  method of reuse is to reuse code from the libraries provided by the language, platform, tools being used, or an
  organization repository. Tasks for construction with reuse include selection of reusable units, databases, test
  procedures, or test data, evaluation of code or test reusability, integration of reusable software assets into current
  software, and reporting of reuse information on new code, test procedures, or test data.</p>
<div class="third">Construction quality</div>
<p>The primary technique use for construction quality include unit testing and integration testing, test-first
  development, use of assertions and defensive programming, debugging, inspections, technical reviews, and static
  analysis. Programmers should know good practices and common vulnerabilities-for example, from widely recognized lists
  about common vulnerabilities. Automated static analysis of code for security weaknesses is available for several
  common programming languages.</p>
<div class="third">Integration</div>
<p>Concerns related to construction integration include planning the sequence in which components will be integrated,
  identifying what hardware is needed, creating scaffolding to support interim versions of the software, determining the
  degree of testing and quality work performed on components before they are integrated, and determining points in the
  project at which interim versions of the software are tested.</p>
<p>Programs can be integrated by either phased or incremental approach. Phased integration, also called "big bang"
  integration, delays integration until all parts are complete. Incremental integration involves writing and testing a
  program in small pieces and then combining the pieces one at a time. Stubs, drivers, and mock objects are usually
  needed to enable incremental integration.</p>
<div class="second">Construction technologies</div>
<div class="third">API design and use</div>
<p>An application programming interface (API) is the set of signatures that are exported and available to the users of a
  library or a framework to write their applications. Besides signatures, an API should always include statements about
  the program's effects and/or behaviors (i.e., its semantics). API design should try to make the API easy to learn and
  memorize, lead to readable code, be hard to misuse, be easy to extend, be complete, and maintain backward
  compatibility.</p>
<div class="third">Object-oriented runtime issues</div>
<p>Object-oriented languages support runtime mechanisms that increase flexibility and adaptability of object-oriented
  programs. Polymorphism is the ability of a language to support general operations without knowing until runtime what
  kind of concrete objects the software will include. Because the program does not know the exact type of objects in
  advance, the exact behavior is determined at runtime (called dynamic binding). Reflection is the ability of a program
  to observe and modify its own structure and behavior at runtime, allowing inspection of classes, interfaces, fields,
  and methods at runtime without knowing their names at compile time.</p>
<div class="third">Parameterization and generics</div>
<p>Parametrized types, also known as generics (Ada, Eiffel) and templates (C++), enable the definition of a type or
  class without specifying all the other types it uses. The unspecified types are supplied as parameters at the point of
  use. Parameterized types provide a third way (in addition to class inheritance and object composition) to compose
  behaviors in object-oriented software.</p>
<div class="third">Assertions, design by contract, and defensive programming</div>
<p>An assertion is an executable predicate that's placed in a program (usually a routine or macro) that allows runtime
  checks of the program. Assertions are normally compiled into the code at development time and are later compiled out
  of code so they don't degrade the performance. Design by contract is a development approach in which preconditions and
  postconditions are included for each routine. A contract provides a precise specification of the semantics of a
  routine. Defensive programming means to protect a routine from being broken by invalid inputs. Common ways are to
  check the values of all the input parameters and deciding how to handle bad inputs.</p>
<div class="third">Error handling, exception handling, and fault tolerance</div>
<p>Errors can be handled with assertions, returning a neutral value, substituting the next piece of valid data, logging
  a warning message, returning an error code, or shutting down. Exceptions detect and process errors or exceptional
  events. The basic structure of an exception is that a routine uses throws a detected exception and an exception
  handling block will catch the exception in a try-catch block. The try-catch block may process the erroneous condition
  or it may return control to the calling routine. Exception handling should be carefully designed following common
  principles such as including in the exception message all information that led to the exception, avoiding empty catch
  blocks, knowing the exceptions the library code throws, perhaps building a centralized exception reporter, and
  standardizing the program's use of exceptions. Fault tolerance is a collection of techniques that increase software
  reliability by detecting errors and recovering from them if possible or containing their effects if recovery is not
  possible. The most common fault tolerance strategies include backing up and retrying, using auxiliary code, using
  voting algorithms, and replacing an erroneous value with a phony value that will have a benign effect.</p>
<div class="third">Executable models</div>
<p>Executable models abstract away the details of specific programming languages and decisions about the organization of
  the software. A specification built in an executable modeling language like xUML (executable UML) can be deployed in
  various software environmnets without change. An executable-model compiler (transformer) can turn an executable model
  into an implementation using a set of decisions about the target hardware and sofware environment. Executable models
  are one foundation supporting the Model-Driven Architecture (DMA) initiative of the Object Management Group (OMG). An
  executable model is a way to completely specify a Platform Indepdent Model (PIM), which is a model of a solution to a
  problem that does not rely on any implementation technologies. The Platform Specific Model (PSM) weaves together the
  PIM and the platform on which it relies.</p>
<div class="third">State-based and table-driven construction techniques</div>
<p>State-based programming, or automata-based programming, is a programming technology using finite state machines to
  describe program behaviors. The main idea is to construct computer programs the same way the automation of
  technological processes is done. Combined with object-oriented programming, state-based programming forms a new
  composite approach called state-based, object-oriented programming. A table-driven method is a schema that uses tables
  to look up information rather than using logic statements such as if and case.</p>
<div class="third">Runtime configuration and internationalization</div>
<p>Runtime configuration is a technique that binds variable values and program settings when the program is running,
  usually by updating and reading configuration files in a just-in-time model. Internationalization is the technical
  activity of preparing a program, usually an interactive one, to support multiple locales. The corresponding activity,
  localization, is the activity of modifying a program to support a specific local language.</p>
<div class="third">Grammar-based input processing</div>
<p>Grammar-based input processing involves syntax analysis, or parsing, of the input token stream, and involves creation
  of a data structure (called a parse tree or syntax tree) representing the input data.</p>
<div class="third">Concurrency primitives</div>
<p>A synchronization primitive is a programming abstraction provided by a programming language or the operating system
  that facilitates concurrency and synchronization. Well-known concurrency primitives include semaphores, monitors, and
  mutexes. A semaphore is a protected variable or abstract data type that provides a simple but useful abstraction for
  controlling access to a common resource by multiple processes or threads in a concurrent programming environment. A
  monitor is an abstract data type that presents a set of programmer-defined operations that are executed with mutual
  exclusion. A mutex (mutual exclusion) is a synchronization primitive that grants exclusive access to a shared resource
  by only one process or thread at a time.</p>
<div class="third">Middleware</div>
<p>Middleware is a broad classification for software that provides services above the operating system layer yet below
  the application program layer. Middleware can provide runtime containers for software components to provide message
  passing, persistence, and a transparent location across a network. Middleware can be viewed as a connector between the
  components that use the middleware. Modern message-oriented middleware usually provides an Enterprise Service Bus
  (ESB), which supports service-oriented interaction and communication between multiple software applications.</p>
<div class="third">Construction methods for distributed software</div>
<p>A distributed system is a collection of physically separate, possibly heterogeneous computer systems that are
  networked to provide the users with access to the various resources that the system maintains. Construction of
  distributed software is distinguished by issues such as paralliems, communication, and fault tolerance. Distributed
  programming typically falls into client-server, 3-tier architecture, n-tier architecture, distributed objects, loose
  coupling, or tight coupling.</p>
<div class="third">Constructive heterogeneous systems</div>
<p>Heterogeneous systems consist of a variety of specialized computational units of different types, such as Digital
  Signal Processors (DSPs), microcontrollers, and peripheral processors. These computational units are independently
  controlled and communicate with one another. Embedded systems are typically heterogeneous systems. Software
  development and virtual hardware development proceed concurrently through stepwise decomposition. The hardware part is
  usually simulated in field programmable gate arrays (FPGAs) or application-specific integrated circuits (ASICs). The
  software part is translated into a low-level programming language.</p>
<div class="third">Performance analysis and tuning</div>
<p>Code efficiency, determined by architecture, detailed design decisions, and data-structure and algorithm selection,
  influences an execution speed and size. Performance analysis is the investigation of a program's behavior using
  information gathered as the program's executes, with the goal of identifying possible hot spots in the program to be
  improved. Code tuning, which improves performance at the code level, is the practice of modifying correct code in ways
  to make it run more efficiently.</p>
<div class="third">Platform standards</div>
<p>Platform standards are a set of standard services and APIs that compatible platform implementations must implement.
  Examples are Java 2 Platform Enterprise Edition (J2EE) and POSIX standard for operating systems (Portable Operating
  System Interface), which reppresents a set of standards implemented primarily for UNIX-based operating systems.</p>
<div class="third">Test-first programming</div>
<p>Test-first programming, also known as test-driven development (TDD), is a popular development style in which test
  cases are written prior to writing any code.</p>
<div class="second">Software construction tools</div>
<div class="third">Development environments</div>
<p>A development environment, or integrated development environment (IDE), provides comprehensive facilities to
  programmers for software construction by integrating a set of development tools. In addition to basic code editing
  functions, modern IDEs often offer other features like compilation and error detection from within the editor,
  itnegration with source code control, build/test/debugging tools, compressed or outline views of programs, automated
  code transforms, and support for refactoring.</p>
<div class="third">GUI builders</div>
<p>A Graphical User Interface (GUI) builder is a software development tool that enables the developer to create and
  maintain GUIs in a What You See Is What You Get (WYSIWYG) mode. A GUI builder usually includes a visual editor for the
  developer to design forms and windows and manage the layout of the widgets by dragging, dropping, and parameter
  setting.</p>
<div class="third">Unit testing tools</div>
<p>Unit testing verifies the functioning of software modules in isolation from other software elements that are
  separately testable and is often automated.</p>
<div class="third">Profiling, performance analysis, and slicing tools</div>
<p>The most common performance analysis tools are profiling tools. An execution profiling tool monitors the code while
  it runs and records how many times each statement is executed or how much time the program spends on each statement or
  execution path. Program slicing involves computation of the set of program statements (the program slice) that may
  affect the values of specified variables at some point of interest, referred to as a slicing criterion.</p>

<div class="first">Software testing</div><a class="anchor" id="software_testing"></a>
<p>Software testing consists of the dynamic verification that a program provides expected behaviors on a finite set of
  test cases, suitably selected from the usually infinite execution domain. Dynamic means that testing always implies
  executing the program on selected inputs (as opposed to static techniques which do not execute code). Finite is
  because exhaustive testing could take forever to execute. Selected is because selection of which test technique and
  which test cases can be difficult. Expected
  means the tests should have some expected outcome.</p>
<p>Software testing is, or should be pervasive throughout the entire development and maintenance life cycle. Software
  failures experienced after delivery are addressed by corrective maintenance.</p>
<p>The test target and test objective together determine how the test set is identified, both with regard to its
  consistency (how much testing is enough, aka test adequacy criteria) and to its composition (which test cases should
  be selected, aka selection criteria).</p>
<div class="second">Software testing fundamentals</div>
<div class="third">Testing-related terminology</div>
<p>Many terms are used in software engineering to describe a malfunction: notably fault, failure, and error. It is
  essential to clearly distinguish between the cause of a malfunction (fault) and an undesired effect in the system's
  delivered services (failure). There may be faults in the software that never manifest as a failure. It is the faults
  that can and must be removed. The more generic term defect can be used to refer to either a fault or a failure, when
  the distinction is not important. To avoid ambiguity, one could refer to failure-causing inputs instead of faults,
  since faults can't be proved to cause a failure.</p>
<div class="third">Key issues</div>
<p>Key issues in testing include having a test selection/test adequacy criterion that can determine whether a set of
  test cases is sufficient, determining testing effectiveness by analyzing a set of program executions, and testing for
  defect discovery by trying to find a test that causes the system to fail. The oracle problem (an agent that decides
  whether a program behaved correctly in a given test). Testing theory warns against ascribing an unjustified field of
  confidence to a series of successful tests. Most results of testing theory are negative, in that they state what
  testing can never achieve as opposed to what is actually achieved. The Dijkstra aphorism is that program testing can
  show bugs but never show their absence. The problem of infeasible paths are control flow paths that cannot be
  exercised by any input data, and pose a significant problem in path-based testing. Testability means either the ease
  with which a given test coverage criterion can be satisfied or the likelihood, measured statistically, that a set of
  test cases will expose a failure if the software is faulty.</p>
<div class="third">Relationship of testing to other activities</div>
<p>Testing is related to but different from, for example, static software quality management techniques, correctness
  proofs and formal verification, debugging, and program construction.</p>
<div class="second">Test levels</div>
<p>Software testing is usually performed at different levels throughout the development and maintenance processes.
  Levels can be distinguished based on the object of testing, called the target, or on the purpose, called the
  objective.</p>
<div class="third">The target of the test</div>
<p>The target of the test can vary: a single module, a group of modules, or an entire system. Three test stages can be
  distinguished: unit, integration, and system.</p>
<p>Unit testing verifies the functioning in isolation of software elements that are separately testable. Typically, unit
  testing occurs with access to the code being tested and with the support of debugging tools. The programmers who wrote
  the code typically, but not always, conduct unit testing.</p>
<p>Integration testing is the process of verifying the interactions among software components. Classic integration test
  strategies, such as top-down and bottom-up, are often used with hierarchically structured software. Modern, systematic
  integration strategies are typically architecture-driven, involving incrementally integrating components based on
  identified functional threads. Integration testing is often an ongoing activity during which software engineers
  abstract away lower-level perspectives and concentrate on the perspectives of the level at which they are integrating.
</p>
<p>System testing is concerned with testing the behavior of an entire system, usually for the nonfunctional system
  requirements such as speed, security, accuracy, and reliability. External interfaces to other applications, utilities,
  hardware devices, or operating environments are usually evaluated at this level.</p>
<div class="third">Objectives of testing</div>
<p>Stating the objectives of testing in precise, quantitative terms supports measurement and control of the test
  process. Test cases can be designed to check that the functional specifications are correctly implemented, referred to
  in the literature as conformance testing, correctness testing, or functional testing. Nonfunctional properties can be
  tested as well. Important objects include reliability, security vulnerabilities, usability, and software acceptance.
</p>
<p>Acceptance or qualification testing determines whether a system satisfies its acceptance criteria, usually by
  checking desired system behaviors against the customer's requirements.</p>
<p>Installation testing can be viewed as system testing conducted in the operational environment of hardware
  configurations and other operational constraints.</p>
<p>Before software is released, it is sometimes given to a small, selected group of potential users for alpha testing
  and to a larger set of representative useres for beta testing.</p>
<p>Testing improves reliability by identifying and correcting faults. Statistical measures of reliability can be
  derived by randomly generating test cases according to the operational profile of the software, an approach called
  operational testing.</p>
<p>Regression testing is the selective retesting of a system or component to verify that modifications have not caused
  unintended effects and that the system or component still complies with its specified requirements.</p>
<p>Performance testing verifies that the software meets the specified performance requirements and assess performance
  characteristics such as capacity and response time.</p>
<p>Security testing is focused on the verification that software is protected from external attacks. Usually, security
  testing also includes verification against misuse and abuse of the software or system (negative testing).</p>
<p>Stress testing exercises software at maximum design load with the goal of determining behavioral limits and to test
  defense mechanisms in critical systems.</p>
<p>Back-to-back testing is testing in which two or more variants of a program are executed with the same inputs, the
  outputs are compared, and errors are analyzed in case of discrepancies.</p>
<p>Recovery testing verifies software restart capabilties after a system crash or other disaster.</p>
<p>Interface testing verifies whether the components interface to provide the correct exchange of data and control
  information. A specific objective of interface testing is to simulate the use of APIs by end-user applications.</p>
<p>Configuration testing verifies software under different configurations.</p>
<p>Usability and human computer-interaction testing evaluates how easy it is for end useres to learn and to use
  software.</p>
<div class="second">Test techniques</div>
<p>One of the aims of testing is to detect failures and to break a program. Testing techniques can be classified based
  on how tests are generated. Sometimes these techniques are classified as white-box or glass-box if the tests are
  based on how the software has been designed or coded, or black-box if the test cases rely only on input/output
  behavior.</p>
<div class="third">Based on the software engineer's intuition and experience</div>
<p>Ad hoc testing uses tests derived relying on the software engnieer's skill, intuion, and experience with similar
  programs. This is perhaps the most widely practiced techniques, and are useful for identifying test cases not
  easily generated by more formal techniques.</p>
<p>Exploratory testing is simultaneous learning, test design, and test execution. The tests are not defined in
  advance but are dynamically designed, executed, and modified.</p>
<div class="third">Input domain-based techniques</div>
<p>Equivalence parititioning involves partitioning the input domain into a collectio nof subsets (or equivalent
  classes). A representative set of tests is taken from each equivalency class.</p>
<p>Pairwise testing involves test cases derived by combining interesting values for every pair of a set of input
  variables. Pairwise testing belongs to combinatorial testing. Techniques that includes higher-level combinations than
  pairs are referred to as t-wise, whereby every possible combination of t input variables is considered.</p>
<p>Boundary-value analysis includes test cases chosen on or near the boundaries of the input domain of variables, with
  the underlying rationale that many faults tend to concentrate near the extreme values of inputs. An extension of this
  technique is robustness testing, wherein test cases are also chosen outside the input domain to test robustness in
  processing unexpected or erroneous inputs.</p>
<p>Random testing have tests generated purely at random (not to be confused with statistical testing from operational
  profile). Fuzz testing or fuzzing is a special form of random testing aimed at breaking the software, most often used
  for security testing.</p>
<div class="third">Code-based techniques</div>
<p>Control flow-based coverage criteria are aimed at covering all the statements, blocks of statements, or specified
  combinations of statements in a program. The strongest of the control flow-based criteria is path testing, which aims
  to execute all entry-to-exit control paths in a program's control flow graph. Since exhaustive path testing is
  generally not feasible because of loops, other less stringent criteria focus on coverage of paths that limit loop
  iterations such as statement coverage, branch coverage, and condition/decision testing. The adequacy of such tests is
  measured in percentages.</p>
<p>Data flow-based testing has a control flow graph annotated with information about how the program varialbes are
  defined, used, and killed (undefined). The strongest criterion, all defition-use paths, requires that for each
  variable every control flow path segment to a use of that definition is executed. Weaker strategies such as
  all-definitions and all-uses are employed to reduce the number of paths required.</p>
<p>The control structure of a program can be graphically represented using a flow graph, a directed graph where the
  nodes and arcs correspond to program elements.</p>
<div class="third">Fault-based techniques</div>
<p>Fault-based techniques specifically aim to reveal categories of likely or predefined faults. To better focus the test
  case generation, a fault model can be introduced that classifies different types of faults.</p>
<p>Error guessing involves test cases designed by software engnieers who try to anticipate the most plausible faults in
  a given program.</p>
<p>A mutant is a slightly modified version of the program under test, differing by a small syntactic change. Every test
  case has the original program and all generated mutants. If a test case is successful in identifying the difference
  between the program and a mutant, the latter is said to be killed. Mutation testing was originally conceived as a
  technique to evaluate test sets, and is a testing criterion in itself. The underlying assumption of mutation testing,
  the coupling effect, is that by looking for simple syntactic faults, more complex but real faults will be found.</p>
<div class="third">Usage-based techniques</div>
<p>Reliability evaluation, or operational testing, the tests reproduce the operational environment of the software, or
  the software's operational profile, as closely as possible. To do this, inputs are assigned probabilities, or
  profiles, according to their frequency of occurrence in actual operation.</p>
<p>Specialized heuristics, called usability inspection methods, are applied for the systematic observation of system
  usage under controlled conditions in order to determine how well people can use teh system and its interfaces.
  Usability heuristics include cognitive walkthroughs, claims analysis, field observations, thinking aloud, and even
  approaches such as user questionnaires and interviews.</p>
<div class="third">Model-based testing techniques</div>
<p>A model in this context is an abstract (formal) representation of the software under test or of its software
  requirements. Model-based testing is used to validate requirements, check their consistency, and generate test cases
  focused on the behavioral aspects of the software.</p>
<p>Decision tables represent logical relationships between conditions and actions. Test cases are systematically derived
  by considering every possible combination of conditions and their corresponding resultant actions. A related technique
  is cause-effect graphing.</p>
<p>A program can be modeled as a finite state machine.</p>
<p>Stating the specifications in a formal language permits automatic derivation of functional test cases and provides an
  oracle for checking test results. Testing and test control notation version 3 (TTCN3) is a language for writing test
  cases, particularly suitable for testing complex communication protocols.</p>
<p>Workflow models specify a sequence of activities performed by humans and/or software applications, usually
  represented through graphical notations. Each sequence of actions constitutes one workflow (ie. scenario).</p>
<div class="third">Techniques based on the nature of the application</div>
<p>Additional techniques for test derivation depends on nature of software (eg. object-oriented, component-based,
  web-based, concurrent, protocol-based, real-time systems, safety-critical systems, service-oriented, open-source,
  embedded).</p>
<div class="third">Selecting and combining techniques</div>
<p>Model-based and code-based test techniques are often contrasted as functional vs. structural testing. These should be
  seen as complements. In addition, test cases can be selected in a deterministic way or randomly drawn from some
  distribution of inputs, such as is done in reliability testing.</p>
<div class="second">Test-related measures</div>
<p>A clear distinction should be made between test-related measures that provide an evaluation of a program under test,
  based on the observed test ouputs, and the measures that evaluate the thoroughness of the test set. For example,
  achieving high branch coverage improves chances of finding failures but is not a testing objective.</p>
<div class="third">Evaluation of the program under test</div>
<p>Measures based on software size (eg. source lines of code, functional size, or frequency with which modules call one
  another) is one measure. It is important to know which types of faults may be found in the software under test. A
  program under test can be evaluated by counting discovered faults as the ratio between the number of faults found and
  the size of the program. A statistical estimate of software reliability, which can be obtained by observing
  reliability achieved, can evaluate a software product and decide whether or not testing can be stopped. Reliability
  growth models provide a prediction of reliability based on failures. These models are divided into failure-count and
  time-between-failure models.</p>
<div class="third">Evaluation of the tests performed</div>
<p>Coverage or thoroughness measures evaluate the thoroughness of executed tests, dynamically measuring the ratio
  between covered elements and the total number.</p>
<p>In fault seeding, some faults are artificially introduced into a program before testing. Testing effectiveness can be
  evaluated, in theory, by how well they discover these artificial faults. In practice, statisticians question the
  distribution and representativeness of seeded faults.</p>
<p>The ratio of killed mutants to the total number of generated mutants measures the effectiveness of an executed test
  set in mutation testing.</p>
<p>It is important to be precise as to the property against which the techniques are being assessed.</p>
<div class="second">Test process</div>
<p>Testing concepts, strategies, techniques, and measures need to be integrated into a defined and controlled process.
  The test process supports testing activities and provides guidance to testers and testing teams.</p>
<div class="third">Pratical considerations</div>
<p>Successful testing requires a collaborative attitude towards testing and quality assurance activities. The mindset of
  individual code ownership among programmers may need to be overcome in order to foster a favorable reception towards
  failure discovery and correction.</p>
<p>The testing phases can be guided by various aims such as risk-based or scenario-based.</p>
<p>Test activities conducted at different levels must be organized.</p>
<p>Documentation is an integral part of the formalization of the test process. Test documents may include the test plan,
  test design specification, test procedure specification, test case specification, test log, and test incident report.
  The software under test is documented as the test item. Test documentation should be under the control of software
  configuration management.</p>
<p>Test-driven development (TDD) originated as one of the core extreme programming (XP) practices and consists of
  writing unit tests prior to writing the code to be tested. In this way, test cases are a surrogate for software
  requirements.</p>
<p>The testing team can be composed of internal members, external members, or both.</p>
<p>Measures related to resources spent on testing are used by managers to control and improve the testing process.</p>
<p>Thoroughness measures and considerations about costs and risks of possible remaining failures, help to determine when
  a test stage can be determinated.</p>
<p>The means used to test each part of the software should be reused systematically. A repository of test materials
  should be under control of software configuration management.</p>
<div class="third">Test activities</div>
<p>Test activities must be planned, including coordination of personnel, availability of test facilities and equipment,
  creation and maintenance of all test-related documentation, and planning for possible undesirable outcomes.</p>
<p>Generation
  of test cases is based on the level of testing to be performed and the particular testing techniques. Test cases
  should be under the control of software configuration management and include the expected results for each test.</p>
<p>The
  environment used for testing should be compatible with other adopted software engineering tools. Executoin of tests
  should embody a basic principle of scientific experimentation: everything during testing should be performed and
  documented clearly enough that another person could replicate the results.</p>
<p>The results of testing should be evaluated
  to determine whether the testing has been successful. Testing activities can be entered into a testing log to identify
  when a test was conducted, who performed the test, what software configuratoin was used, and other relevant
  identificatio ninformation. Defects can be tracked and analyzed to determine when they were introduced into the
  software, why they were created (eg. poorly defined requirements, incorrect variable declaration, memory leak,
  programming syntax error), and when they could have been first observed in the software.</p>
<div class="second">Software testing tools</div>
<div class="third">Testing tool support</div>
<p>Testing requires many labor-intensive tasks, running numerous program executions, and handling a great amount of
  information. Appropriate tools can make this less error-prone. Tool selection depends on diverse evidence, suc has
  development choices, evaluation objectives, execution facilities, and so on.</p>
<div class="third">Categories of tools</div>
<p>Test harnesses (drivers, stubs) provide a controlled environment in which tests can be launched and test ouputs can
  be logged. In order to execute parts of a program, dirvers and stubs are provided to simulate calling and are called
  modules, respectively. Test generators provide assistance in the generation test cases. Capture/replay tools replay
  previously executed tests. Oracle/file comparators/assertion checking tools assist in deciding whether a test outcome
  is successful or not. Coverage analyzers assess which and how many entities of the program flow graph have been
  exercised amongst all those required by the selected test coverage criterion. The analysis can be done thanks to
  program instrumenters that insert recording probes into the code. Tracers record the history of a program's execution
  paths. Regression testing tools support the reexecution of a test suite after a section of software has been modified.
  Reliability evaluation tools support test results analysis and graphical visualization.</p>


<div class="first">Software maintenance</div><a class="anchor" id="software_maintenance"></a>
<p>Historically, software development has been more important than maintenance, but this is changing as organizations
  strive to get more out of their software by making it operate as long as possible as well as the development of the
  open source paradigm making people maintain software developed by others. Software maintenance is defined as the
  totality of activities required to provide cost-effective support to software. Software maintenance involves
  both pre-delivery and post-delivery aspects.</p>
<div class="second">Software maintenance fundamentals</div>
<div class="third">Definitions and terminology</div>
<p>The objective of software maintenance is to modify existing software while preserving its integrity.</p>
<div class="third">Nature of maintenance</div>
<p>Software maintenance sustains the software product throughout its life cycle. A maintainer is an organization that
  performs maintenance activities, and sometimes refers to the individuals who perform those activities, contrasting
  them with developers.</p>
<div class="third">Need for maintenance</div>
<p>Maintenance must be performed to correct faults, improve design, implement enhancements, interface with other
  software, adapt programs so that different hardware, software, sytsem features, and telecommunications facilities can
  be used, migrate legacy software, and retire software. Five key characteristics of maintainer's activites are
  maintaining control over the software's day-to-day functions, maintaining control over software modification,
  perfecting existing functions, identifying security threats and fixing security vulnerabilities, and preventing
  software performance from degrading to unacceptable levels.</p>
<div class="third">Majority of maintenance costs</div>
<p>Maintenance of software costs a lot but there is a misconception that most of the costs are from corrections.</p>
<div class="third">Evolution of software</div>
<p>One of the eight "Laws of Evolution" is that maintenance is evolutionary development and that maintenance decisions
  are aided by understand what happens to software over time. Software will grow more complex unless some action is
  taken to reduce this complexity.</p>
<div class="third">Categories of maintenance</div>
<p>Corrective maintenance is reactive modifications or repairs of a software product peformed after delivery to correct
  discovered problems. Included in ths category is emergency maintenance, which is an unscheduled modification performed
  to temporarily keep a software product operational pending corrective maintenance.</p>
<p>Adaptive maintenance is modification of a software product performed after delivery to keep a software product usable
  in a changed or changing environment.</p>
<p>Perfective maintenance is modification of a software product after delivery to provide enhancements for users,
  improvement of program documentation, and recoding to improve software performance, maintainability, or other software
  attributes.</p>
<p>Preventive maintenance is modification after delivery to detect and correct latent faults before they become
  operational faults.</p>
<p>Adaptive and perfective maintenance are classified as maintenance enhancements. Preventive and perfective are
  proactive, while corrective and adaptive are reactive. Preventive and corrective are correction.</p>
<div class="second">Key issues in software maintenance</div>
<div class="third">Technical issues</div>
<p>Limited understanding refers to how quickly a software engineer can understand where to make a change or conrrection
  in software that he did not develop. The topic of software comprehension is of great interest to software engineers.
  Comprehension is more difficult in text-oriented representation (eg. in source code).</p>
<p>Regression testing (selective retesting of software or a component to verify that modifications have not caused
  unintended effects) is an important testing concept in maintenance. Coordinating tests when different members of the
  maintenance team are working on different problems at the same time remains a challenge. When software performs
  critical functions, it may be difficult to bring it offline to test. Tests cannot be executed in the most meaning
  place-the production system.</p>
<p>Impact analysis describes how to conduct, cost-effectively, a complete analysis of the impact of a change in existing
  software. It identifies all systems and software products affected by a software change request and develops an
  estimate of the resources needed to accomplish the change. The change request, sometimes called a modification request
  (MR) or problem report (PR), must first be analyzed and translated into software terms. Impact analysis is performed
  after a change request enters the software configuration management process.</p>
<p>The severity of a problem is often used to decide how and when it will be fixed.</p>
<p>Maintainability is the capability of the software product to be modified, including corrections, improvements, and
  adaptations of the software to changes in environment as well as changes in requirements and functional
  specifications. Developers are prone to disregard maintainer's requirements which can result in lack of software
  documentation and test environments, which leads to difficulties in program comprehension and subsequent impact
  analysis.</p>
<div class="third">Management issues</div>
<p>Organizational objectives help to look at the return on investment of software maintenance activities. Extending the
  life of software can have less clarity in terms of the return on investment, so the view at the senior management
  level is often that of a major activity consuming significant resources with no clear quantifiable benefit for the
  organization.</p>
<p>Staffing refers to how to attract and keep software maintenance staff. Maintenance is not viewed as glamorous work.
  Software maintenance personnel are frequently viewed as "second-class citizens," and morale therefore suffers.</p>
<p>The software life cycle process is a set of activities, methods, practices, and transformations that people use to
  develop and maintain software and its associated products. At the process level, software maintenance is similar to
  software development.</p>
<p>Organizational aspects describe how to identify which organization and/or functil will be responsible for the
  maintenance of software. The team that developed the software is not necessarily the team that maintains it. Having a
  permanent maintenance team allows for specialization, creates communication channels, promotes an egoless, collegiate
  atmosphere, reduces dependency on individuals, and allows for periodic audit checks.</p>
<p>Outsourcing and offshoring software maintenance has become a major industry. Organizations are outsourcing entire
  portfolios of software, including software maintenance More often, the outsourcing option is selected for less
  mission-critical software. Outsourcing requires a significant initial investment and the setup of a maintenance
  process that will require automation.</p>
<div class="third">Maintenance cost estimation</div>
<p>The two most popular approaches to estimating resources for software maintenance are the use of parametric models and
  the use of experience.</p>
<p>Parametric cost modeling (mathematical models) uses historical data from past maintenance to use and calibrate
  mathematical models, and cost drier attributes affect the estimates.</p>
<p>Experience, in the form of expert judgment, is often used to estimate maintenance effort. Clearly, the best approach
  to maintenance estimation is to combine historical data and experience.</p>
<div class="third">Software maintenance measurement</div>
<p>Measurable entities related to maintenance include process, resource, and product.</p>
<p>Measures for subcharacteristics of maintainability include analyzability, the measures of the maintainer's effort or
  resources expended in trying to diagnose deficiencies or causes of failure, changeability, the measures of the
  maintainer's effort associated with implementing a specified modification, stability, measures of the unexpected
  behavior of software, testability, measures of effort in trying to test the modified software, size, complexity,
  understandability, and maintainability.</p>
<div class="second">Maintenance process</div>
<div class="third">Maintenance processes</div>
<p>Software maintenance activities iclude process implementation, problem and modification analysis, modification
  implementation, maintenance review/acceptance, migration, and software retirement. Ohter maintenance process models
  include quick fix, spiral, Osborne's, iterative enhancement, and reuse-oriented.</p>
<div class="third">Maintenance activities</div>
<p>Maintenance activities are similar to those of software development: performing analysis, design, coding, testing,
  and documentation, tracking requirements, and updating documentation.</p>
<p>Unique activities include program understanding, transition (transfer software progressively from developer to
  maintainer), modification request acceptance/rejection, maintenance help desk, impact analysis, and maintenance
  service-level agreements (SLAs) and maintenance licenses and contracts that describe services and quality objectives.
</p>
<p>Supporting activities include documentation, software configuration management, verification and validation, problem
  resolution, software quality assurance, reviews, audits, training maintainers and users.</p>
<p>Planning activities are associated with business planning (organizational level), maintenance planning (transition
  level), release/version planning (software level), and individual software change request planning (request level). At
  the individual request level, planning is carried out during impact analysis. At the release/version planning level,
  maintainer collects the dates of availability of individual requests, agrees with users on content of
  releases/versions, identifies potential conflicts and develops alternatives, assesses risks, and informs all
  staeholders.</p>
<p>Maintenance phase usually lasts for many years. Making estimates of resouces is a key element of maintenance
  planning. A concept document should be developed followed by a maintenance plan. The document should address the scope
  of software maintenance, adaptation of the software maintenanc eprocess, identification of the osftware maintenance
  organization, and estimate of software maintenance costs. The software maintenance plan should specify how users will
  request software modifications or report problems.</p>
<p>Software configuration management procedures should provide for the verification, validation, and audit of each step
  required to identify, authorize, implement and release the software product. Modifications to software must be
  controlled by using an approved software configuration management (SCM) process.</p>
<p>Maintainers should have a software quality program.</p>
<div class="second">Techniques for maintenance</div>
<div class="third">Program comprehension</div>
<p>Code browsers are key tools for program comprehension and are used to organize and present source code. Clear and
  concise documentation can also aid in program comprehension.</p>
<div class="third">Reengineering</div>
<P>Reengineering is the examination and alteration of software to reconstitute it in a new form, including the
  subsequent implementation of the new form. It is often undertaken to replace aging legacy software. Refactoring is a
  reengineering technique that aims at reorganizing a program without changing its behavior.</p>
<div class="third">Reverse engineering</div>
<p>Reverse engineering is the process of analyzing software to identify the software's components and their
  inter-relationships and to create representations of the software in another form or at higher levels of abstraction.
  Reverse engineering does not change the software or result in new software, but produces call graphs and control flow
  graphs. Types include redocumentation, design recovery, and data reverse engineering, where logical schemas are
  recovered from physical databases.</p>
<div class="third">Migration</div>
<p>The maintainer needs to determine the actions needed to migrate software to run in different environments in a
  migration plan that covers migration requirements, migration tools, conversion of product and data, execution,
  verification, and support. Migrating software can also entail notification of intent (a statement of why the old
  environment is no longer supported), parallel operations (make available old and new environments so the user
  experiences a smooth transition), notification of completion, postoperation review, and data archival.</p>
<div class="third">Retirement</div>
<p>Retirement involves a retirement plan, which covers retirement requirements, impact, replacement, schedule, and
  effort.</p>
<div class="second">Software maintenance tools</div>
<p>Tools for program comprehension include program slicers (selecting parts of a program affected by a change), static
  analyzers (general viewing and summaries of a program content), dynamic analyzers (allow maintainers to trace the
  execution path of a program), data flow analyzers (track all possible data flows), cross-referencers (generate indices
  of program components), and dependency analyzers (understand relationships between components of a program).</p>

<div class="first">Software configuration management</div><a class="anchor" id="software_configuration_management"></a>
<p>A system can be defined as the combination of interacting elements organized to achieve one or more stated purposes.
  The configuration of a system is the functional and physical characteristics or hardware or software as set forth in
  technical documentation or achieved in a product. It can also be thought of as a collection of specific versions of
  hardware, firmware, or software items combined according to specific build procedures to serve a particular purpose.
  Configuration management (CM), then, is the discipline of identifying the configuration of a system at distinct points
  in time for the purpose of systematically controlling changes to the configuration and maintaining the integrity and
  traceability of the configuration throughout the sytsem life cycle. Software configuration management (SCM) is a
  supporting-software life cycle process.</p>
<p>The concepts of configuration management apply to all items to be controlled although there are some differences
  between hardware CM and software CM.</p>
<div class="second">Management of the SCM process</div>
<p>SCM controls the evolution and integrity of a product by identifying its elements; managing and controlling change;
  and verifying, recording, and reporting on configuration information. SCM facilitates development and change
  implementation activities.</p>
<div class="third">Organizational context for SCM</div>
<p>Although the responsibility for performing certain SCM tasks might be assigned to other parts of the organization,
  the overall responsibility for SCM often rests with a distinct organizational element or designated individiaul.</p>
<div class="third">Constraints and guidance for the SCM process</div>
<p>Policies and procedures set forth at corporate or other organization levels, contract between acquirer and supplier,
  external regulatory bodies, and the particular software life cycle process chosen can all affect design and
  implementation of the SCM process.</p>
<div class="third">Planning for SCM</div>
<p>The major activities covered are software configuration identification, software configuration control, software
  configuration status accounting, software configuration auditing, and software release management and delivery. The
  reuslts of the planning activity are recorded in an SCM Plan (SCMP), which is typically subject to SQA review and
  audit.</p>
<p>From an SCM standpoint, a branch is defined as a set of evolving source file versions. Merging consists in combining
  different changes to the same file.</p>
<p>Organizational roles to be involved in the SCM process need to be clearly identified.</p>
<p>Planning for SCM identifies the staff and tools involved, address scheduling questions by establishing necessary
  sequences of SCM tasks and identifying their relationships to the project management planning stage, and specifying
  training requirements for new members.</p>
<p>When selecting SCM tools, consider organization (what motivates tool acquisition from an organizational perspective),
  tools (commercial or develop ourselves), environment (what are the constraints), legacy (how will projects use the new
  tools), financing (who will pay for the tools), scope (how will the new tools be deployed), ownership (who is
  responsible for the introduction of new tools), future (what is the plan for the tools' use), change (how adaptable
  are the tools), branching and merging, integration, and migration. SCM typically requires a set of tools, sometimes
  referred to as workbenches. Another important consideration is if the SCM workbench will be open (if tools from
  different suppliers will be used in different activities of the SCM process) or integrated (where elements of the
  workbench are designed to work together).</p>
<p>SCM planning considers if acquiring or making use of purchased software products such as compilers or other tools
  will be taken under configuration control.</p>
<p>SCM planning considers how software items that interface with another software or hardware item will be identified
  and how changes to the items will be managed and communicated.</p>
<div class="third">SCM plan</div>
<p>The results of SCM planning for a given project are recorded in a software configuration management plan (SCMP), a
  "living document" which serves as a reference. Categories of SCM information to be included in an SCMP are
  introduction (purpose, scope, terms used), SCM management (organization, responsibilities, authorities, applicable
  policies, directives procedures), SCM activities (configuration identification, configuration control), SCM schedules,
  SCM resources, and SCMP maintenance.</p>
<div class="third">Surveillance of software configuration management</div>
<p>After the SCM process has been implemented, some degree of surveillance may be necessary. The use of integrated SCM
  tools with process control capability can make the surveillance task easier.</p>
<p>SCM measures can be designed to provide specific information on the evolving product or to provide insight into the
  functioning of the SCM process.</p>
<p>Audits can be carried out during the software engineering process to investigate the current status of specific
  elements of the configuration or to assess the implementation of the SCM process.</p>
<div class="second">Software configuration identification</div>
<p>Software configuration identification identifies items to be controlled, establishes identification schemes for the
  items and their versions, and establishes the tools and techniques to be used in acquiring and managnig controlled
  items.</p>
<div class="third">Identifying items to be controlled</div>
<p>Identifying items involves understanding the software configuration within the context of the system configuration,
  selecting software configuration items, developing a startegy for labeling software items and describing their
  relationships, and identifying the baselines to be used and the procedure for a baselnie's acquisition of the items.
</p>
<p>Software configuration is the functional and physical characteristics of hardware or software as set forth in
  technical documentation or achieved in a product, viewed as part of an overall system configuration.</p>
<p>A configuration item (CI) is an item or aggregation of hardware or software or both that is designed to be managed as
  a single eneity. A software configuration item (SCI) is a software entity that has been established as a configuration
  item. These include plans, specifications and design documentation, testing materials, software tools, source and
  executable code, code libraries, data and data dictionaries, documentation for installation, maintenance, operations,
  and software use.</p>
<p>Structural relationships among selected SCIs should be properly tracked.</p>
<p>A version of a software item is an identified instance of an item. A variant is a version of a program resulting from
  the application of software diversity.</p>
<p>A software baseline is a formally approved version of a configuration item (regardless of media) that is formally
  designated and fixed at a specific time during the configuration item's life cycle. This also refers to a particular
  version of a software configuration item that has been agreed on. The baseline can only be changed through formal
  change control procedures. A baseline represents the current approved configuration.</p>
<p>The functional baseline corresponds to the reviewed system requirements. The allocated baseline corresponds to the
  reviewed software requirements specification and software interface requirements specification. The developmental
  baseline represents the evolving software configuration at selected times during the software life cycle. The product
  baseline corresponds to the completed software product delivered for system integration.</p>
<p>The triggering of placing SCIs under SCM control is the completion of some form of formal acceptance task, such as a
  formal review. In acquiring an SCI, its origin and initial integrity must be established. Following acquisition of an
  SCI, changes to the item must be formally approved and defined in the SCMP. Once approved, the item is incorporated
  into the software baseline.</p>
<div class="third">Software library</div>
<p>A software library is a controlled collection of software and related documentation designed to aid in software
  development, use, or maintenance. A working library could support coding, a project support library could support
  testing, while a master library could be used for finished products. An appropriate level of SCM control is associated
  with each library. Security, in terms of access control and the backup facilities, is a key aspect of library
  management.</p>
<div class="second">Software configuration control</div>
<p>Software configuration control is concerned with managing changes during the softawre life cycle. It covers the
  process of determining what changes to make, the autorhity for approving certain changes, support for the
  implementation of those changes, and the concept of formal deviations from project requirements as well as waivers of
  them.</p>
<div class="third">Requesting, evaluating, and approving software changes</div>
<p>The software change request process provides formal procedures for submitting and recording change requests,
  evaluating the potential cost and impact of a proposed change, and accepting, modifying, deferring, or rejecting the
  proposed change. A change request (CR) is a request to expand or reduce the project scope, modify policies, processes,
  plans, or procedures, modify costs or budgets, or revise schedules. The type of change is usually recorded on the
  Software CR (SCR).</p>
<p>Once an SCR is received, an impact analysis (technical evaluation) is performed and an established authority will
  accept, modify, reject, or defer the proposed change.</p>
<p>The authority for accepting or rejecting proposed changes rests with an entity known as a Configuration Control Board
  (CCB). When the scope of authority of a CCB is strictly software, it is known as a Software Configuration Control
  Board (SCCB).</p>
<p>An effective software change request (SCR) process requires originating change requests, enforcing flow of change,
  capturing CCB decisions, and reporting change process information.</p>
<div class="third">Implementing software changes</div>
<p>Approved SCRs are implemented using defied software procedures in accordance with the applicable schedule
  requirements. Changes may be supported by source code version control tools, which allow a team of software engineers
  to track and document changes to source code, and provide a single repository for storing the source code, and can
  prevent more than one software engineer from editing the same module at the same time, and records all changes made to
  source code.</p>
<div class="third">Deviations and waivers</div>
<p>A deviation is a written authorization, granted prior to the manufacture of an item, to depart form a particular
  performance or design requiremment for a specific number of units or a specific period of time. A waiver is a written
  authorization to accept a configuration item that is found to depart from specified requirements but is nevertheless
  considered suitable for use.</p>
<div class="second">Software configuration status accounting</div>
<p>Software configuration status accounting (SCSA) is an element of configuration management consisting of recording adn
  reporting of information to manage a configuration effectively.</p>
<div class="third">Software configuration status information</div>
<p>The SCSA activity designs and operates a system for the capture and reporting of necessary information as the life
  cycle proceeds.</p>
<div class="third">Software configuration status reporting</div>
<p>Reported information can take on the form of adhoc queries to answer specific questions or the periodic production of
  predesigned reports.</p>
<div class="second">Software configuration auditing</div>
<p>A software audit is an independent examination of a work product or set of work products to assess compliance with
  specifications, standards, contractual agreements, or other criteria. Software configuration auditing determines the
  extent to which an item satisfies the required functional and physical characteristics. Two types of formal audits
  might be required by the governing contract: the Functional Configuration Audit (FCA) and the Physical Configuration
  Audit (PCA). Successful completion of these audits can be a prerequisite for the establishment of the product
  baseline.</p>
<div class="third">Software functional configuration audit</div>
<p>The purpose of FCA is to ensure that the audited software item is consistent with governing specifications. The
  output of software verification and validation activities is a key input to this audit.</p>
<div class="third">Software physical configuration audit</div>
<p>The purpose of PCA is to ensure that the design and reference documentation is consistent with the as-built software
  product.</p>
<div class="third">In-process audits of a software baseline</div>
<p>An audit could be applied to sampled baseline items to ensure performance is consistent with specifications and to
  ensure that evolving documentation continues to be consistent.</p>
<div class="second">Software release management and delivery</div>
<p>In this context, release refers to the distribution of a software configuration item outside the development
  activity, including internal releases. The software librar yis a key element in accomplishing release and delivery
  tasks.</p>
<div class="third">Software building</div>
<p>Software building is the activity of combining the correct versions of software configuration items, using the
  appropriate configuration data, into an executable program for delivery to a customer or other recipient, such as the
  testing activity. In addition to building software for new releases, it is usually also necessary for SCM to reproduce
  prevoius releases for recovery, testing, maintenance, or additional release purposes.</p>
<p>A tool capability is useful for selecting the correct versions of software items for a given target environment and
  for automating the process of building the software from selected versions and appropriate configuration data.</p>
<div class="third">Software release management</div>
<p>Software release management encompasses the identification, packing, and delivery of the elements of a product (eg.
  an executable program, documentation, release notes, and configuration data). The information documenting the physical
  contents of a release is known as a version description document. The release notes typically describe new
  capabilities, known problems, and platform requirements necessary for proper product operation. It is useful to have a
  connection with the tool capability supporting the change request process to map release contents to the SCRs that
  have been received.</p>
<div class="second">Software configuration management tools</div>
<p>SCM tools can be divded into individual support, project-related support, and companywide-process support. Individual
  support tools are appropriate for small organizations and include version control tools (track, document, and store
  individual configuration items such as source code and external documentation), build handling tools (compile and link
  an executable version of the software), and change control tools (support the control of change requests and events
  notification).</p>
<p>Project-related support tools support workspace management for development teams and integrators, able to support
  distributed development environments.</p>
<p>Companywide-process support tools typically automate portions of a companywide process, providing support for
  workflow management, roles, and responsibilities, supporting a more formal development process including certification
  requirements.</p>

<div class="first">Software engineering management</div><a class="anchor" id="software_engineering_management"></a>
<p>Software engineering management can be defined as the application of management activities (planning, coordinating,
  measuring, monitoring, controlling, and reporting) to ensure that software products and services are delivered
  efficiently, effectively, and to the benefit of stakeholders.</p>
<p>Unique aspects of software management include clients often don't know what is needed or what is feasible, clients
  often lack appreciation for complexities inherent in engineering, especially those that change requirements. Increased
  understanding and changing conditions will generate new or changed software requirements. Software is often built
  using an iterative process rather than as a sequence of closed tasks. Software engineering necessaril yincorporates
  creativity and discipline. The degree of novelty and complexity is often high, and there is often a rapid rate of
  changein the underlying technology.</p>
<p>Software engineering management activities occur at the organizational/infrastructure level, project level, and
  measurement program level. Organizational management involves personnel management policies, organizationa lpolicies
  and procedures that provide the framework in which software engineering projects are undertaken, communication
  management, portfolio management (overall view of softwrae currently under development and integrated projects),
  software reuse.</p>
<p>Measurement-informed management, a basic principle of any true engineering discipline, can help improve perception
  and reality. In essence, management without measurement suggests a lack of discipline, and measurement witout
  management suggests lack of purpose or context. Management is a system of processes and controls required to achieve
  the strategic objectives set by the organization. Measurement refers to the assignment of values and labels to
  software engineering work products, processes, and resources plus the models that are derived from them.</p>
<div class="second">Initiation and scope definition</div>
<p>Initiation and scope defintion is on effective determination of software requirements using various elicitation
  methods and the assessment of feasibility from a variety of standpoints.</p>
<div class="third">Determination and negotation of requirements</div>
<p>Activities niclude requirements elicitation, analysis, specification, and validation.</p>
<div class="third">Feasibility analysis</div>
<p>Feasability analysis develops a clear description of project objectives and evaluates alternative approaches.
  Resources include a sufficient number of people who have the needed skills, facilities, infrastructure, and support.
  Feasibility analysis often requires approximate estimations.</p>
<div class="third">Process for the review and revision of requirements</div>
<p>Stakeholders should agree on the means by which requirements and scope are to be reviewed and revised.</p>
<div class="second">Software project planning</div>
<div class="third">Process planning</div>
<p>Software development life cycle (SDLC) models span a continuum from predictive to adaptive. Predictive SDLCs
  emphasize planning and requirements and less on iteration. Adaptive SDLCs accommodate emergent requirements and
  iteration. Well known SDLCs include the waterfall, incremental, and spiral models plus various forms of agile software
  development.</p>
<div class="third">Determine deliverables</div>
<p>The work product of each project activity should be identified and characterized. Opportunities to reuse should be
  evaluated. Procurement of software and use of third parties to develop deliverables should be planned and suppliers
  selected.</p>
<div class="third">Effort, schedule, and cost estimation</div>
<p>Estimated range of effort required can be determined using a calibrated estimation model based on historical size and
  effort data and other relevant methods such as expert judgment and analogy.Task dependencies can be established and
  potential opportunities for completing tasks concurrently and sequentially can be identified and documented using a
  Gantt chart.</p>
<div class="third">Resource allocation</div>
<p>Equipment, facilities, and people should be allocated to the identifeid tasks. A matrix that shows who is responsible
  for what can be used.</p>
<div class="third">Risk management</div>
<p>Uncertainty results from lack of information. Risk is characterized by probability of an event that will result in a
  negative impact plus a characterizatin of the negative impact. Risk is often the result of uncertainty. The converse
  of risk is opportunity. Risk management entails identification of risk factors and analysis of probability and
  potential impact, prioritization of risk factors, and development of risk mitigation strategies. Project abandonment
  conditions can also be determined at this point. Software-unique aspects include software engineers' tendency to add
  unneeded features and software's intangible nature.</p>
<div class="third">Quality management</div>
<p>Quality requirements and thresholds should be set.</p>
<div class="third">Plan management</div>
<p>Plans should be managed.</p>
<div class="second">Software project enactment</div>
<p>Softwre project enactment (also known as project executation) involves implementation of the plan. Software
  acquisition and supplier contract management is concerned with issues involved in contracting with customers and with
  suppliers who supply products or services.</p>
<div class="third">Implementation of plans</div>
<div class="third">Software acquisition and supplier contract management</div>
<p>Agreements with customers and suppliers typically specify scope of work and
  deliverables and include clauses such as penalteis for late delivery or nondelivery and intellectual property
  agreements that specify what the suppliers are providing annd what the acquirer is paying for.</p>
<div class="third">Implementation of measurement process</div>
<div class="third">Monitor process</div>
<p>Adherence to project plan, outputs, completion criteria, deliverables, effort expenditure, schedule adherence, costs
  to date, resource usage, project risk profile, measurement data should be monitored. Variance analysis based on
  deviation of actual from expected outcomes and values should be determined.</p>
<div class="third">Control process</div>
<p>Changes can be made to the project.</p>
<div class="third">Reporting</div>
<p>At specified and agreed-upon times, progress to date should be reported.</p>
<div class="second">Review and evaluation</div>
<div class="third">Determining satisfaction of requirements</div>
<p>Achieving stakeholder satisfaction is a principal goal of the software engineering manager, and so progress should be
  assessed periodically. Software configuration control and software configuration management procedures should be
  followed, decisions documented and communicated, plans revisted and revised, and relevant data recorded.</p>
<div class="third">Reviewing and evaluating performance</div>
<p>Periodic performance reviews for pojrect personnel can provide insights to likelihood of adherence to plans and
  processes as well as possible areas of difficulty.</p>
<div class="second">Closure</div>
<p>An entire project, a major phase, or an iterative cycle reaches closure when all plans and processes have been
  enacted and completed.</p>
<div class="third">Determining closure</div>
<p>Closure occurs when the specified tasks for a project, a phase, or an iteration have been completed and satisfactory
  achievement of the completion criteria has been confirmed.</p>
<div class="third">Closure activities</div>
<p>After closure is confirmed, project materials should be archived in accordance with stakeholder agreed-upon methods,
  location, and duration, possibly including destruction of sensitive information, software, and the medium on which
  copies are resident. Lessons learned should be drawn from and fed into organizational learning and improvement
  endeavors.</p>
<div class="second">Software engineering measurement</div>
<p>Effective measurement is one of the cornerstones of organizational maturity.</p>
<div class="third">Establish and sustain measurement commitment</div>
<p>Each measurement endeavor should be guided by organizational objectives and driven by a set of measurement
  requirements established by the organization.</p>
<p>The organizational unit to which each measurement requirement is to be applied should be established. This may
  consist of a functional area, a single project, a single site, or an entire enterprise. Temporal scope should also be
  considered.</p>
<p>Team commitment should be formally established, communicated, and supported by resources.</p>
<p>Assigning resources includes allocation of responsibility for the various tasks of the measurement process, such as
  analyst and librarian. Adequate funding, training, tools, and support to conduct the porocess should also be
  allocated.</p>
<div class="third">Plan the measurement process</div>
<p>Characterize the organization unit, which provides context for measurement. Identify information needs based on
  goals, constraints, risks, and problems of the organization unit, and prioritize. Select measures based on these
  priorities and other criteria such as cost, degree of process disruption, ease, consistency, and ease of analysis and
  reporting. Define data collection, analysis, and rpeorting procedures. Select criteria for evaluting the information
  products. Provide resources for measurement tasks. Identify resources to be made available for implementation. Acquire
  and deploy supporting technologies.</p>
<div class="third">Perform the measurement process</div>
<p>Integrate measurement procedures with relevant software processes, integrating measurement procedures into the
  software processes they are measuring, taking into account morale, communicatio, training, and support. Collect data,
  verify it, and store it. This can be automated sometimes by software engineering management tools. The results of this
  analysis are typically indciators such as graphs, numbers, or other indicators that will be interpreted. Communciate
  results to users and stakeholders.</p>
<div class="third">Evaluate measurement</div>
<p>Evaluation may be performed internally or externally. Lessons should be recorded in a database. Identify potential
  improvements and determine costs and benefits. Communicate proposed improvements to measurement process owner and
  stakeholders for review and approval.</p>
<div class="second">Software engineering management tools</div>
<p>Software engineering management tools are often used to provide visibility and control of software engineering
  management processes. Some tools are automated while others are manually implemented.</p>
<p>Project planning and tracking tools can be used to estimate project effor tand cost and to prepare project schedules.
  Planning tools also include automated scheduling tools that analyze tasks within a work breakdown structure, their
  estimated durations, their precedence relationships, and the resources assigned to each task to produce a schedule in
  the form of a Gantt chart.</p>
<p>Risk management tools track risk identification, estimation, and monitoring, using approaches such as simulation and
  decision trees to analyze the effect of costs versus payoffs and subjective estimates of probabilities of risk events.
  Monte Carlo simulation tools can be used to produce probability distributions of effort, schedule, and risk by
  combining multiple input probability distributions in an algorithmic manner.</p>
<p>Communication tools include things like email notifications and broadcasts to team members and stakeholders. They
  also include communication of minutes from regularly scheduled project meetings, daily stand-up meetings, plus charts
  showing progress, backlogs, and maintenance request resolutions.</p>
<p>Measurement tools are used t ogather ,analyze, and report project measurement data and may be based on spreadsheets
  developed by project team members or organizational employees.</p>
<div class="first">Software engineering process
</div><a class="anchor" id="software_engineering_process"></a>
<p>An engineering process consists of a set of interrelated activities that transform one or more inputs into outputs
  while consuming resources to accomplish the transformation. Software engineering processes are concerned with work
  activities accomplished by software engineers to develop, maintain, and operate software, such as requirements,
  design, construction, testing, configuration management, and other software engineering processes. Software processes
  are specified to facilitate human understanding, communication, and coordination, to aid in the management of software
  projects, and to measure and improve the quality of software products, to support process improvement, and to provide
  a basis for automated support of process execution.</p>
</p>
<div class="second">Software process definition</div>
<p>A software process is a set of interrelated activities and tasks that transform input work products into output work
  products. At minimum, the description of a software process includes required inputs, transforming work activities,
  and outputs generated. It may also include entry and exit criteria and decomposition of work activities into tasks,
  which are the smallest units of work subject to management accountability. A software process may include
  subprocesses, eg. software requirements validation. Complete definition of a software process may also include the
  roles and competencies, IT support, software engineering techniques and tools, and work environment needed to perfor
  mthe process, as well as approaches and measures to determine efficiency and effectiveness of performing the process.
</p>
<p>Notations for defining software processes include textual lists of constituent activites and tasks described in
  natural language, data-flow diagrams, state charts, BPMN, IDEF0, Petri nets, and UML activity diagrams. The
  transforming tasks within a process may be defined as procedures. No ideal process, or set of processes, exists.</p>
<div class="third">Software process management</div>
<p>Software process manamagent realize effectiveness from a systematic approach to accomplishing software processes and
  produce work products and introduce new or improved processes. When existing processes are modified when other new
  processes are deployed for the first time, we call this "process evolution."</p>
<div class="third">Software process infrastructure</div>
<p>Establishing, implementnig, and managing software processes and software life cycle models often occurs at the level
  of individual software projects, but systematic application of these across an organization can provide benefits to
  all software work but requires commitment at an organization level. Software process infrastructure varies depending
  on the size and complexity of the organization. Experience has shown that implementing systematic improvement of
  software processes tends to result in lower cost.</p>
<div class="second">Software life cycles</div>
<p>A software development life cycle (SDLC) includes the software processes used to specify and transform software
  requirements into a dleiverable software product. A software product life cycle (SPLC) includes a software development
  cycle plus additional softawre processes that provide for deployment, maintenance, support, evolution, retirement, and
  all other inception-to-retirement processes, and may include multiple SDLCs. The temporal relationship among software
  processes is provided by an SDLC or SPLC. They also include control mechanisms for applying entry and exit criteria.
</p>
<div class="third">Categories of software processes</div>
<p>Primary processes include software processes for development, operation, and maintenance of software. Supporting
  processes are intermittent and include configuration management, quality assurance, and verification and validation.
  Organizational process include training, process measurement analysis, infrastructure management, portfolio and reuse
  management, organizational process improvement, and management of software life cycle models. Cross-project processes
  include reuse, software product line, and domain engineering.</p>
<p>Project management processes include planning and estimating, resource management, measuring and controlling,
  leading, managing risk, managing stakeholders, and coordinating the primary, supporting, organization, and
  cross-project processes.</p>
<div class="third">Software life cycle models</div>
<p>Agile models typically involve frequent demonstrations of working software to a customer or user representative who
  directs development of the software in short iterative cycles that produce small increments of working, deliverable
  software.</p>
<p>Linear SDLC models are sometimes referred to as predictive, while iterative and agile are referred to as adaptive.
  Linear models typically develop a complete set of software requirements during project initiation and planning. These
  requirements are then rigorously controlled, and changes are based on change requests processed by a change control
  board. An incremental model produces successive increments of working, deliverable software based on partitioning of
  the requirements. Agile models define product scope and high-level features and facilitate evolution during the
  project.</p>
<div class="third">Software process adaptation</div>
<p>Organizational context, innovations in technology, project size, product criticality, regulatory requirements,
  industry practices, and corporate culture may determine needed adaptations, which may consist of adding more details
  to software processes, activities, tasks, and procedures to address critical concerns.</p>
<div class="third">Practical considerations</div>
<p>Software life cycle models that specify discrete software processes with rigorously specified entry and exit criteria
  and prescribed boundaries and interfaces should be recognized as idealizations that must be adapted to reflect
  reality.</p>
<div class="second">Software process assessment and improvement</div>
<p>Software process assessments are used to evaluate the form and content of a software process. In some instances, the
  terms "process appraisal" and "capability evaluation" are used instead of process assessment. Capability evaluations
  are typically performed by an acquirer or by an external agent on behalf of an acquirer. Performance appraisals are
  typically performed within an organization to determine if a software process is in need of improvement.</p>
<p>A process audit is conducted to ascertain compliance with policies and standards whereas assessments are performed to
  determine levels of capability or maturity.</p>
<div class="third">Software process assessment models</div>
<p>Software process assessment models typically include assessment criteria for software processes regarded as
  constituting good practices.</p>
<div class="third">Software process assessment methods</div>
<p>A software process assessment method can be qualitative, which rely on the judgment of experts, or quantitative,
  which assign numerical scores to software processes. A typical method of software process assessment includes
  planning, fact-finding, collection and validation of process data, and analysis and reporting. The goal of a software
  process assessment is to gain insight that will establish the current status of a process or processes and provide a
  basis for process improvement.</p>
<div class="third">Software process improvement models</div>
<p>A software process improvement cycle typically involves subprocesses of measuring, analyzing, and changing. The
  Plan-Do-Check-Act model is a well-known iterative approach. Planning is identifying and prioritizing desired
  improvements, doing is introducing an improvement, checking is evaluating the improvement, and acting is making
  further modifications.</p>
<div class="third">Continuous and staged software process ratings</div>
<p>A continuous rating system involves assigning a rating to each software process of interest; a staged rating system
  is established by assigning the same maturity rating to all of the software processes within a specified process
  level. Continuous models typically use a level 0 rating, while staged models typically do not. Level 0 means a
  software process is incompletely performed or may not be performed. At level 1, a software process is being performed.
  At level 2, a process is being performed that allows managers visibility into intermediate work products and can exert
  some control over transitions between processes. At level 3, the processes are well defined and are being repeated
  across different projects. At level 4, quantitative measures can be applied and used for process assessment, and
  statistical analysis may be used. At level 5, mechanisms for continuous process improvements are applied.</p>
<div class="second">Software measurement</div>
<p>Before a new process is implemented or a current process is modified, measurement results should be obtained to
  provide a baseline for comparison. For example, before introducing software inspection process, effort required to fix
  defects discovered by testing should be measured. Then, the combined effort of inspection plus testing can be compared
  to the previous amount of effort required for testing alone.</p>
<div class="third">Software process and product measurement</div>
<p>The efficiency of a software process, activity, or task is the ratio of reousrces actually consumed to resources
  expected or desired to be consumed in accomplishing a task. Effort (or equivalent cost) is the primary measure of
  resources for most software processes, measured in units such as person-hours, person-days, staff-weeks, or
  staff-months, or in equivalent monetary units. Effectiveness is the ratio of actual output to expected output. One
  must take care not to be misled by a low effectiveness measure as determined by an unexpectedly low number of defects.
</p>
<p>Product measures important to effectiveness include product complexity, total defects, defect density, quality of
  requirements, design documentation, etc.</p>
<p>Note that efficiency and effectiveness are independent. An effective software can be inefficient. Causes of low
  efficiency or low effectiveness may include deficient input work products, inexperienced personnel, lack of adquate
  tools and infrastructure, learning a new process, a complex product, or an unfamiliar product domain. They are also
  affected by turnover in software personnel, schedule changes, a new customer representative, or a new organizational
  policy.</p>
<p>Software productivity calculated at the level of individuals can be misleading because of the many factors that can
  affect individual productivity. Standardized definitions and counting rules to measure processes and work products are
  necessary.</p>
<p>"Good" software processes produce anticipated results when institutionalzied by adoption within the local or larger
  organization units.</p>
<div class="third">Quality of measurement results</div>
<p>The quality of measurement results is determined by reliability and validity of the measured results.</p>
<div class="third">Software information models</div>
<p>Software information models allow modeling, analysis, and prediction of software process and software product
  attributes to provide answers to relevant questions and achieve process and product improvement goals. Needed data can
  be collected and retained in a repostiory, from where it can be analyzed and models can be constructed. Validation and
  refinement of models ensure level of accuracy is sufficient and that their limitations are understood.</p>
<p>Analysis-driven softawre information model building involves development, calibration, and evaluation of a model.
  Models can be very close, close, or very far from actual results. The Goals/Questions/Metrics (GQM) method guides
  analysis-driven software information model building. An example goal is to reduce the average change request
  processing time by 10% within six months. Questions are what is the baseline change request processing time? Metrics
  include average, standard deviation of change procesisng times on starting date.</p>
<div class="third">Software process measurement techniques</div>
<p>Software process measurement techniques are used to collect process data and work product data, transform data into
  useful information, and analyze information to identify process activities that are candiates for improvement.</p>
<p>Quantitative process measurement collects, transforms, and analyze quantitative process and work product data to
  indicate where process improvements are needed. Their techniques collects data in numerical form to apply mathematical
  and statistical techniques.</p>
<p>Basic tools for quality control can be used to analyze quantitative process measurement data (eg. check sheets,
  Pareto diagrams, histograms, scatter diagrams, run charts, control charts, and cause-and-effect diagrams). Orthogonal
  Defect Classification (ODC) can be used to analyze quantitative process measurement data, and it groups detected
  defects into categories and links the defects to the software processes where they originated. ODC can provide
  quantitative data for applying root cause analysis. Statistical Process Control can track process stability, or the
  lack of process stability, using control charts.</p>
<p>Qualitative process measurement echniques, such as interviews, questionnaires, and expert judgment can be used to
  augment quantitative process measurement echniques. Group consensus techniques including the Delphi technique can be
  used to obtain consensus among groups of stakeholders.</p>
<div class="second">Software engineering process tools</div>
<p>Software process tools include notations such as data-flow diagrams, state charts, BPMN, IDEF0 diagrams, Petri nets,
  and UML activity diagrams. General purpose business tools such as spreadsheets can be useful. Computer-Assisted
  Software Engineering (CASE) tools can reinforce use of integrated processes. Simple tools such as word processors and
  spreadsheets can be used to prepare textual decriptions of processes, activities, and tasks. A project control panel
  or dashboard can display selected processes and product attributes for software products and indicate measurements
  that are within control limits and those needing corrective action.</p>

<div class="first">Software engineering models and methods</div><a class="anchor"
  id="software_engineering_models_and_methods"></a>
<p>Software engineering models and methods impose structure on software engineering with the goal of making that
  activity systematic, repeatable, and ultimately more success-oriented. Using models provides an approach to problem
  solving, a notation, and procedures for model construction and analysis. Methods provide an approcah to systematic
  specification, design, construction, test, and verification of the end-item software and associated work products.</p>
<p>Modeling discusses general practice of modeling and presents topics in modeling principles, properties and expression
  of models, modeling syntax, semantics, and pragmatics, and preconditions, postconditions, and invariants. Types of
  models provides some characteristics of model types commonly found in software engineering practice. Analysis of
  models presents some common analysis techniques to verify completeness, consistency, correctness, traceability, and
  interaction. Software engineering methods discusses commonly used software engineering methods, provides a summary of
  heuristic methods, formal methods, prototyping, and agile methods.</p>
<div class="second">Modeling</div>
<p>Modeling of software is a technique to help software engineers understand, engineer, and communicate aspects of the
  software to appropriate stakeholders.</p>
<div class="third">Modeling principles</div>
<p>Good models to not usually represent every aspect or feature of the software under every possible condition, but
  intsead abstracts way nonessential information.</p>
<p>Perspective (such as structural view, behavioral view, temporal view, organizational view, etc.) focuses modeling
  efforts on specific concerns using appropriate notation, vocabulary, methods, and tools.</p>
<p>Modeling employs the application domain vocabulary of the software, a modeling language, and semantic expression.</p>
<p>A model is an abstraction or simplification of a software component. No single abstraction completely describes a
  software component. A model presents the parts of a software that is needed to make a decision.</p>
<div class="third">Properties and expression of models</div>
<p>Completeness of a model is the dgree to which all requirements have been implemented and verified within the model.
  Consistency is the dgree to which the model contains no conflicting requirements, assertions, constraints, functions
  or component descriptoins. Correctness is the degree to which the model satisfies its requirements and design
  specifications and is free of defects.</p>
<p>Models are constructed to represent real-world objects and their behaviors. The primary expression element of a model
  is an entity, which can represnet concrete artifacts (eg. processors, sensors, robots) or abstract artifacts (eg.
  software modules or communication protocols). Model entities are connected to other entities using relations.
  Expression of model entities may be accomplished using textual or graphical modeling languages. Multiple views of the
  model may be required to capture the needed semantics of the software.</p>
<div class="third">Syntax, semantics, and pragmatics</div>
<p>A complete model may be a union of multiple submodels and any special function models. Examination and
  decision-making relative to a single model may be problematic. Understanding precise meanings of modeling constructs
  can be difficult. Modeling languages are defined by syntactic and semantic rules. Syntax is defined using a notation
  grammar that defines valid language constructs (eg. Backus-Naur Form BNF). For graphical languages, syntax is defined
  using graphical models called metamodels. Semantics specify the meaning attached to entities and relations. For
  example, a simple diagram of two boxes connected by a line can have many meanings. Meaning is communicated through the
  model even in the presence of incomplete information through abstraction; pragmatics explains how meaning is embodied
  in the model.</p>
<div class="third">Preconditions, postconditions, and invariants</div>
<p>Preconditions are a set of conditions that must be satisfied prior to execution of the function or method.
  Postconditions are a set of conditoins that is guaranteed to be true after execution. Typically, the postconditions
  represent how the state of the software has changed, how parameters passed to the function, how data values have
  changed, or how the return value has been affected. Invariants are a set of conditions within the operational
  environment taht persist before and after the exectuion of a function or method.</p>
<div class="second">Types of models</div>
<p>Use of diagrams along with modeling language constructs brings about three broad model types: information models,
  behavioral models, and structure models.</p>
<div class="third">Information modeling</div>
<p>Information modeling provides a central focus on data and information. It is an abstract representation that
  identifies and defines a set of concepts, properties, relations, and constraints on data entities. The semantic or
  conceptual information model is often used to provide some formalism and context to the software being modeled as
  viewed from the problem perspective, without concern for how this model is actually mapped to the implementation of
  the software.</p>
<div class="third">Behavioral modeling</div>
<p>Behavioral models identify and define the functions of the software benig modeled, taking on three basic forms: state
  machines (software as a collection of defined states, events, and transitions), control-flow models (depicts how a
  sequence of events causes processes to be activated or deactivated), and data-flow models (a sequence of steps where
  data moves through processes toward data stores or data sinks).</p>
<div class="third">Structure modeling</div>
<p>Structure models illustrates physical or logical composition of software, defining a boundary between software being
  implemented and the environment in which it is to operate. Some common constructs are composition, decomposition,
  generalization, and specialization of entities; identification of relevant relations and cardinality between entities;
  and the definition of process or functional interfaces.</p>
<div class="second">Analysis of models</div>
<p>Analysis of models is needed to ensure that they are complete, consistent, and correct enough to serve their intended
  purpose for stakeholders.</p>
<div class="third">Analyzing for completeness</div>
<p>Completeness is the degree to which all of the specified requirements have been implemented and verified. Models may
  be checked for completeness using techniques such as structural analysis and state-space reachability analysis, which
  ensures all paths are reached by some set of correct inputs. Models may also be checked for completeness manually by
  using inspections or other review techniques.</p>
<div class="third">Analyzing for consistency</div>
<p>Consistency is the degree to which models contain no conflicting requirements, assertions, constraints, functions, or
  component descriptoins. This is accomplished using an automated analysis function or manual inspection.</p>
<div class="third">Analyzing for correctness</div>
<p>Correctness is the degree to which a model satisfies its software requirements and software design specifications, is
  free of defects, and ultimately meets the stakeholders' needs. Analysis involves verifying syntactic correctness of
  the model (that is, correct use of modeling language grammar and constructs) and verifying semantic correctness
  (correctly using modeling language to represent the meaning of that which is being modeled).</p>
<div class="third">Traceability</div>
<p>As work products are related through various dependency relationships, there is a need to map and control
  traceability relationships to demonstrate requirements consistency.</p>
<div class="third">Interaction analysis</div>
<p>Interaction analysis focuses on communications or control flow relations between entites used to accomplish a
  specific task or function, examining dynamic behavior of the interactions between different portions of the software
  model, including other software layers.</p>
<div class="second">Software engineering methods</div>
<div class="third">Heuristic methods</div>
<p>Heuristic methods are experience-based. Structured analysis and design methods involves a software model that is
  developed primarily from a functional or behavioral viewpoint, starting form a high-level view of software and then
  progressively decomposing the model components through increasingly detailed designs. The detailed design eventually
  converges to specifications of the software that must be coded, built, tested, and verified.</p>
<p>Data modeling methods is constructed form the viewpoint of data or information used, using data tables and
  relationships.</p>
<p>Object-oriented analysis and design methods use models that represent collections of objects that encapsulate data
  and relationships and interact with other objects through methods.</p>
<div class="third">Formal methods</div>
<p>Formal methods are software engineering methods used to specify, develop, and verify the software through application
  of a rigorous mathematically based notation and language.</p>
<p>Specification languages provide the mathematical basis for a formal method, and are formal, higher level computer
  languages (not a classic 3rd Generation Language 3GL programming language) and are not directly executable languages
  but comprised of a notation and syntax, semantics, and a set of allowed relations.</p>
<p>Program refinement is the process of creating a lower level (or more detailed) specification using a series of
  transformations. Specifications may be refined, adding details until the model can be formulated in a 3GL programming
  language or in an executable portion of the chosen specification language.</p>
<p>Model checking is a formal verification method, usually performing a state-space exploration or reachability analysis
  to demonstrate that the software design has the model properties of interest. An example of model checking is an
  analysis that verifies the correct program behavior under all possible interleaving of event and message arrivals.</p>
<p>Logical inference is a method of designing software that involves specifying preconditions and postconditoins around
  each block of the design and developing the proof that those conditions must hold under all inputs using mathematical
  logic.</p>
<div class="third">Prototyping methods</div>
<p>Software prototyping is an activity that generally creates incomplete or minimally functional versions of a software
  application, usually for trying out specific new features, soliciting feedback on software requirements or user
  interfaces, further exploring software requirements, software design or implementation options, and/or gaining other
  useful insight into software.</p>
<p>Prototyping style discusses that prototypes can be developed as throwaway code or paper products, as an evolution of
  a working design, or as an executable specification.</p>
<p>The target of the prototype activity is the specific product being served by the prototyping effort. Examples include
  a requirements specification, an architectural design element or component, an algorithm, or a human-machine user
  interface.</p>
<p>A prototype may be evaluated or tested against actual implemented software or against a target set of requirements.
</p>
<div class="third">Agile methods</div>
<p>Agile methods were born in the 1990s from the need to reduce apparently large overhead associated with heavyweight,
  plan-based methods in large-scale software-development projects. Agile methods are considered lightweight
  characterized by short, iterative development cycles, self-organizing teams, simpler designs, code refactoring,
  test-driven development, frequent customer involvement, and emphasis on creating a demonstrable working product with
  each development cycle.</p>
<p>RAD is a rapid software development method used primarily in data-intensive ,business-systems application
  development.</p>
<p>XP uses stories or scenarios for requirements, develops tests first, has direct customer involvement on the team
  (typically defining acceptance tests), uses pair programming, and provides for continuous code refactoring and
  integration. Stories are decomposed into tasks, prioritized, estimated, developed, and tested.</p>
<p>Scrum is more project management-friendly. The scrum master manages activites within the project increment; each
  increment is called a sprint and lasts no more than 30 days. A Product Backlog Item (PBI) list is developed from tasks
  that are identified, defined, prioritized, and estimated. Daily scrum meetings ensure work is managed to plan.</p>
<p>FDD is a model-driven, short, iterative software development approach using five phases: 1) develop a product model
  to scope the breadth of the domain, 2) create the list of needs or features, 3) build the feature development plan, 4)
  develop designs for iteration-specific features, 5) code, test, and then integrate the features. Code ownership is
  assigned to individuals rather than the team. FDD emphasizes an overall architectural approach which promotes building
  the feature correctly the first time rather than emphasiznig continual refactoring.</p>

<div class="first">Software quality</div><a class="anchor" id="software_quality"></a>
<p>Software quality may refer to desirable characteristics of software products and to what extend a particular product
  possesses those characteristics, and to processes, tools, and techniques used to achieve those characteristics. More
  recently, software quality is defined as the capability of software product to satisfy stated and implied needs under
  specified conditions. Definitions mostly embrace the premise of conformance to requirements, but do not refer to types
  of requirements. There is an ambiguity between software quality and software quality requirements, which are actually
  attributes of functional requirements. Stakeholders could value price, lead time, and software quality.</p>
<div class="second">Software quality fundamentals</div>
<p>A software engineer should understand quality concepts, characteristics, values, and their application to the
  software under development or maintenance. The important concept is that software requirements define the required
  quality attributes of the software.</p>
<div class="third">Software engineering culture and ethics</div>
<p>A healthy software engineering culture understands the tradeoffs among cost, schedule, and quality. A strong ethic
  assumes that engineers accurately report information, conditions, and outcomes related to quality.</p>
<div class="third">Value and costs of quality</div>
<p>Cost of software quality (CoSQ) is a set of measurements derived from the economic assessment of software quality
  development and maintenance processes. The premise underlying the CoSQ is that the level of quality in a software
  product can be inferred from the cost of activities related to dealing with the consequences of poor quality. There
  are four cost of quality categories: prevention, apraisal, internal failure, and external failure. Prevention costs
  include investments in software process improvement efforts, quality infrastructure, quality tools, training, audits,
  and management reviews. Appraisal costs arise from project activities that find defects, categorized into cost of
  reviews and cost of testing. Costs of internal failures are those incurred to fix defects before delivery, and
  external failure costs respond to software problems discovered after delivery.</p>
<div class="third">Models and quality characteristics</div>
<p>It is not possible to completely distinguish process quality from product quality. Determining whether a process has
  the capability to consistently produce products of desired quality is not simple.</p>
<p>The software engineer, first of all, must determine the real purpose of software, incorporating stakeholder
  requirements. All software development processes (eliciting requirements, designing, constructing, building, checking,
  improving quality) are deisgned with these quality requirements in mind. The term work-product means any artifact that
  is the outcome of a process used to create a final software product. While some treatments of quality are described in
  terms of final software and system performance, sound engineering practice requires that intermediate work-products
  are relevant to quality to be evaluated throughout the software engineering process.</p>
<div class="third">Software quality improvement</div>
<p>The quality of software products can be improved through preventative processes or an iterative process of continual
  improvement, whcih requires management control, coordination, and feedback from many concurrent processes: 1) software
  life cycle processes, 2) the process of fault/defect detection, removal, and prevention, and 3) the quality
  improvement process.</p>
<p>Approaches such as the Deming improvement cycle of Plan-Do-Check-Act (PDCA), evolutionary delivery, kaizen, and
  quality function deployment (QFD) offer techniques to specify quality objectives and determine whether they are met.
  The Software Engineering Institute's IDEAL is another method.</p>
<p>Management sponsorship supports process and product evaluations and the resulting findings. Then an improvement
  program is developed.</p>
<div class="third">Software safety</div>
<p>Safety-critical systems are those in which a system failure could harm human life, other living things, physical
  structures, or the environment. Examples include software in mass transit systems, chemical manufacturing plants, and
  medical devices. Industry standards such as DO-178C, have been developed to reduce the risk of injecting faults into
  the software and thus improve software reliability. Direct safety-critical software is one that is embedded in a
  safety-critical system, such as the flight control computer of an aircraft. Indirect are those that are used to
  develop safety-critical software.</p>
<p>Three complementary techniques for reducing the risk of failure are avoidance, detection and removal, and damage
  limitation. Increasing level of risk means more quality assurance and control techniques. Another technique is
  building assurance cases, a reasoned, auditable artifact created to support the contention that its claims are
  satisfied.</p>
<div class="second">Software quality management processes</div>
<p>Software quality management (SQM) is the collection of all processes that ensure that software products, services,
  and life cycle process implementations meet organizational software quality objectives and achieve stakeholder
  satisfaction. SQM defines processes, process owners, requirements for the processes, measurements of the processes and
  their outputs, and feedback channels throughout the whole software life cycle.</p>
<p>SQM comprises four subcategories. Software quality planning includes determining which quality standards are to be
  used, defining specific quality goals, and estimating effort and schedule of software quality activities. Software
  quality assurance (SQA) assess adequacy of software processes to provide evidence that establishes confidence that the
  software processes are appropriate for and produce software of suitable quality for their intended purposes. Software
  quality control (SQC) examines specific project artifacts to determine whether they comply with standards established
  for the project. Software process improvement (SPI), or software corrective and preventive action, seek to improve
  process effectiveness, efficiency, and other characteristics with the ultimate goal of improving software quality.</p>
<div class="third">Software quality assurance</div>
<p>Software quality assurance is not testing, but a set of activities that define and assess the adequacy of software
  processes to provide evidence that establishes confidence that the software processes are appropriate and produce
  software products of suitable quality for their intended purposes. A key attribute of SQA is the objectivity of the
  SQA function with respect to the project, and may be organizationally independent from technical, managerial, and
  financial pressures form the project. SQA has two aspects: product assurance and process assurance.</p>
<p>The software quality plan defines thea ctivities and tasks employed to ensure that software developed satisfies the
  project's requirements and is commensurate with project risks. The SQAP first ensures that quality targets are clearly
  defined and understood.</p>
<p>The SQA plan identifies documents, standards, practices, and conventions that are checked and monitored, measures,
  statistical techniques, procedures for problem reporting and corrective action, resources such as tools, techniques,
  and methodologies, security for physical media, training, and SQA reporting and documentation.</p>
<div class="third">Verification & validation</div>
<p>The purpose of verification & validation (V&V) is to help the development organization build quality into the system
  during the life cycle. Verification is an attempt to ensure that the product is built correctly. Validation is an
  attempt to ensure that the right product is built. Both processes begin early in the development or maintenance phase,
  and provide an examination of key product features in relation to both the product's immediate predecessor and the
  specifications to be met.</p>
<div class="third">Reviews and audits</div>
<p>Reviews and audit processes are broadly defined as static, meaning that no software programs or models are executed,
  examination of software engineering artifacts with respect to standards that have been established. Product assurance
  and process assurance audits are typically conducted by SQA personnel independent of development teams. Product
  assurance audits make certain that processes conform to contracts. Product assurance audits make certain that products
  comply with contracts. Management reviews evaluate actual project results and are conducted by organization or project
  management. Technical reviews examine work-products and are conducted by engineering staff.</p>
<p>Management reviews monitor progress, determine status of plans and schedules, and evaluate effectiveness of
  management processes, tools, and techniques. The main parameters are project cost, schedule, scope, and quality.
  Management reviews evaluate decisions about corrective actions, changes in the allocation of resources, or changes to
  the scope of the project. Inputs may include audit reports, progress reports, V&V reports, and plans of many types.
</p>
<p>Technical reviews evaluate a software product to determine its suitability for its intended use and identify
  discrepancies from specifications and standards. Most importantly, the level of formality distinguish different types
  of technical reviews. Inspections are the most formal, walkthroughs less, and pair reviews or desk checks are the
  least formal. A technical review might require mandatory inputs such as statement of objectives, specific software
  product, specific project management plan, issues list associated with the product, and technical review procedure.
  Technical reviews of source code may include a wide variety of concerns such as analysis of algorithms, utilization of
  critical computer resources, adherence to coding standards, structure and organization of code for testability, and
  safety-critical concerns.</p>
<p>Inspections are differentiated from other technical reviews by rules (defined set of criteria), sampling (checks
  subsets of documents), peer (not management review), led (impartial moderator leads inspection meetings), and meeting
  (includes meetings). Inspections always involve the author of the product, as well as an inspection leader, a
  recorder, a reader, and a few checkers (inspectors). Inspections are usually done on a small section of the product at
  a time.</p>
<p>Walkthroughs are distinguished from inspections in that the author presents the work-product to the other
  participants.</p>
<p>An important point of audits is that they can occur on almost any product at any stage of the development or
  maintenance process.</p>
<div class="second">Practical considerations</div>
<div class="third">Software quality requirements</div>
<p>Factors that influence software quality requirements include the domain of the system, whetehr the system functions
  are critical, the physical environment, system and software functional (what it does) and quality (how well it does
  it) requirements, commercial (external) or standard (internal) components used, specific software engineering
  standards, methods and tools used, budget, staff, organization, intended users and use, and integrity level of the
  system.</p>
<p>In cases where system failure may have extremely severe consequences, overall dependability is the main quality
  requirement over and above basic functionality. This is the case if system failures affect a large number of people,
  if users often reject systems that are unreliable, unsafe, or insecure, if system failure costs are enormous, and if
  undependable systems may cause information loss.</p>
<p>Defining integrity levels is a method of risk management. Software integrity levels are a range of vlaues that
  represent software complexity, criticality, risk, safety level, security level, desired performance, reliability, or
  other project-unique characteristics that define the importance of software to the user and acquirer.</p>
<div class="third">Defect characterization</div>
<p>Software quality evaluation techniques find defects, faults, and failures. Problems may need to be organized into
  taxonomies. The word defect is often overloaded to refer to different types of anomalies. A computational error is
  the difference between a computed, observed, or measured value and the true value. An error is a human action that
  produces an incorrect result. A defect is an imperfection or deficiency in a work product, caused by a person
  committing an error. A fault is a defect in source code, aka bug. A failure is an event in which the system does not
  perform a required function.</p>
<p>Three widely used software quality measurements are defect density (number of defects per unit size of documents),
  fault density (number of faults per 1K lines of code), and failure intensity (failures per use-hour or per test-hour).
  Reliability models are built from failure data and can be used to estmiate probability of future failures.</p>
<p>Actions that may result from finding defects is removal of defects and attempts to eliminate the cause of defects,
  such as with root cause analysis (RCA), which includes analyzing and summarizing the findings to find root causes.</p>
<div class="third">Software quality management techniques</div>
<p>Software control techniques use static and dynamic techniques. Static techniques do not execute code. Formal methods
  are static techniques mostly used in verification of crucial parts of critical systems. Dynamic techniques are
  generally testing techniques, but also may inclue simulation and model analysis. Code reading is a static technique
  but experienced engineers may execute the code as they read through it.</p>
<p>Testing that fall under V&V include evaluation and tests of tools to be used on the project, and conformance tests of
  components and commercial off-the-shelf (COTS) products to be used. Sometimes a third party that is independent and
  accredited by some body of authority tests a product for conformance.</p>
<div class="third">Software quality measurement</div>
<p>Software quality measurements support decision-making, such as when to stop testing and release a product.
  Mathematical and graphical techniques can aid in interpretation of software quality measurement data, such as
  descriptive statistics based (Pareto analysis, run charts, scatter plots, normal distribution), statistical tests
  (binomial test, chi-squared test), trend analysis (control charts), and prediction (reliability models). Descriptive
  statistics-based techniques often provide a snapshot of the more troublesome areas of the software product. Results
  from trend analysis may indciate that a schedule is being met. Predictive techniques assist in estimating efforts and
  predicting failures. From measurement methods, defect profiles can be developed and used to guide future SQM
  processes.</p>
<div class="second">Software quality tools</div>
<p>Software quality tools include static and dynamic analysis tools. Static analysis tools include those that facilitate
  and partially automate reviews and inspections of documents and code and allow users to enter defects found for later
  removal. Some tools help organizations perform software safety hazard analysis, such as automated support for failure
  mode and effects analysis (FMEA) and fault tree analysis (FTA). Other tools analyze data captured from software
  engineering environments and test environments.</p>

<div class="first">Software engineering professional practice</div><a class="anchor"
  id="software_engineering_professional_practice"></a>
<div class="second">Professionalism</div>
<p>Software engineers must possess knowledge, skills, and attitudes to practice in a professional, responsible, and
  ethical manner. The term "professional practice" refers to a way of conducting services so as to achieve certain
  standards or criteria in both the process of performing a service and the end product resulting from the service.</p>
<div class="third">Accreditation, certification, and licensing</div>
<p>Accreditation is a process to certify the competency, authority, or credibility of an organization. In many
  countries, the basic means by which engineers acquire knowledge is through completion of an accredited course of
  study, sometimes by a government organization such as in China, France, Germany, Israel, Italy, and Russia. In the
  United States, engineering accreditation is performed by an organization known as ABET. An organization known as CSAB
  serving as a participating body of ABET is the lead society within ABET for the accreditation of degree programs in
  software engineering.</p>
<p>Certification refers to the confirmation of a person's particular characteristics. A common type of certification is
  professional certification, usually obtained by passing an examination. The IEEE CS has enacted two certification
  programs (CSDA and CSDP) designed to confirm a software engineer's knowledge of standard software engineering
  practices. Currently certification is completely voluntary, and in fact, most software engineers are not certified
  under any program.</p>
<p>Licensing is the action of giving a person authorization to perform certain kinds of activities and to take
  responsibility for resultant engineering products. Government authorities or statutory bodies usually issue licenses.
</p>
<div class="third">Codes of ethics and professional conduct</div>
<p>Codes of ethics and professional conduct present guidance in the face of comflicting imperatives. Violations may be
  acts of commission, such as concealing inadequate work, disclosing confidential information, or falsifying
  information, or through omission. Violations may result in penalties and possible expulsion from professional status.
  Since standards and codes of ethics and professional conduct change over time, each individual must study to stay
  current in their professional practice.
</p>
<div class="third">Nature and role of professional societies</div>
<p>Professional societies are comprised of a mix of practitioners and academics and serve to define, advance, and
  regulate their professions.</p>
<div class="third">Nature and role of software engineering standards</div>
<p>By establishing a consensual body of knowledge and experience, software engineering standards establish a basis upon
  which further guidelines may be developed.</p>
<div class="third">Economic impact of software</div>
<p>At an individual level, an engineer's continuing employment may depend on their ability and willingness to interpret
  and execute tasks in meeting customers' or employers' needs and expectations. At the business level, software properly
  applied to a problem can eliminate months of work and translate to elevated profits or more effective organizations.
  At the societal level, software failure can lead to accidents, interruptions, and loss of service.</p>
<div class="third">Employment contracts</div>
<p>Software engineering work may be solicited as company-to-customer supplier, engineer-to-customer consultancy, direct
  hire, or even volunteering.</p>
<p>A common concern in software engineering contracts is confidentiality. Employers derive commercial advantage from
  intellectual property. Software engineers are often required to sign non-disclosure (NDA) or intellectual property
  (IP) agreements as a precondition to work. Another concern is IP ownership. Rights to software engineering assets may
  reside with the employer or customer, either under explicit contract terms or relevant laws.</p>
<div class="third">Legal issues</div>
<p>Legal issues include matters related to standards, trademarks, patents, copyrights, trade secrets, professional
  liability, legal requirements, trade compliance, and cybercrime. Legal issues are jurisdictionally based.</p>
<p>Software engineering standards establish guidelines for generally accepted practices and minimum requirements for
  products and services provided by a software engineer.</p>
<p>A trademark relates to any word, name, symbol or device used in business transactions, used to indicate the source or
  origin of goods. The World Intellectual Property Organization (WIPO) is the authority that frames the rules and
  regulations on trademarks.</p>
<p>Patents protect an inventor's right to manufacture and sell an idea, consisting of a set of exclusive rights granted
  by a sovereign government to an individual, group of individuals, or organization for a limited period of time. Note
  that if inventions are made during the course of a software engineering contract, ownership may belong to the employer
  or customer or be jointly held rather than belong to the software engineer. In many countries, software code is not
  patentable, though software algorithms may be.</p>
<p>Most governments in the world give exclusive rights of an original work to its creator, usually for a limited time,
  enacted as a copyright. Copyrights are long-term and renewable.</p>
<p>In many countries, an intellectual asset such as a formula, algorithm, process, design, method, pattern, instrument,
  or compilation of information may be considered a "trade secret" if they are not generally known and may provide a
  business some economic advantage and provides legal protection if stolen. However, another party can still derive or
  discover the same asset legally.</p>
<p>Engineers may be held to account for failing to fully and conscientiously follow recommended practice, known as
  negligence. The engineer may also be held to warrant that the product is both suitable and safe for use. Privity is
  the idea that one could only sue the person selling the product, but is no longer a defense against liability actions.
  A defense against such an allegation is to show that standards and generally accepted practices were followed in the
  development of the product.</p>
<p>Software engineers must be aware of legal requirements for registration and licensing (examination, education,
  experience, training requirements), contractual agreements, noncontractual legalities, basic information on
  international legal framework as accessed from the World Trade Organization (WTO).</p>
<p>All software professionals must be aware of legal restrictions on import, export, or reexport of goods, services, and
  technology in the jurisdictions inwhich they work.</p>
<p>Cybercrime refers to any crime that involves a computer, computer software, computer networks, or embedded software
  controlling a system. This category of crime includes fraud, unauthorized access, spam, obscene or offensive content,
  threats, harassment, theft of sensitive personal data or trade secrets, and use of one computer to damage or
  infiltrate other networked computers and automated system controls. Forms of unauthorized access include hacking,
  eavesdropping, and using computer systems in aw ay that is concealed form their owners.</p>
<div class="third">Documentation</div>
<p>Providing clear, thorough, and accurate documentation is the responsibility of each software engineer. Software
  engineers should document relevant facts, significant risks and tradeoffs, and warnings of undesirable or dangerous
  consequences from use or misuse of the software. Software engineers should avoid certifying or approving unacceptable
  products, disclosing confidential information, or falsifying facts or data. For the development organization,
  engineers should provide documentation for software requirements specifications, software design documents, details on
  the software engineering tools used, software test specifications and results, and details on the adopted software
  engineering methods, as well as problems encountered during the development process. For external stakeholders,
  engineers should document information needed to determine if the software is likely to meet the customer's and users'
  needs, description of the safe, and unsafe, use of the software, description of protection of sensitive information
  created by or stored using the software, and clear identification of warnings and critical procedures.</p>
<div class="third">Tradeoff analysis</div>
<p>A software engineer's analysis of alternative problem solutions is tradeoff analysis. The first step is to establish
  design goals and setting the relative importance of those goals. Design goals may include minmization of monetary cost
  and maximization of reliability, peformance, or some other criteria on a wide range of dimensions. Any conflict of
  interest must be disclosed up front.</p>
<div class="second">Group dynamics and psychology</div>
<div class="third">Dynamics of working in teams/groups</div>
<p>Performing teams demonstrate consistent quality of work and progress towards goals and are cooperative, honest, and
  have a focused atmosphere. Individual and team goals are aligned. Team members facilitate this atmosphere by being
  intellectually honest, making use of group thinking, admitting ignorance, and acknowledging mistakes. They communicate
  clearly and directly to each other. Peer reviews are constructive and nonpersonal.</p>
<div class="third">Individual cognition</div>
<p>In software engineering, notably due to the highly abstract nature of software itself, individual cognition plays a
  very prominent role in problem solving. An individual's ability to decompose a problem and creatively develop a
  solution can be inhibited by a need for more knowledge, subconscious assumptions, volume of data, fear of failure,
  culture, lack of ability to express the problem, perceived working atmosphere, and the emotional status of the
  individual.</p>
<p>There is a set of basic methods engineers use to facilitate problem solving. Breaking down problems and soving them
  one piece at a time reduces cognitive overload. Taking advantage of professional curiosity and pursuing continuous
  professional development through training and study adds skills and knowledge to the software engineer's portfolio.
  Reading, networking, and experimenting with new tools, techniques, and methods are all valid means of professional
  development.</p>
<div class="third">Dealing with problem complexity</div>
<p>Many, if not most, software engineering problems are too complex and difficult to address as a whole or to be tackled
  by individual software engineers. The usual means is to adopt teamwork and problem decomposition. Some specific
  teamwork examples are pair programming and code review.</p>
<div class="third">Interacting with stakeholders</div>
<p>Stakeholders should provide support, information, and feedback at all stages of the software life cycle process. It
  is vital to maintain open and productive communication with stakeholders.</p>
<div class="third">Dealing with uncertainty and ambiguity</div>
<p>The software engineer must attack and reduce any lac kof clarity that is an obstacle to performing work. Often,
  uncertainty is simply a reflection of lack of knowledge. In this case, investigation through recourse to formal
  sources such as textbooks and professional journals, interviews with stakeholders, or consultation with teammates and
  peers can overcome it. When uncertainty cannot be overcome easily, software engineers may choose to regard it as a
  project risk. In this case, work estimates or pricing is adjusted to mitigate the anticipated cost of addressing it.
</p>
<div class="third">Dealing with multicultural environments</div>
<p>Multicultural environments are common in software engineering due to geographical separation and outsourcing.
  Tolerance and understanding can be facilitated by support of leadership and management, with frequent communication,
  face-to-face meetings, and communicating in native language.</p>
<div class="second">Communication skills</div>
<p>The software engineer's own career success is affected by the ability to consistently provide oral and written
  communication effective and on time.</p>
<div class="third">Reading, undestanding, and summarizing</div>
<p>Software engineres are able to read and understand technical material, including reference books, manuals, research
  papers, and program source code. A software engineer sifts through accumulated information, filtering out pieces that
  will be most helpful. Customers may ask a software engineer to summarize information for them, simplifying or
  explaining it so they can make the final choice between competing solutions.</p>
<div class="third">Writing</div>
<p>Written products include source code, software project plans, software requirement documents, software test plans,
  user manuals, technical reports and evaluations, justifications, diagrams and charts, and so forth.</p>
<div class="third">Team and group communication</div>
<p>The number of communication paths grows quadratically with the addition of each team member, and team members are
  unlikely to communicate with anyone perceive dto be removed form them by more tha ntwo degrees. Software documentation
  is sometimes a substitute for direct interaction. Email is useful but not always enough, especially if used too much.
  Increasingly, organizations are using enterprise collaboration tools to share information.</p>
<div class="third">Presentation skills</div>
<p>The software engineer's ability to convey concepts effectively in a presentation influences product acceptance,
  management, and support.</p>

<div class="first">Software engineering economics</div><a class="anchor" id="software_engineering_economics"></a>
<p>Software engineering economics is about making decisions related to software engineering in a business context.
  Economics is the study of value, costs, resources, and their relationship in a given context or situation. Software
  engineering economics is concerned with aligning software technical decisions with the business goals of the
  organization, which translate sinto sustainably staying in business.</p>
<p>Economic analysis and decision-making are important engineering considerations because engineers are capable of
  evaluating decisions both technically and from a business perspective. Many engineering proposals and decisions, such
  as make versus buy, have deep intrinsic economic impacts that should be considered explicitly.</p>
<div class="second">Software engineering economics fundamentals</div>
<div class="third">Finance</div>
<p>Finance is the branch of economics concerned with issues such as allocation, management, acquisition, and investment
  of resources. Finance is an element of every organization, including software engineering organizations. The field of
  finance deals with time, money, risk, and how they are interrelated.</p>
<p>To ensure sustainability, an organization must identify organization goals, time horizons, risk factors, tax
  considerations, and financial constraints, identiy and implement the appropriate business strategy, and measure
  financial performance such as cash flow and ROI, and take corrective actions in case of deviation from objectives and
  strategy.</p>
<div class="third">Accounting</div>
<p>Accounting is part of finance, and allows people whose money is being used to run an organization to know the results
  of their investment and figure out if they are getting the profit they were expecting, referring to tangible return on
  investment (ROI) or a business's sustainability (for nonprofits). The primary goal of accounting is to measure the
  organization's actual financial performance and to communicate that information to stakeholders such as shareholders,
  financial auditors, and investors. Communication is generally in the form of financial statements that show in money
  terms the economic resources to be controlled.</p>
<div class="third">Controlling</div>
<p>Controlling is an element of finance and accounting, and involves measuring and correcting the performance of finance
  and accounting. Controlling cost is a special branch used to detect variances of actual costs from planned costs.</p>
<div class="third">Cash flow</div>
<p>Cash flow is the movement of money into or out of a business, project, or financial product over a given period. The
  term cash flow stream refers to set of cash flow instances over time caused by carrying out some given proposal. A
  cash flow diagram is a picture of a cash flow stream.</p>
<div class="third">Decision-making process</div>
<p>A COTS object-request broker product might cost a few thousand dollars, but a homegrown services could easily cost
  several hundred thousand tmies that amount. If the candidate solutions all solve the problem from a technical
  perspective, then selection should be based on commercial factors such as optimizing total cost of ownership (TCO) or
  maximizing short-term return on investment (ROI). In preparing decisions, it is recommended to turn any given set of
  proposals, along with their various interrelationships, into a set of mutually exclusive alternatives.</p>
<div class="third">Valuation</div>
<p>The decision-making process is about maximizing value. A financial basis for value-based comparison is comparing two
  or more cash flows. Several bases of comparison are available including present worth, future worth, annual
  equivalent, internal rate of return, and payback period. Comparing cash flows only makes sense when they are expressed
  in the same time frame.</p>
<div class="third">Inflation</div>
<p>Inflation describes long-term trends in prices, when same things cost more than they did before. The present time
  value needs to be adjusted for inflation rates and for exchange rate fluctuations.</p>
<div class="third">Depreciation</div>
<p>Depreciation involves spreading the cost of a tangible asset across a numbe rof time periods and is used to determine
  how investments in capitalized assets are charged against income over several years. It is an important part of
  determing after-tax cash flow, which is critical for accurately addressing profit and taxes. The depreciation expense
  for each time period is the capitalized cost of developing the software divided across the number of periods in which
  the software will be sold.</p>
<div class="third">Taxation</div>
<p>Companies have to pay income taxes. A proposal with a high pretax profit won't look nearly as poriftable in posttax
  terms. Not accounting for taxation can lead to unrealistically high expectations about profit.</p>
<div class="third">Time-value of money</div>
<p>The value of money changes over time. This is a concept around since the earliest human history and is known as
  time-value. Costs, values, and risk should be normalized to net present value.</p>
<div class="third">Efficiency</div>
<p>Economic efficiency is the ratio of resources actually consumed to resources expected to be consumed. Efficiency
  means "doing things right." Factors influencing efficiency include product complexity, quality requirements, time
  pressure, process capability, team distribution, interrupts, feature churn, tools, and programming language.</p>
<div class="third">Effectiveness</div>
<p>Effectiveness is about having impact. It is the relationship between achieved objectives to defined objects. It means
  "doing the right things." Effectiveness looks only at whether defined objectives are reached and not how they are
  reached.</p>
<div class="third">Productivity</div>
<p>Productivity is the ratio of output over input. Output is value delivered and input is all the resources spent to
  generate the output.</p>
<div class="second">Life cycle economics</div>
<div class="third">Product</div>
<p>A product is an economic good or output that is created in a process that transforms product factors or inputs to an
  output. When sold, a product is a deliverable that creates both a value and an experience for its users. A product can
  be a combination of systems, solutions, materials, and services delivered internally or externally, as-is or as a
  component for another product.</p>
<div class="third">Project</div>
<p>A project is a temporary endeavor undertaken to create a unique product, service, or result. Different project types
  are distinguished (product development, outsourced services, software maintenance, service creation, etc.).</p>
<div class="third">Program</div>
<p>A program is a group of related projects, subprograms, and program activites managed in a coordinated way to obtain
  benefits not available from managing them individually. Programs are often used to identify and manage different
  deliveries to a single customer or market over a time horizon of several years.</p>
<div class="third">Portfolio</div>
<p>Portfolios are projects, programs, subportfolios, and operations managed by a group to achieve strategic objectives.
</p>
<div class="third">Product life cycle</div>
<p>A software product life cycle (SPLC) includes all the activities needed to define, build, operate, maintain, and
  retire a software product or service and its variants. The SPLC activities of operate, maintain, and retire typically
  occur in a much longer time frame and consume more total effort and resources.</p>
<div class="third">Project life cycle</div>
<p>Project life cycle activites typically involve five process groups: initiating, planning, executing, monitoring and
  controlling, and closing. The software project life cycle and software product life cycle are interrelated. An SPLC
  may contain several SDLCs.</p>
<div class="third">Proposals</div>
<p>Proposals are the start of a business decision and relate to reaching a business objective. A proposal is a single,
  separate option that is being considered, like carrying out a particular project or not. The whole purpose of business
  decision-making is to figure out which proposals should be carried out and which shouldn't.</p>
<div class="third">Investment decisions</div>
<p>Investors, either inside (finance, board), or outside (banks) make investment decisions to spend money and resources
  on achieving a target objective.</p>
<div class="third">Planning horizon</div>
<p>Frozen assets are when an organization chooses to invest in a particular proposal and money gets tied up in that
  proposal. Operating and maintenance costs of that proposal tend to start low but increase over time. The total cost of
  the proposal is the sum of operating and maintenance costs. Early on, frozen asset costs dominate; later, operating
  and maintenance costs dominate. When the sum of the costs is minimized, it is the minimum cost lifetime. The planning
  horizon, sometimes known as the study period, is the consistent time frame over which proposals are considered.</p>
<div class="third">Price and pricing</div>
<p>A price is what is paid in exchange for a good or service. The four Ps of the marketing mix is price, product,
  promotion, and place. Price is the only revenue-generating element among the four Ps and the rest are costs. Pricing
  is an element of finance and marketing, and is the process of determining what a company will receive in exchange for
  its products.</p>
<div class="third">Cost and costing</div>
<p>A cost is the value of money that has been used up to produce something. A cost is an alternative that is given up as
  a result of a decision. A sunk cost is the expenses before a certain time, which can cause emotional hurdles in
  looking forward. From a traditional economics point of view, sunk costs should not be considered in decision making.
  Opportunity cost is the cost of an alternative that must be forgone in order to pursue another alternative. Costing is
  part of finance and is the process to determine the cost based on expenses and on the target cost to be competitive
  and successful in a market. The planning and controlling of these costs is cost management. Total cost of ownership
  (TCO) is the total cost for acquiring, activating, and keeping a product running, grouped as direct and indirect
  costs. TCO is an accounting method crucicial in making sound economic decisions.</p>
<div class="third">Performance measurement</div>
<p>Performance measurement is the process whereby an organization establishes and measures the parameters used to
  determine whether programs, investments, and acquisitions are achieving the desired results.</p>
<div class="third">Earned value management</div>
<p>Earned value management (EVM) is a project management technique for measuring progress based on created value. At a
  given moment, the results achieved to date in a project are compared with the projected budget and the planned
  schedule progress for that date. A key principle in EVM is tracking cost and schedule variances via comparison of
  planned versus actual schedule and budget versus actual cost.</p>
<div class="third">Termination decisions</div>
<p>Termination mneans to end a project or pdocut, and can be preplanned or can come spontaneously. Sunk costs should not
  be considered in such decision making because they have been spent and will not reappear as value.</p>
<div class="third">Replacement and retirement decisions</div>
<p>A replacement decision is made when an organization already has a particular asset and they are considering replacing
  it with something else, such as supporting a legacy software product or redeveloping it from the ground up.
  Replacement decisions are similar except also consider sunk cost and salvage value. Retirement decisions are about
  getting out of an activity altogether. These can be influenced by lock-in factors such as technology dependency and
  high exit costs.</p>
<div class="second">Risk and uncertainty</div>
<div class="third">Goals, estimates, and plans</div>
<p>Goals in software engineering economics are mostly business goals, which relates business needs to investign
  resources. An estimate is a well-founded evaluation of resources and time that will be needed to achieve stated goals,
  and are typically internally generated and not necessariy visible externally. A plan describes the activities and
  milestones necessary to reach the goals of a project.</p>
<div class="third">Estimation techniques</div>
<p>Estimations consist of five families of techniques: expert judgment, analogy, estimation by parts, parametric methds,
  and statistical methods. Convergence among estimates produced by different techniques suggests accuracy.</p>
<div class="third">Addressing uncertainty</div>
<p>Estimates are inherently uncertain. Techniques for addressing uncertainty include considering ranges of estimates,
  analyzing sensitivity to changes of assumptions, and delaying final decisions.</p>
<div class="third">Prioritization</div>
<p>Prioritization involves ranking alternatives based on common criteria to deliver the best possible value.</p>
<div class="third">Decisions under risk</div>
<p>Decisions under risk techniques are used when the decision maker can assign probabilities to the different possible
  outcomes. Specific techniques include expected value decision making, expctation variance and decision making, Monte
  Carlo analysis, decision trees, and expected value of perfect information.</p>
<div class="third">Decisions under uncertainty</div>
<p>Decisions under uncertainty techniques are used when the decision maker cannot assign probabilities to the different
  possible outcomes because the needed information is not available. Specific techniques inclue the Laplace rule,
  Maximin rule, Maximax rule, Hurwicz rule, and Minimax Regret rule.</p>
<div class="second">Economic analysis methods</div>
<div class="third">For-profit decision analysis</div>
<p> For-profit decision analysis depends on ROI or Return on Capital Employed (ROCE). For-profit decision techniques
  don't apply for government and nonprofit organizations. In these cases, organizations have different goals.</p>
<div class="third">Minimum acceptable rate of return</div>
<p>The minimum acceptable rate of return (MARR) is the lowest internal rate of return the organization would consider to
  be a good investment. The MARR is a statement that an organization is confident it can achieve at least that rate of
  return.</p>
<div class="third">Return on investment</div>
<p>Return on investment (ROI) is a measure of profitability of a company or business unit, defined as the ratio of money
  gained or lost on an investment relative to the amount of money invested.</p>
<div class="third">Return on capital employed</div>
<p>The return on capital employed (ROCE) is a measure of the profitability of a company or business unit, defined as the
  ratio of a gross profit before taxes and interest (EBIT) to the total assets minus current liabilities. It describes
  the return on the used capital.</p>
<div class="third">Cost-benefit analysis</div>
<p>Cost-benefit analysis evalutes individual proposals. Any proposal with a benefit-cost ratio of less than 1 can
  usually be rejected without further analysis.</p>
<div class="third">Cost-effectiveness analysis</div>
<p>Cost-effectiveness analysis is similar to cost-benefit analysis. There are two versions: fixed-cost version that
  maximizes the benefit given some upper bound on cost, and fixed-effectiveness version minimizes cost needed to achieve
  a fixed goal.</p>
<div class="third">Break-even analysis</div>
<p>Break-even analysis identifes the point where the costs of developing a product and the revenue to be generated are
  equal. Given estimated costs and revenues of two or more proposals, break-even analysis helps in choosing among them.
</p>
<div class="third">Business case</div>
<p>The business case is the consolidated information summarizing and explaining a business proposal from different
  perspectives for a decision maker, often used to assess the potential value of a product, and can be used as a basis
  in the investment decision-making process.</p>
<div class="third">Multiple attribute evaluation</div>
<p>The alternative to making decisions based on money is to make decisions based on other attributes that factor in
  nonfinancial criteria. One family of multiple attribute decision techniques is compensatory, which collaps all
  attributes into a single figure of merit. This family is called compensatory, or single-dimensioned, because a lower
  score in one attribute can compensate for a higher score in another. Compensatory techniques include nondimensional
  scaling, additive weighting, and analytic hierarchy process. The other family is noncompensatory, or fully
  dimensioned, which does not allow tradeoffs, and each attribute is treated as a separate entity. The noncompensatory
  techniques include dominance, satisficing, and lexicogrpahy.</p>
<div class="third">Optimization analysis</div>
<p>Optimization analysis studies a cost function over a range of values to find the point where overall performance is
  best. Software's classic space-time tradeoff is an example of optimization. An algorithm that runs faster will often
  use more memory. Optimization balances a faster runtime against the cost of additional memory. Real options analysis
  can be used to quantify the value of project choices, including the value of delaying a decision. Such options are
  difficult to compute with precision.</p>
<div class="second">Practical considerations</div>
<div class="third">The "good enough" principle</div>
<p>Often software engineering projects and products are not precise about the targets. Software requirements are stated,
  but the marginal value of adding a bit more functionality cannot be measured. The result could be late or too-high
  cost. The "good enough" principle relates marginal value to marginal cost and provides guidance to determine criteria
  when a deliverable is "good enough" to be delivered.</p>
<p>The RACE principle (reduce accidents and control essence) is a popular rule towards good enough software. Accidents
  imply unnecessary overheads such as gold-plating and rework due to late defect removal or too many requirements
  changes. Essence is what customers pay for. Insufficient quality or insufficient quantity is not good enough. Agile
  methods are one example of trying to reduce gold plating and delayed rework.</p>
<div class="third">Friction-free economy</div>
<p>Economic friction is everything that keeps markets from having perfect competition. It involves distance, cost of
  delivery, restrictive regulations, and/or imperfect information. In high-friction markets, customers don't have many
  suppliers from which to choose. It's hard for new competitors to start business and compete. Friction-free markets are
  the reverse. New competitors emerge and customers are quick to respond, and the marketplace is unpredictable.
  Theoretically, software and IT are friction-free. New companies can easily create products and often do so at a much
  lower cost than established companies, since they need not consider any legacies. Marketing and sales can be done via
  Internet and social networks, and basically free distribution mechanisms can enable a ramp up to a globla business.
  Competition among software app developers is inhibited when apps must be sold through an app store and comply with a
  store's rules.</p>
<div class="third">Ecosystems</div>
<p>An ecosystem is an environment consisting of all the mutually dependent stakeholders, business units, and companies
  working in a particular area. In a typical ecosystem, there ar eproducers and consumers. Note that a consumer is not
  the end user but an organization that uses the product to enhance it. A software ecosystem is, for instance, a
  supplier of an application working with companies doing the installation and support in different regions. Neither
  could exist without the other.</p>
<div class="third">Offshoring and outsourcing</div>
<p>Offshoring means executing a business activity beyond sales and marketing outside the home country of an enterprise,
  usually in low-cost countries or from specialized companies. Offshoring within a company is called captive offshoring.
  Outsourcing is the result-oriented relationship with a supplier who executes business activities for an enterprise
  when, traditionally, those activities were executed inside the enterprise. Outsourcing is site-independent.
  Outsourcing might reduce cost per hour of software development but might increase number of hours and capital expenses
  due to increased need for monitoring and communication.</p>

<div class="first">Computing foundations</div><a class="anchor" id="computing_foundations"></a>
<p>Because no software can exist in a vacuum or run without a computer, the core of such an environment is the computer.
  It is generally accepted that software engineering builds on top of computer science and mathematics. Software
  engineering is concerned with the application of computers, computing and software to practical purposes.</p>
<div class="second">Problem solving techniques</div>
<div class="third">Definition of problem solving</div>
<p>Problem solving refers to thinking and activities conducted to answer or derive a solution to a problem. A general
  guideline is to formulate a real problem, analyze the problem, and design a solution search strategy.</p>
<div class="third">Formulating the real problem</div>
<p>The problem statement specifies what the problem and the desired outcome are. Some general techniques include
  statement-restatement, determining the source and the cause, revising the statement, analyzing present and desired
  state, and using the fresh eye approach.</p>
<div class="third">Analyze the problem</div>
<p>The next step is to analyze the problem statement to help structure our search for a solution. Four types of analysis
  include situation analysis, in which most urgent or critical aspects are identified first, problem analysis, in which
  cause of problem is determined, decision analysis, in which the action needed to correct the problem must be
  determined, and potential problem analyais, in which action needed to prevent reoccurrences of the problem must be
  determined.</p>
<div class="third">Design a solution search strategy</div>
<p>The "best" solution could mean faster, cheaper, more usable, different capabilities, etc. Eliminate paths that do not
  lead to viable solutions, design tasks in a way that provide the most guidance in searching for a solution.</p>
<div class="third">Problem solving using programs</div>
<p>To solve a problem using computers, we need to figure out what to tell the computer to do, how to convert the problem
  statement into an algorithm, and how to convert the algorithm into machine instructions. In general, a problem should
  be expressed in a way to facilitate the development of algorithms and data structures. Once an algorithm is found, the
  final step converts the algorithm into machine instructions to form software.</p>
<p>Abstractly speaking, problem solving using a computer can be considered a process of problem transformation of a
  problem statement into a problem solution, to transform a problem expressed in natural language into electrons running
  around a circuit. This transformation is broken down into 1) development of algorithms from the problem statement, 2)
  application of algorithms to the problem, and 3) transformation of algorithms to program code. This process usually
  follows a "stepwise refinement" or systematic decomposition in which a problem statement is rewritten as a task and
  recursively decomposed into simpler subtasks. Three basic ways of decomposing are sequential, conditional, and
  iterative.</p>
<div class="second">Abstraction</div>
<p>Abstraction refers to generalizing by reducing information of a concept, problem, or observable phenomenon, viewing
  the problem from a higher level of conceptual understanding.</p>
<div class="third">Levels of abstraction</div>
<p>When abstracting, we concentrate on one level of the big picture at a time with confidence that we can then connect
  effectively with levels above and below. Abstraction levels correspond to well-defined standard interfaces such as
  programming APIs.</p>
<div class="third">Encapsulation</div>
<p>Encapsulation is a mechanism used to implement abstraction. When we are dealing with one level of abstraction, the
  information below and above are encapsulated. Encapsulation usually comes with some degree of information hiding.</p>
<div class="third">Hierarchy</div>
<p>Most of the time, different levels of abstraction are organized in a hierarchy. Sometimes, a hierarchy of abstraction
  is sequential, sometimes it has a tree-like structure, or a many-to-many structure. At no time shall there be a loop
  in a hierarchy. A hierarchy forms naturally in task decomposition.</p>
<div class="third">Alternate abstractions</div>
<p>Alternate abstractions can complement each other in helping understanding the problem and its solution.</p>
<div class="second">Programming fundamentals</div>
<p>Programming is composed of the methodologies or activities for creating computer programs that perform a desired
  function, an indispensable part in software construction, and is considered the process of designing, writing,
  testing, debugging, and maintaining source code written in a programming language. Writing source code often requires
  expertise in areas such as application domain, appropriate data structures, specialized algorithms, various language
  constructs, good programming techniques, and software engineering.</p>
<div class="third">The programming process</div>
<p>Programming involves design, writing, testing, debugging, maintenance. Design is conception or invention of a scheme
  for turning a customer requirement for computer software into operational software, linking application requirements
  to coding and debugging. Writing is the actual coding of the design in an appropriate programming language. Testing is
  the activity to verify that the code one writes actually does what it is supposed to do. Debugging finds and fix bugs
  (faults) in the source code or design. Maintenance updates, corrects, and ehnances existing programs.</p>
<div class="third">Programming paradigms</div>
<p>Various programming paradigms have been developed to provide standardization to the highly creative and personal
  activity of programming. Unstructured programming involves programmers following his/her hunch to write code as long
  as the function is operational, also known as ad hoc programming. Structured/procedural/imperative programming uses
  well-defined control structures, including procedures and/or functions with each performing a specific task.
  Interfaces exist between procedures to facilitate correct and smooth calling operations of the programs.
  Object-oriented programming (OOP) organizes a program around objects, which are abstract data structures that combine
  both data and methods used to access or manipulate the data. Aspect-oriented programming (AOP) is a program paradigm
  built on top of OOP which aims to isolate secondary or supporting functions from the main program's business logic by
  focusing on the cross sections (concerns) of the objects. The motivation for AOP is to resolve object tangling and
  scattering, emphasizing the separation of concerns. Functional programming treats all computations as the evaluation
  of mathematical functions, and avoids state and mutable data.</p>
<div class="second">Programming language basics</div>
<div class="third">Programming language overview</div>
<p>A programming language is designed to express computations that can be performed by a computer. It is a notation for
  writing programs and thus should be able to express most data structures and algorithms. Some people restrict
  programming languages to those languages that can express all possible algorithms. The most popular languages are
  often defined by a specification document established by a well-known and respected organization. For example, the C
  programming language is specified by an ISO standard named ISO/IEC 9899. Other languages such as Perl and Python often
  have a dominant implementation used as a reference instead.</p>
<div class="third">Syntax and semantics of programming languages</div>
<p>Programming languages typically have some form of written specification of their syntax (form) and semantics
  (meaning). Such specifications may include specific requirements for the definition of variables and constants (in
  other words, declarations and types) and format requirements for instructions themselves. In general, a programming
  language supports constructs such as variables, data types, constants, literals, assignment statements, control
  statements, procedures, functions, and comments.</p>
<div class="third">Low-level programming languages</div>
<p>Low-level languages can be understood by a computer with no or minimal assistance and typically include machine
  languages and assembly languages. A machine language uses ones and zeros to represent instructions and variables. An
  assembly language contains same instructions as a machine language but the instructions and variables have symbolic
  names easier for humans to remember. Assembly languages cannot be directly understood by a computer and must be
  translated into a machine language by a utility program called an assembler. There often exists a correspondence
  between instructions of an assembly language and a machine language. For example, "add r1, r2, r3" is an assembly
  instruction for adding r2 and r3 and storing the sum into register r1. This instruction can be translated to machine
  code "0001 0001 0010 0011" assuming the proper operation codes are used. One common trait is their close association
  with the specifics of a type of computer or instruction set architecture (ISA).</p>
<div class="third">High-level programming languages</div>
<p>A high-level programming language has a strong abstraction from the detail of the computer's ISA. It often uses
  natural-language lemeents and is thus much easier for humans to understand. While each microprocessor has its own ISA,
  code written in a high-level programming language is usually portable between many different hardware platforms. Most
  programmers use and most software are written in high-level programming languages, such as C, C++, C# ,and Java.</p>
<div class="third">Declarative vs. imperative programming languages</div>
<p>Most programming languages are imperative, which involve the programmer specifying every step clearly to the
  computer. Declarative langauges allow programmers to describe the function without specifying exact instruction
  sequences, and are high-level languages. The actual implementation of computation in a declarative language is hidden
  from programmers. Declarative programming only describes what the program should accomplish without describing how to
  accomplish it. Declarative programming languages include Lisp (also a functional programming language) and Prolog,
  while imperative programming languages include C, C++, and Java.</p>
<div class="second">Debugging tools and techniques</div>
<p>Debugging is a methodical process of finding and reducing the number of bugs or faults in a program.</p>
<div class="third">Types of errors</div>
<p>Logical errors and data errors are known as two categories of "faults". Syntax errors are an error that prevents the
  translator (compiler/interpreter) from successfully parsing the statement. In high-level programming languages, syntax
  errors are caught during compilation or translation from the high-level language into machine code. Logic errors are
  semantic errors that result in incorrect computations or program behaviors. Your program is legal, but wrong. Logic
  errors are usually not machine checkable. Data errors are input errors that result either in input data that is
  different from what the program expects or in the processing of wrong data.</p>
<div class="third">Debugging techniques</div>
<p>Static debugging usually takes the form of code review, while dynamic debugging usually takes the form of tracing and
  is closely associated with testing. Postmortem debugging is the act of debugging the core dump (memory dump) of a
  process, which is generated after a process has terminated due to an unhandled exception. The three ways to trace a
  program is single-stepping (one instruction at a time), breakpoints (stopping at specific instructions), and watch
  points (tell the program to stop when a register or memory location changes or when it equals to a specific value).
  Watch points are useful when one doesn't know when a value is changed.</p>
<div class="third">Debugging tools</div>
<p>Debuggers are widely used to enable the programmer to monitor the execution of a program, stop the execution, restart
  the execution, set breakpoints, change values in memory, and even, in some cases, go back in time. There are also many
  static code analysis tools which look for specific set of known problems within source code. The UNIX lint program is
  an example of a static code analysis tool.</p>
<div class="second">Data structure and representation</div>
<div class="third">Data structure overview</div>
<p>Data structures are computer representations of data and are used in almost every program. Fundamentally, data
  structures are abstractions defined on a collection of data and its associated operations. Examples of data structures
  designed for algorithm efficiency are stacks, queues ,and heaps. Data structures can also be used for conceptual unity
  such as the name and address of a person. A data structure is either linear or nonlinear, and can have classifications
  such as homogeneous vs. heterogeneous, static vs. dynamic, persistent vs. transient, external vs. internal, primitive
  vs. aggregate, recursive vs. nonrecursive, passive vs. active, and stateful vs. stateless.</p>
<div class="third">Types of data structure</div>
<p>Linear structures organize data items in a single dimension in which each data entry has one predecessor and one
  successor with the exception of first and last entry. Nonlinear structures organize data items in two or more
  dimensions. Examples of linear structures include lists, stacks, and queues. Examples of nonlinear structures include
  heaps, hash tables, and trees (binary trees, balance trees, B-trees, and so forth). A compound data structure builds
  on top of other more primitive data structures. Examples of compound structures are sets, graphs, and partitions (eg.
  a partition is a set of sets).</p>
<div class="third">Operations on data structures</div>
<p>Basic operations supported by all data structures include create, read, update and delete (CRUD). Create inserts a
  new data entry into the structure. Read retrieves a data entry from the structure. Update modifies an existing data
  entry. Delete removes a data entry from the structure. Some additional operations include finding a particular
  element, sorting all elements, traversing all elements in some order, and reorganizing or rebalancing the structure.
  Different structures support different operations with different efficiencies. For example, it is easy to retrieve the
  last item inserted into a stack, but finding a particular element within a stack is rather slow and tedious.</p>
<div class="second">Algorithms and complexity</div>
<div class="third">Overview of algorithms</div>
<p>Abstractly speaking, algorithms guide the operations of computers and consist of a sequence of actions composed to
  solve a problem. Some agreement exists that an algorithm needs to be correct, finite (will terminate eventually) and
  unambiguous.</p>
<div class="third">Attributes of algorithms</div>
<p>The attributes of algorithms include modularity, correctness, maintainability, functionality, robustness,
  user-friendliness, programmer time, simplicity, and extensibility. Performance or efficiency discuss time and
  resource-usage. Efficiency determines if an algorithm is practical or not. An algorithm that takes one hundred years
  to terminate is virtually uesless and is even considered incorrect.</p>
<div class="third">Algorithmic analysis</div>
<p>Analysis of algorithms is the theoretical study of computer-program performance and resource usage and can determine
  goodness of an algorithm. Worst-case analysis determines the maximum time or resources required by the algorithm on
  any input of size n. In average-case analysis, one determines time or resources required over all inputs of size n,
  needing to make an assumption on the statistical distribution of inputs. Best-case analysis determines minimum time or
  resources required on any input of size n. Average-case analysis is most relevant but also the most difficult to
  perform.</p>
<p>Amortized analysis determines the maximum time required by an algorithm over a sequence of operations, and
  competitive analysis determines relative performance merit of an algorithm against the optimal algorithm in the same
  category.</p>
<div class="third">Algorithmic design strategies</div>
<p>Brute force, divide and conquer, dynamic programming, and greedy selection are strategies for designing algorithms.
  Brute force strategy is a no-strategy, exhaustively trying every possible way to tackle a problem. The divide and
  conquer strategy divides a big problem into smaller, homogeneous problems, recursively solving smaller problems and
  combining the solutions to solve the bigger problem. Dynamic programming eliminates redundant subproblems to improve
  efficiency. Greedy selection recognizes that not all sub-problems contribute to the solution and eliminates all but
  one sub-problem. Randomization can improve on greedy selection by eliminating complexity in determining the greedy
  choice through coin flipping or randomization.</p>
<div class="third">Algorithmic analysis strategies</div>
<p>Basic counting analysis counts the number of steps an algorithm takes to complete the task. Asymptotic analysis only
  considers the order of magnitude of the number of steps. Probabilistic analysis uses probabilities to analyze average
  performance of an algorithm. Amortized analysis uses aggregation, potential, and accounting to analyze the worst
  performance of an algorithm on a sequence of operations. Competitive analysis uses potential and accounting to analyze
  the relative performance of an algorithm to an optimal algorithm.</p>
<div class="second">Basic concept of a system</div>
<p>A system can be simple like an ink pen or complex like an aircraft. Systems can be divided into technical
  computer-based systems and sociotechnical systems. A sociotechnical system requires humans (eg. manned space
  vehicles), whereas a technical computer-based system functions without human involvement.</p>
<div class="third">Emergent system properties</div>
<p>Emergent properties develop after integration of constituent parts, and can be functional (describes what a system
  does) or nonfunctional (describes how a system behaves).</p>
<div class="third">Systems engineering</div>
<p>Systems engineering is the interdisciplinary approach governing the total technical and managerial effort
  required to transform a set of customer needs, expectations, and constraints into a solution and to support that
  solution throughout its life.</p>
<div class="third">Overview of a computer system</div>
<p>A computer is a machine that executes programs or software, consisting of a purposeful collection of mechnical,
  electrical, and electronic componnets. A computer receives some input, stores and manipulates some data, and provides
  some output. The most distinct feature of a computer is its ability to store and execute sequences of instructions
  called programs. Computers have universal equivalence in functionality. Given enough time and memory, all computers
  are capable of computing exactly the same things.</p>
<p>Most computer systems have a structure known as the von Neumann model. A memory for storing instructions and data, a
  central processing unit for performing arithmetic and logical operations, a control unit for sequencing and
  interpreting instructions, input for getting external information into the memory, and output for producing results
  for the user.</p>
<img src="https://neuralhuborg.s3.amazonaws.com/img/vonneumann.png">
<div class="second">Computer organization</div>
<p>The gap between a computer's intended behavior and the underlying electronic devices is the computer organization,
  which meshes various electrical, electronic, and mechanical devices into one device that forms a computer.</p>
<div class="third">Computer organization overview</div>
<p>A computer generally consists of a CPU, memory, input devices, and output devices. Abstractly, the organization of a
  computer can be divided into four levels. The macro architecture level is the formal specification of all the
  functions a particular machine can carry out and is known as the instruction set architecture. The micro architecture
  is the implementation of the ISA in a specific CPU. The logic circuits level is the level where each functional
  component of the micro architecture is built up of circuits that make decisions based on simple rules. The devices
  level is the level where each logic circuit is actually built of electronic devices such as complementary metal-oxide
  semiconductors (CMOS), n-channel metal oxide semiconductors (NMOS), or gallium arsenide (GaAs) transistors, etc.</p>
<p>To a programmer, the most important level of abstraction is the ISA, which specifies such things as native data
  types, instructions, registers, addressing modes, memory architecure, interrupt and exception handling, and the I/Os.
  Overall, the ISA specifies the ability of a computer and what can be done on the computer with programming.</p>
<div class="third">Digital systems</div>
<p>The computer uses circuits and memory to hold charges that represents the presence or absence of voltage. The
  presence of voltage is equal to 1 while the basence of voltage is a zero. Everything is expressed or encoded using
  digital zeros and ones. In this sense, a computer becomes a digital system. For example, the decimal value 6 can be
  encoded as 110, the addition struction may be encoded as 0001, and so forth.</p>
<div class="third">Digital logic</div>
<p>The logic behind a computer is called digital logic because it deals with operations of digital zeros and ones. For
  example, digital logic spells out what the value will be if a zero and one is ANDed, ORed, or exclusively ORed
  together. It also specifies how to build decoders, multiplexers (MUX), memory, and adders used to assemble the
  computer.</p>
<div class="third">Computer expression of data</div>
<p>A computer is a binary expression system, and so the maximum numerical value expressible by an n-bits binary code is
  \(2^n-1\). Specifically, binary number \(a_na_n-1...a_1a_0\) corresponds to
  \(a_n\times2^n+a_{n-1}\times2^{n-1}+...+a_1\times2^1+a_0\times2^0\). Thus, the numerical value of the binary
  expression of 1011 is \(1\times8+0\times4+1\times2+1\times1=11\). To express a nonnumerical value, we need to decide
  the number of zeros and ones to use and the order in which those zeros and ones are arranged. Integers can be
  expressed in the form of unsigned, one's complement, or two's complement. For characters, there are ASCII, Unicode,
  and IBM's EBCDIC standards. For floating point numbers, there are IEEE-754 FP 1, 2, and 3 standards.</p>
<div class="third">The central processing unit (CPU)</div>
<p>The central processing unit is the place where instructions are actually executed. The executation usually takes
  several steps, including fetching the program instruction, decoding the instruction, fetching operands, performing
  arithmetic and logical operations on the operands, and storing the result. The main components of a CPU consist of
  registers where instructions and data are often read from and written to. The arithmetic and logic unit (ALU) that
  performs the actual arithmic and logic, the control unit responsible for producing proper signals to control the
  operations, and various buses that link the components together and transport data to and from these components.</p>
<div class="third">Memory system organization</div>
<p>Memory is the storage unit of a computer. It concerns the assembling of a large-scale memory system from smaller and
  single-digit storage units. The main topics covered by memory system architecture are memory cells and chips
  (single-digital storage and assembling of single-digit units into one-dimensional memory arrays and the assembling of
  one-dimensional storage arrays into multi-dimensional storage memory chips), memory boards and modules (assembling of
  memory chips into memory systems), memory hierarchy and cache (supports efficient memory operations), and memory as a
  subsystem of the computer (interface between the memory system and other parts of the computer).</p>
<div class="third">Input and output (I/O)</div>
<p>Common input devices include keyboard and mouse, and common ouput devices include disk, screen, printer, speakers.
  I/O is how computers connect and manage various inputs and output devices to facilitate interaction between computers
  and humans. Hardware I/O includes dedicated I/O (dedicates the CPU to the actual input and ouput operations during
  I/O), memory-mapped I/O (treats I/O operations as memory operations), and hybrid I/O (combines the two into a single
  holistic I/O operation mode).</p>
<p>Software I/O can be programmed I/O (lets the CPU wait while the I/O device is using I/O), interrupt-driven I/O (lets
  the CPU's handling of I/O be driven by the I/O device), and direct memory access DMA (lets I/O be handled by a
  secondary CPU embedded in a DMA device).</p>
<p>The main issues involved in I/O include I/O addressing (how to identify the I/O device for a specific I/O operation),
  synchronization (how to make the CPU and I/O device work in harmony), and error detection and correction (deals with
  the occurrence of transmission errors).</p>
<div class="second">Compiler basics</div>
<div class="third">Compiler/interpreter overview</div>
<p>The CPU converts high level language code written by programmers into machine code using a compiler or an interpreter
  in the process of compilation or interpretation.</p>
<div class="third">Interpretation and compilation</div>
<p>Interpretation translates the source code one statement at a time into machine language and executes it on the spot,
  then goes back for another statement. Compilation translates the high-level-language source code into an entire
  machine-language program (an executable image) by a program called a compiler. After compilation, only the executable
  image is needed to run the program. Most application software is sold in compilation form. A compiler makes the
  conversion just once, while an interpreter typically converts every time a program is executed. Interpreting code is
  slower than running compiled code, and interpreters are slower at accessing variables.</p>
<p>The primary tasks of a compiler may include preprocessing, lexical analysis, parsing, semantic analysis, code
  generation, and code optimization. Program fualts caused by incorrect compiler behavior can be very difficult to track
  down, so compiler implementers invest a lot of time ensuring the correctness of their software.</p>
<div class="third">The compilation process</div>
<p>Most compilers break down compilation into lexical analysis, syntax analysis or parsing, semantic analysis, and code
  generation.</p>
<p>Lexical analysis partitions the input text (the source code), which is a sequence of characaters, into separate
  comments, which are to be ignored in subsequent actions, and basic symbols, which have lexical meanings. These basic
  symbosl must correspond to some terminal symbols of the grammar of the particular programmign language.</p>
<p>Syntax analysis is based on the results of lexical anlaysis and discovers the structure in the program and determines
  whether or not a text conforms to an expected format. Syntax analysis determines if source code is correct and
  converts it into a more structured representation (parse tree) for semantic analysis or transformation.</p>
<p>Semantic analysis adds semantic information to the parse tree built during the syntax analysis and builds the symbol
  table. It performs various semantic checks including type checking, object binding (associating variable and function
  references with their definitions), and definite assignment (requiring all local variables to be initialized before
  use).</p>
<p>Once semantic analysis is complete, code generation transforms intermediate code produced into the machine machine
  language of the computer under consideration. This involves resource and storage decisions, such as deciding which
  variables to fit into registers and memory and the selection and scheduling of appropriate machine instructions, along
  with their associated addressing modes.</p>
<div class="second">Operating system basics</div>
<p>A computer as a complex electrical-mechanical system has a manager for managing the resources and activities
  occurring on it, called the operating system (OS).</p>
<div class="third">Operating systems overview</div>
<p>Operating systems is a collection of software and firmware that controls the execution of computer programs and
  provides services such as computer resource allocation, job control, input/output control, and file management.
  Conceptually, an operating system is a computer program that manages hardware resources and makes it easier to use by
  applications by presenting nice abstractions. This abstraction is often called the virtual machine and includes such
  things as processes, virtual memory, and file systems. An OS hides the complexity of the underlying hardware and is
  found on all modern computers. The principal roles of OSs are management (allocation and recovery) (management of
  physical resources among
  multiple competing users/applications/tasks) and illusion (nice abstractions provided).</p>
<div class="third">Tasks of an operating system</div>
<p>Modern operating systems have tasks including CPU management, memory management, disk management (file system), I/O
  device management, and security and protection. CPU management deals with allocation and releases of the CPU among
  competing programs (called processes/threads in OS jargon), including the operating system itself. The main
  abstraction provided by CPU management is the process/thread model. Memory management deals with allocation and
  release of memory space among competing processes, and the main abstraction for memory management is virtual memory.
  Disk management deals with sharing of magnetic or optical or solid state disks among multiple programs/users and its
  main abstraction is the file system. I/O device management deals with allocation and releases of various I/O devices
  among competing processes. Security and protection deal with the protection of computer resources from illegal use.
</p>
<div class="third">Operating system abstractions</div>
<p>OSs use five abstractions: process/thread, virtual memory, file systems, input/output, and protection domains. The
  overall OS abstraction is the virtual machine. For each task area of OS, there is both a physical reality and a
  conceptual abstraction. The physical reality refers to the hardware resource under management; the conceptual
  abstraction refers to the interface the OS presents to the users/programs above. In the thread model, the physical
  reality is the CPU and the abstraction is multiple CPUs. Thus, a user doesn't have to work about sharing the CPU with
  others when working on the abstraction provided by an OS. In the virtual memory abstraction, the physical reality is
  the physical RAM or ROM and the abstraction is multiple unlimited memory space so the user doesn't have to worry about
  sharing physical memory with others. In this context, virtual applies to something that appears to be there but isn't,
  and transparent means something is there but appears not to be.</p>
<div class="third">Operating systems classification</div>
<p>From a historical perspective, an operating system can be classified as a batching OS (organizes and processes work
  in batches such as IBM's FMS, IBSYS, and University of Michigan's UMES), multiprogrammed batching OS (adds multitask
  capability such as IBM's OS/360), time-sharing OS (adds multi-task and interactive capabilities such as UNIX, Linux,
  and NT), real-time OS (adds timing predictability by scheduling individual tasks according to each task's completion
  deadlines such as VxWorks WindRiver and DART EMC), distributed OS (adds managing a network of computers), and embedded
  OS (used for embedded systems such as cars and PDS such as Palm OS, Windows CE, and TOPPER).</p>
<p>An OS can also be classified by its applicable target machine/environment, such as into mainframe OS (runs on the
  mainframe computers such as OS/360, OS/390, AS/400, MVS and VM), server OS (runs on workstations or servers such as
  UNIX, Windows, Linux, VMS), multicomputer OS (runs on multiple computers such as Novell Netware), personal computers
  (runs on personal computers such as DOS, Windows, Mac OS, Linux), and mobile device OS (runs on personal devices such
  as iOS, Android, Symdbian).</p>
<div class="second">Database basics and data management</div>
<p>A database consists of an organized collection of data for one or more uses. A database is usually external to
  programs and permanent in existence compared to data structures. Databasea are used when the data volume is large or
  logical relations between data items are miportant. The factors considered in database design include performance,
  concurrency, integrity, and recovery from hardware failures.</p>
<div class="third">Entity and schema</div>
<p>Entities are things a database tries to model and store. An entity can be real-world objects, and can be primitive
  such as a name, or a composite such as an employee that has a name, number, salary etc. The single most important
  concept in a database is the schema, a description of the entire database structure from which all other database
  activities are built. A schema defines the relationships between various entities. The database model describes the
  type of relationship among various entities, such as relational, network, and object models.</p>
<div class="third">Database management systems (DBMS)</div>
<p>Database Management System (DBMS) components include database applications for the storage of structured and
  unstructured data and the required functions needed to view, collect, store, and retrieve data. A DBMS controls the
  creation, maintenance, and use of the database and is categorized acording to the database model it supports. A
  relational database management system (RDBMS) implements features of a relational model, while an object database
  management system (ODBMS) implements features of the object model.</p>
<div class="third">Database query language</div>
<p>Users/applications interact with a database through a databaes query language. One commonly used query language for
  the relational database is the structured query language (SQL). A common query language for object databases is object
  query language (OQL). SQL is made of Data Definition Language (DDL), Data Manipulation Language (DML), and Data
  Control Language (DCL). An example of an DML query may look like the following:</p>
<pre>
  SELECT Component_No, Quantity
  FROM COMPONENT
  WHERE Item_No = 100
</pre>
<p>The above query selects all the Component_No and its corresponding quantity from a database table called COMPONENT,
  where the Item_No equals to 100.</p>
<div class="third">Tasks of DBMS packages</div>
<p>Database development defines and organizes content, relationships, and structure of the data needed to build a
  database.</p>
<p>Database interrogation accesses data in a database for information retrieval and report generation. This is the
  operation most users know about databases.</p>
<p>Database maintenance adds, deletes, updates, and corrects data in a database.</p>
<p>Application development develops prototypes of data entry screens, queries, forms, reports, tables, and labels for a
  prorotyped application. It also refers to the use of 4th Generation Language or application generators to develop or
  generate program code.</p>
<div class="third">Data management</div>
<p>In a relational model, data are organized as tables with different tables representing different entities or
  relations among a set of entities. The storage of data deals with the storage of these databaes tables on disks. The
  common ways for achieving this is to use files. Sequential, indexed, and hash files are all used in this purpose.</p>
<div class="third">Data mining</div>
<p>To take full advantage of a database (instead of "pinpointing" by looking for specific parts of the database), one
  can perform statistical analysis and pattern discovery on the content of a
  database using a technique called data mining, which is used to support a number of business activities such as
  marketing, fraud detection, and trend analysis. Numerous ways to perform data mining have been invented in the past
  decade and include techniques such as class description, class discrimination, cluster analysis, association analysis,
  and outlier analysis.</p>
<div class="second">Network communication basics</div>
<p>A computer network connects a collection of computers and allows users of different computers to share resources with
  other users. A network gives the illusion of a single, omnipresent computer. Every device connected to a network is a
  network node. Computnig paradigms which have benefited from networks are distributed computing, grid computing,
  Internet computing, and cloud computing.</p>
<div class="third">Types of network</div>
<p>Personal area network/home network is a computer network used for communication among devices close to one person,
  such as PCs, faxes, PDAs, and TVs. This is the base on which the Internet of Things is built.</p>
<p>Local area network (LAN) connects devices in a limited geographical area such as a school campus or computer lab.</p>
<p>Campus network is an interconnection of LANs within a limited geographical area.</p>
<p>Wide area network (WAN) is a network covering a large geographic area, such as a city or country. A WAN limited to a
  city is sometimes called a Metropolitan Area Network.</p>
<p>Internet is the global network.</p>
<p>other classifications of networks divide networks into control networks, storage networks, virtual private networks
  (VPN), wireless networks, point-to-point networks, and Internet of Things.</p>
<div class="third">Basic network components</div>
<p>All networks are made of the following hardware components: computers, network interface cards (NICs), bridges, hubs,
  switches, and routers. All these components are called nodes in networking jargon. Each component performs a
  distinctive function essential for packaging, connecting, transmitting, amplifying, controlling, unpacking, and
  interpreting data. A repeater amplifies signals, a switch performs many-to-many connections, a hub performs
  one-to-many connections, an interface card performs data packing and transmission, a bridge connects one network with
  another, and a router is a computer itself that performs data analysis and flow control to regulate data from the
  network. These functions performed by varying components are specified by one or more levels of the seven-layer Open
  Systems Interconnect (OSI) networking model.</p>
<div class="third">Networking protocols and standards</div>
<p>Computers communicate with each other using protocols, which specify the format and regulations used to pack and
  un-pack data. Network protocols are divided into different layers. The physical layer deals with the physical
  connection, the data link layer deals with raw data transmission and flow control, and the network layer deals with
  packing and unpacking data into a particular format understandable by the relevant parties. The most commonly used OSI
  networking model organizes network protcols into seven layers: the application layer, presentation layer, session
  layer, transport layer, network layer, data link layer, and physical layer.</p>
<p>Not all network protocols implement all layers of the OSI model. The TCP/IP protocol implements neither the
  presentation layer nor the session layer. UDP and TCP both work on the transport layer above IP's network layer,
  providing best-effort unreliable transport (UDP) vs. reliable transport function (TCP). Physical layer protocols
  include token ring, Ethernet, fast Ethernet, gigabit Ethernet, and wireless Ethernet. Data-link layer protocols
  include frame-relay, asynchronous transfer mode (ATM), and Point-to-Point Protocol (PPP). Application layer protocols
  include Fibre channel, Small Computer System Interface (SCSI), and Bluetooth.</p>
<div class="third">The Internet</div>
<p>The Internet is a global system of interconnected governmental, academic, corporate, public, and private computer
  networks. Access to the internet is through organizations known as internet service providers (ISP), which maintains
  one or more switching centers called a point of presence which connect users to the Internet.</p>
<div class="third">Internet of things</div>
<p>The Internet of Things refers to networking of everyday objects using wired or wireless networking technologies. The
  purpose is to interconnect all things to facilitate autonomous and better living. Technologies used include RFID,
  wireless and wired networking, sensor technology, and a lot of software.</p>
<div class="third">Virtual private network (VPN)</div>
<p>A virtual private network is a preplanned virtual connection between nodes in a LAN/WAN or on the internet. It allows
  the network administrator to separate network traffic into user groups, improving performance and security between
  nodes and allowing easier maintenance of circuits when troubleshooting.</p>
<div class="second">Parallel and distributed computing</div>
<p>Parallel computing emerges with the development of multi-functional units within a computer. The main objective of
  parallel computing is to execute several tasks simultaneously on different functional units to improve throughput or
  response or both. Distributed computing is a paradigm that emerges with the development of computer networks. Its
  objective is to make use of multiple computers in the network to accomplish things otherwise not possible within a
  single computer or to improve efficiency.</p>
<div class="third">Parallel and distributed computing overview</div>
<p>Parallel computing investigates ways to maximize concurrency (simultaneous execution of multiple tasks) within one
  computer. Distributed computing consists of multiple autonomous computers that communicate through a computer network.
  Distributed computing studies the ways of dividing a problem into many tasks and assigning such tasks to various
  computers. Distributed computing is another form of parallel computing albeit on a grander scale. In distributed
  computing, the functional units rae not ALU, FPU, or separate cores, but individual computers. Both parallel and
  distributed computing are called concurrent computing.</p>
<div class="third">Difference between parallel and distributed computing</div>
<p>Parallel computing can be run on different processors within a single computer. The scope of parallel computing is
  limited to where a shared memory is used by all processors involved in the computing, while distributed computing
  refers to computations where private memory exists for each processor involved in the computations. Parallel computing
  necessitates concurrent execution of several tasks while distributed computing does not have this necessity. Multicore
  computing is a form of parallel computing.</p>
<div class="third">Parallel and distributed computing models</div>
<p>In a shared memory (parallel) model, all computers have access to a shared central memory where local caches are used
  to speed up the processing power. These caches use a protocol to insure the lcoalized data is fresh and up to date,
  typically the MESI protocol. The algorithm designer choosees the probgram for execution by each computer. Access to
  the central memory can be synchronous or asynchronous, and must be coordinated such that coherency is maintained.</p>
<p>In a message-passing (distributed) model, all computers run some programs that collectively achieve some purpose. The
  system must work correctly regardless of the structure of the network. This mdoel can be further classified into
  client-server (C/S), browser-server (B/S), and n-tier models. In the C/S model, the server provides services and the
  client requests services from the server. In the B/S model, the server provides services and the client is the
  browser. In the n-tier model, each tier provides services to the tier immediately above it and requests services form
  the tier immediately below it. The n-tier model can be seen as a chain of client-server models. The tiers between the
  bottommost and topmost tier are called middleware.</p>
<div class="third">Main issues in distributed computing</div>
<p>Memory coherency and consensus among all computers are the most difficult issues faced by coordinating all the
  components in a distributed environment.</p>
<div class="second">Basic user human factors</div>
<div class="third">Input and output</div>
<p>Input and output are the interfaces between users and software. Issues considered for input include the input
  required, how the input is passed from users to computers, what the most convenient way for users to enter input is,
  and the format by which computers require input data. The designer should request the minimum data from human input
  only when data is not already stored in the system, and the designer should format and edit the data at the time of
  entry to reduce errors arising from incorrect or malicious data entry. For output, consider the format users would
  like to see the output, the most pleasing way to display output. Rules of thumb include simple and natural dialogue,
  speaking users' language, minimizing user memory load, consistency, minimal surprise, conformance to standards (eg.
  automobiles have a standard interface for accelerator, brake, steering).</p>
<div class="third">Error messages</div>
<p>Error messages should clearly explain what is happening (no codes), should pinpoint the cause of the error, and
  should be displayed right when the error condition occurs. Error messages should not overload the users with too much
  information that cause them to ignore the messages altogether. Messages relating to security access errors should not
  provide extra information allowing unauthorized access.</p>
<div class="third">Software robustness</div>
<p>Software robustness refers to the ability of software to tolerate erroneous inputs. One should always check the
  validity of the inputs and return values, one should always throw an exception when something unexpected occurs, and
  one should never quit a program without first giving users/applications a chance to correct the condition.</p>
<div class="second">Basic developer human factors</div>
<p>To ensure that software is easy to read, structure (program layouts) and comments (documentation) exist.</p>
<div class="third">Structure</div>
<p>Well-structured programs are easier to understand. White space, indentation, parentheses, blank lines, braces, etc.
  can be used to add structure to a program.</p>
<div class="third">Comments</div>
<p>Good comments explain the intent of the code and justify why this code looks the way it does. If the code is written
  in a clear and precise manner such that its meaning is self-proclaimed, then no comment is needed. However, most
  programs are not self-explanatory. Comments should be consistent across the entire program, each function should be
  commented to explain the purpose of the function. Within a function, comments should explain the meaning and intention
  of each logical section. Comments should stipulate what freedom does or does not the maintaining programmers have with
  respect to making changes to that code. If a statement needs comments, one should reconsider the statement.</p>
<div class="second">Secure software development and maintenance</div>
<div class="third">Software requirements security</div>
<p>Software requirements security deals with the clarification and specification of security policy and objectives into
  software requirements.</p>
<div class="third">Software design security</div>
<p>Software design security deals with the design of software modules that fit together to meet security objectives
  specified in the requirements. Factors considered may include frameworks and access modes that setup the overall
  security monitoring/enforcement strategies.</p>
<div class="third">Software construction security</div>
<p>Software construction security concerns the question of how to write actual programming code for specific situations
  such that security considerations are taken care of. It can mean coding a function in a secure way or the coding of
  security into software. Most people are unclear as to how one can make specific coding more secure. For example, in C,
  the expression i&#60;&#60;1 (shift the binary representation of i's value to the left by one bit) and 2*i (mlutiply
  the value of variable i by constant 2) mean the same thing semantically, but might not have the same security
  ramification. So most people regard construction security as coding of security into software. Rules for this include
  structuring the process so that all sections requiring extra privilages are modules made as small as possible. Ensure
  that any assumptions in the program are validated. If this is not possible, document them for installers and
  maintainers so they know the assumptions that attackers will try to invalidate. Ensure that the program does not shrae
  objects in memory with other programs. Check the error status of every function and do not try to recover unless
  neither the cause of the error nor its effects affect any escurity considerations. The program should restore the
  state of the software to the state it had before the process began, and the nterminate.</p>
<div class="third">Software testing security</div>
<p>Software testing security determines that software protects data and maintains security specification as given.</p>
<div class="third">Build security into software engineering process</div>
<p>One trend that emerges to build security into the engineering process is the Secure Development Lifecycle (SDL)
  concept, which is a classical spiral model that takes a holistic view of security and ensures that security is
  inherent in software design and development, not as an afterthought.</p>
<div class="third">Software security guidelines</div>
<p>Some reputable guidelines are published by the Computer Emergency Response Team (CERT) and its top 10 software
  security practices are 1) validate input, 2) heed compiler warnings, 3) architect and design for security policies, 4)
  keep it simple, 5) default deny, 6) adhere to the principle of least privilege, 7) sanitize data sent to other
  software, 8) practice defense in depth, 9) use effective quality assurance techniques, 10) adopt a software
  construction security standard.</p>

<div class="first">Mathematical foundations</div><a class="anchor" id="mathematical_foundations"></a>
<p>Mathematics, in a sense, is the study of formal systems, associated with preciseness, so there cannot be any
  ambiguity or erroneous interpretation of fact.</p>
<div class="second">Sets, relations, functions</div>
<div class="third">Set operations</div>
<div class="third">Properties of set</div>
<div class="third">Relation and function</div>
<div class="second">Basic logic</div>
<div class="third">Propositional logic</div>
<div class="third">Predicate logic</div>
<div class="second">Proof techniques</div>
<div class="third">Methods of proving theorems</div>
<div class="second">Basics of counting</div>
<div class="second">Graphs and trees</div>
<div class="third">Graphs</div>
<div class="third">Trees</div>
<div class="second">Discrete probability</div>
<div class="second">Finite state machines</div>
<div class="second">Grammars</div>
<div class="third">Language recognition</div>
<div class="second">Numerical precision, accuracy, and errors</div>
<div class="second">Number theory</div>
<div class="third">Divisibility</div>
<div class="third">Prime number, GCD</div>
<div class="second">Algebraic structures</div>
<div class="third">Group</div>
<div class="third">Rings</div>

<div class="first">Engineering foundations</div><a class="anchor" id="engineering_foundations"></a>
<div class="second">Empirical methods and experimental techniques</div>
<div class="third">Designed experiment</div>
<div class="third">Observational study</div>
<div class="third">Retrospective study</div>
<div class="second">Statistical analysis</div>
<div class="third">Unit of analysis (sampling units), population, and sample</div>
<div class="third">Concepts of correlation and regression</div>
<div class="second">Measurement</div>
<div class="third">Levels (scales) of measurement</div>
<div class="third">Direct and derived measures</div>
<div class="third">Reliability and validity</div>
<div class="third">Assessing reliability</div>
<div class="second">Engineering design</div>
<div class="third">Engineering design in engineering education</div>
<div class="third">Design as a problem solving activity</div>
<div class="third">Steps involved in engineering design</div>
<div class="second">Modeling, simulation, and prototyping</div>
<div class="third">Modeling</div>
<div class="third">Simulation</div>
<div class="third">Prototyping</div>
<div class="second">Standards</div>
<div class="second">Root cause analysis</div>
<div class="third">Techniques for conducting root cause analysis</div>



<div class="first">Overview of operating systems</div><a class="anchor" id="overview_operating"></a>
<h4>Kernel</h4>
<p>The kernel is a computer program at the core of a computer's operating system that generally has complete
  control
  over everything in the system. It is the portion of the operating system that is always resident in memory, and
  facilitates interactions between hardware and software components. It controls all hardware resources (e.g. I/O,
  memory, cryptography) via device drivers, arbitrates conflicts between processes concerning such resources, and
  optimizes the utilization of common resources e.g. CPU & cache usage, file systems, and network sockets. It is
  one
  of
  the first programs loaded on startup (after the bootloader). It connects the application software to the
  hardware of
  a
  computer.</p>
<h4>Operating system</h4>
<p>An operating system is system software that manages computer hardware, software resources, and provides common
  services for computer programs.</p>
<h4>Assembly language</h4>
<p>Assembly language, or assembler language, is any low-level programming language in which there is a very strong
  correspondence between the instructions in the language and the architecture's machine code instructions. It
  usually
  has
  one statement per machine instruction. Assembly code is converted into executable machine code by a utility
  program
  reffered to as an assembler. The term originally meant "a program that assembles another program consisting of
  several
  sections into a single program." The conversion process is referred to as assembly, as in assembling the source
  code.
</p>
<h4>Linux</h4>
<p>Linux is a family of free and open-source operating systems based on the Linux kernel, examples include Debian,
  Ubuntu, Fedora, CentOS, Gentoo, Arch Linux, and many others. 90% of all cloud infrastructure and 74% of the
  world's
  smartphones are powered by Linux. Linux-based operating systems depend heavily on working with the command line
  interface, while most personal computers rely on graphical interfaces. Linux filesystems also have a different
  structure than those found on Windows or MacOS.</p>
<h4>Terminal</h4>
<p>A terminal is an input and output environment that presents a text-only window running a shell. A shell is a
  program
  that exposes the computer's operating system to a user or program. In Linux systems, the shell presented in a
  terminal
  is a command line interpreter. A command line interface is a user interface which processes commands to a
  computer
  program and outputs the results.</p>

<div class="first">Introduction</div><a class="anchor" id="platform"></a>
<h4>Overview of platforms (e.g., Web, Mobile, Game, Industrial)</h4>
<h4>Programming via platform-specific APIs</h4>
<h4>Overview of Platform Languages (e.g., Objective C, HTML5)</h4>
<h4>Programming under platform constraints</h4>
<div class="first">Web platforms</div><a class="anchor" id="web"></a>
<h4>Web programming languages (e.g., HTML5, JavaScript, PHP, CSS)</h4>
<h4>Web platform constraints</h4>
<h4>Software as a Service (SaaS)</h4>
<h4>Web standards</h4>
Design and implement a simple web application
Describe the constraints that the web puts on developers
Compare and contrast web programming with general purpose programming
Describe the differences between Software-as-a-Service and traditional software products
Discuss how web standards impact software development
Review an existing web application against a current web standard
<div class="first">Mobile platforms</div><a class="anchor" id="mobile"></a>
<div class="first">Industrial platforms</div><a class="anchor" id="industrial"></a>
<div class="first">Game platforms</div><a class="anchor" id="game"></a>
<div class="first">Object-oriented programming</div><a class="anchor" id="platform"></a>
<h4>Object-oriented design</h4>
Decomposition into objects carrying state and having behavior, class-hierarchy design for modeling
<h4>Definition of classes: fields, methods, and constructors</h4>
<h4>Subclasses, inheritance, and method overriding</h4>
<h4>Dynamic dispatch: definition of method-call</h4>
<div class="first">Functional programming</div><a class="anchor" id="platform"></a>
<h4>Effect-free programming</h4>
Function calls have no side effects, facilitating compositional reasoning
Variables are immutable, preventing unexpected changes to program data by other code
Data can be freely aliased or copied without introducing unintended effects from mutation
<h4>Processing structured data (e.g., trees) via functions with cases for each data variant</h4>
Associated language constructs such as discriminated unions and pattern-matching over them
Functions defined over compound data in terms of functions applied to the constituent pieces
<h4>First-class functions (taking, returning, and storing functions)</h4>
<div class="first">Event-driven and reactive programming</div><a class="anchor" id="platform"></a>
<h4>Events and event handlers</h4>
<h4>Canonical uses such as GUIs, mobile devices, robots, servers</h4>
<h4>Using a reactive framework</h4>
Defining event handlers/listeners
Main event loop not under event-handler-writer's control
<h4>Externally-generated events and program-generated events</h4>
<h4>Separation of model, view, and controller</h4>
<div class="first">Basic type systems</div><a class="anchor" id="platform"></a>
<h4>A type as a set of values together with a set of operations</h4>
Primitive types (e.g., numbers, Booleans)
Compound types built from other types (e.g., records, unions, arrays, lists, functions, references)
<h4>Association of types to variables, arguments, results, and fields</h4>
<h4>Type safety and errors caused by using values inconsistently given their intended types</h4>
<h4>Goals and limitations of static typing</h4>
Eliminating some classes of errors without running the program
Undecidability means static analysis must conservatively approximate program behavior
<div class="first">Program representation</div><a class="anchor" id="platform"></a>
<div class="first">Language translation and execution</div><a class="anchor" id="platform"></a>
<div class="first">Syntax analysis</div><a class="anchor" id="platform"></a>
<div class="first">Compiler semantic analysis</div><a class="anchor" id="platform"></a>
<div class="first">Code generation</div><a class="anchor" id="platform"></a>
<div class="first">Runtime systems</div><a class="anchor" id="platform"></a>
<div class="first">Static analysis</div><a class="anchor" id="platform"></a>
<div class="first">Advanced programming constructs</div><a class="anchor" id="platform"></a>
<div class="first">Concurrency and parallelism</div><a class="anchor" id="platform"></a>
<div class="first">Type systems</div><a class="anchor" id="platform"></a>
<div class="first">Formal semantics</div><a class="anchor" id="platform"></a>
<div class="first">Language pragmatics</div><a class="anchor" id="platform"></a>
<div class="first">Logic programming</div><a class="anchor" id="platform"></a>
<div class="first">Algorithms and design</div><a class="anchor" id="algorithm_design"></a>
<h4>The concept and properties of algorithms</h4>
Informal comparison of alogirthm efficiency (e.g., operation counts)
<h4>The role of algorithms in the problem-solving process</h4>
<h4>Problem-solving strategies</h4>
Iterative and recursive mathematical functions
Iterative and recursive traversal of data structures
Divide-and-conquer strategies
<h4>Fundamental design concepts and principles</h4>
Abstraction, program decomposition, encapsulation and information binding, separation of behavior and
implementation
<div class="first">Fundamental programming concepts</div><a class="anchor" id="fundamental_programming"></a>
<h4>Basic syntax and semantics of a higher-level language</h4>
<h4>Variables and primitive data types (e.g., numbers, characters, Booleans)</h4>
<h4>Expressions and assignments</h4>
<h4>Simple I/O including file I/O</h4>
<h4>Conditional and iterative control structures</h4>
<h4>Functions and parameter passing</h4>
<h4>The concept of recursion</h4>
<div class="first">Fundamental data structures</div><a class="anchor" id="fundamental_data"></a>
<h4>Arrays</h4>
<h4>Records/structs (heterogeneous aggregates)</h4>
<h4>Strings and string processing</h4>
<h4>Abstract data types and their implementation</h4>
Stacks, queues, priorty queues, sets, maps
<h4>References and aliasing</h4>
<h4>Linked lists</h4>
<h4>Strategies for choosing the appropriate data structure</h4>
<div class="first">Development methods</div><a class="anchor" id="development_methods"></a>
<h4>Program comprehension</h4>
<h4>Program correctness</h4>
Types of errors (syntax, logic, run-time), the concept of a specification, defensive programming (e.g., secure
coding,
exception handling), code reviews, testing fundamentals and test-case generation, the role and the use of
contracts,
including pre- and post-conditions, unit testing
<h4>Simple refactoring</h4>
<h4>Modern programming environments</h4>
Code search, programming using library components and their APIs
<h4>Debugging strategies</h4>
<h4>Documentation and program style</h4>
<div class="first">Software processes</div><a class="anchor" id="platform"></a>
<h4>Systems level considerations, i.e., the interaction of software with its intended environment (cross-reference
  IAS/Secure Software Engineering)</h4>
<h4>Introduction to software process models (e.g., waterfall, incremental, agile)</h4>
Activities within software lifecycles
<h4>Programming in the large vs. individual programming</h4>
<div class="first">Software project management</div><a class="anchor" id="platform"></a>
<h4>Team participation</h4>
Team processes including responsibilities for tasks, meeting structure, and work schedule
Roles and responsibilities in a software team
Team conflict resolution
Risks associated with virtual teams (communication, perception, structure)
<h4>Effort estimation (at the personal level)</h4>
<h4>Risk (cross reference IAS/Secure Software Engineering)</h4>
The role of risk in the lifecycle
Risk categories including security, safety, market, financial, technology, people, quality, structure and process
<div class="first">Tools and environments</div><a class="anchor" id="platform"></a>
<h4>Software configuration management and version control</h4>
<h4>Release management</h4>
<h4>Requirements analysis and design modeling tools</h4>
<h4>Testing tools including static and dynamic analysis tools</h4>
<h4>Programming environments that automate parts of program construction processes (e.g., automated builds)</h4>
Continuous integration
<h4>Tool integration concepts and mechanisms</h4>
<div class="first">Requirements engineering</div><a class="anchor" id="platform"></a>
<h4>Describing functional requirements using, for example, use cases or user stories</h4>
<h4>Properties of requirements including consistency, validity, completeness, and feasibility</h4>
<div class="first">Software design</div><a class="anchor" id="platform"></a>
<h4>System design principles</h4>
Levels of abstraction (architectural design and detailed design), separation of concerns, information hiding,
coupling
and cohesion, re-use of standard structures
<h4>Design paradigms</h4>
Structured design (top-down functional decomposition), object-oriented analysis and design, event driven design,
component-level design, data-structured centered, aspect oriented, function oriented, service oriented
<h4>Structural and behavioral models of software designs</h4>
<h4>Design patterns</h4>
<div class="first">Software construction</div><a class="anchor" id="platform"></a>
<h4>Coding practices</h4>
techniques, idioms/patterns, mechanisms for building quality programs (cross-reference IAS/Defensive Programming;
SDF/Development Methods)
Defensive coding practices
Secure coding practices
Using exception handling mechanisms to make programs more robust, fault-tolerant
<h4>Coding standards</h4>
<h4>Integration strategies</h4>
<h4>Development context: "green field" vs. existing code base</h4>
Change impact analysis, change actualization
<div class="first">Software verification and validation</div><a class="anchor" id="platform"></a>
<h4>Verification and validation concepts</h4>
<h4>Inspections, reviews, audits</h4>
<h4>Testing types, including human computer interface, usability, reliability, security, conformance to
  specification
  (cross-reference IAS/Secure Software Engineering)</h4>
<h4>Testing fundamentals (cross-reference SDF/Development Methods)</h4>
Unit, integration, validation, and system testing
Test plan creation and test case generation
Black-box and white-box testing techniques
Regression testing and test automation
<h4>Defect tracking</h4>
<h4>Limitations of testing in particular domains, such as parallel or safety-critical systems</h4>
<div class="first">Software evolution</div><a class="anchor" id="platform"></a>
<div class="first">Software reliability</div><a class="anchor" id="platform"></a>
<div class="first">Formal methods</div><a class="anchor" id="platform"></a>

<div class="size">
  https://www.acm.org/binaries/content/assets/education/cs2013_web_final.pdf
  <p>The top core competencies from computer science for general application include software development
    fundamentals,
    discrete structures, algorithms and complexity, and systems fundamentals.</p>
  <h1>Computer Science</h1>
  <h2>Algorithms and Complexity (AL)</h2>
  <h3>Basic Analysis</h3>
  Differences among best, expected, and worst case behaviors of an algorithm
  • Asymptotic analysis of upper and expected complexity bounds
  • Big O notation: formal definition
  • Complexity classes, such as constant, logarithmic, linear, quadratic, and exponential
  • Empirical measurements of performance
  • Time and space trade-offs in algorithms
  <h3>Algorithmic Strategies</h3>
  <h3>Fundamental Data Structures and Algorithms</h3>
  Simple numerical algorithms, such as computing the average of a list of numbers, finding the min, max,
  and mode in a list, approximating the square root of a number, or finding the greatest common divisor
  • Sequential and binary search algorithms
  • Worst case quadratic sorting algorithms (selection, insertion)
  • Worst or average case O(N log N) sorting algorithms (quicksort, heapsort, mergesort)
  • Hash tables, including strategies for avoiding and resolving collisions
  • Binary search trees
  o Common operations on binary search trees such as select min, max, insert, delete, iterate over tree
  • Graphs and graph algorithms
  o Representations of graphs (e.g., adjacency list, adjacency matrix)
  o Depth- and breadth-first traversals
  Heaps
  • Graphs and graph algorithms
  o Shortest-path algorithms (Dijkstra’s and Floyd’s algorithms)
  o Minimum spanning tree (Prim’s and Kruskal’s algorithms)
  • Pattern matching and string/text algorithms (e.g., substring matching, regular expression matching, longest
  common subsequence algorithms)
  <h3>Basic Automata Computability and Complexity</h3>
  Finite-state machines
  • Regular expressions
  • The halting problem
  <h3>Advanced Computational Complexity</h3>
  <h3>Advanced Automata Theory and Computability</h3>
  <h3>Advanced Data Structures Algorithms and Analysis</h3>
  <h2>Architecture and Organization (AR)</h2>
  <h3>Digital Logic and Digital Systems</h3>
  <h3>Machine Level Representation of Data</h3>
  Bits, bytes, and words
  • Numeric data representation and number bases
  • Fixed- and floating-point systems
  • Signed and twos-complement representations
  • Representation of non-numeric data (character codes, graphical data)
  • Representation of records and arrays
  <h3>Assembly Level Machine Organization</h3>
  <h3>Memory System Organization and Architecture</h3>
  Storage systems and their technology
  • Memory hierarchy: importance of temporal and spatial locality
  • Main memory organization and operations
  • Latency, cycle time, bandwidth, and interleaving
  • Cache memories (address mapping, block size, replacement and store policy)
  • Multiprocessor cache consistency/Using the memory system for inter-core synchronization/atomic memory
  operations
  • Virtual memory (page table, TLB)
  • Fault handling and reliability
  • Error coding, data compression, and data integrity (cross-reference SF/Reliability through Redundancy)
  <h3>Interfacing and Communication</h3>
  <h3>Functional Organization</h3>
  <h3>Multiprocessing and Alternative Architectures</h3>
  <h3>Performance Enhancements</h3>
  <h2>Computational Science (CN)</h2>
  <p>Computational science is a field of applied computer science, that is, the application of computer science to
    solve
    problems across a range of disciplines. It combines computer simulation, scientific visualization,
    mathematical
    modeling, computer programming and data structures, networking, database design, symbolic computation, and
    high
    performance computing with various disciplines. The needs of scientists and engineers for computation have
    long
    driven research and innovation in computing. Computational neuroscience is a subfield of computational
    science.
  </p>
  <p>A goal in neural engineering is to abstract the concept of the brain. But the brain trying to model itself is
    a
    bit
    meta. Some say that dreams are essential models of various situations our subconscious is trying to prepare us
    for.
    In order to understand the brain, we should understand various modeling and simulation techniques used in
    computer
    science.</p>
  <h3>Introduction to Modeling and Simulation</h3>
  Models as abstractions of situations
  • Simulations as dynamic modeling
  • Simulation techniques and tools, such as physical simulations, human-in-the-loop guided simulations, and
  virtual reality
  • Foundational approaches to validating models (e.g., comparing a simulation’s output to real data or the
  output of another model)
  • Presentation of results in a form relevant to the system being modeled
  <h3>Modeling and Simulation</h3>
  Purpose of modeling and simulation including optimization; supporting decision making, forecasting,
  safety considerations; for training and education
  • Tradeoffs including performance, accuracy, validity, and complexity
  • The simulation process; identification of key characteristics or behaviors, simplifying assumptions;
  validation of outcomes
  • Model building: use of mathematical formulas or equations, graphs, constraints; methodologies and
  techniques; use of time stepping for dynamic systems
  Formal models and modeling techniques: mathematical descriptions involving simplifying assumptions
  and avoiding detail. Examples of techniques include:
  o Monte Carlo methods
  o Stochastic processes
  o Queuing theory
  o Petri nets and colored Petri nets
  o Graph structures such as directed graphs, trees, networks
  o Games, game theory, the modeling of things using game theory
  o Linear programming and its extensions
  o Dynamic programming
  o Differential equations: ODE, PDE
  o Non-linear techniques
  o State spaces and transitions
  • Assessing and evaluating models and simulations in a variety of contexts; verification and validation of
  models and simulations
  • Important application areas including health care and diagnostics, economics and finance, city and urban
  planning, science, and engineering
  • Software in support of simulation and modeling; packages, languages
  <h3>Processing</h3>
  <h3>Interactive Visualization</h3>
  <h3>Data, Information, and Knowledge</h3> !!
  Content management models, frameworks, systems, design methods (as in IM. Information Management)
  • Digital representations of content including numbers, text, images (e.g., raster and vector), video (e.g.,
  QuickTime, MPEG2, MPEG4), audio (e.g., written score, MIDI, sampled digitized sound track) and
  animations; complex/composite/aggregate objects; FRBR
  • Digital content creation/capture and preservation, including digitization, sampling, compression,
  conversion, transformation/translation, migration/emulation, crawling, harvesting
  • Content structure / management, including digital libraries and static/dynamic/stream aspects for:
  o Data: data structures, databases
  o Information: document collections, multimedia pools, hyperbases (hypertext, hypermedia),
  catalogs, repositories
  o Knowledge: ontologies, triple stores, semantic networks, rules
  • Processing and pattern recognition, including indexing, searching (including: queries and query languages;
  central / federated / P2P), retrieving, clustering, classifying/categorizing, analyzing/mining/extracting,
  rendering, reporting, handling transactions
  • User / society support for presentation and interaction, including browse, search, filter, route, visualize,
  share, collaborate, rate, annotate, personalize, recommend
  • Modeling, design, logical and physical implementation, using relevant systems/software
  <h3>Numerical Analysis</h3>
  <h2>Discrete Structures (DS)</h2>
  <h3>Sets, Relations, and Functions</h3>
  Sets
  o Venn diagrams
  o Union, intersection, complement
  o Cartesian product
  o Power sets
  o Cardinality of finite sets
  • Relations
  o Reflexivity, symmetry, transitivity
  o Equivalence relations, partial orders
  • Functions
  o Surjections, injections, bijections
  o Inverses
  o Composition
  <h3>Basic Logic</h3>
  Propositional logic (cross-reference: Propositional logic is also reviewed in IS/Knowledge Based
  Reasoning)
  • Logical connectives
  • Truth tables
  • Normal forms (conjunctive and disjunctive)
  • Validity of well-formed formula
  • Propositional inference rules (concepts of modus ponens and modus tollens)
  • Predicate logic
  o Universal and existential quantification
  • Limitations of propositional and predicate logic (e.g., expressiveness issues)
  <h3>Proof Techniques</h3>
  Notions of implication, equivalence, converse, inverse, contrapositive, negation, and contradiction
  • The structure of mathematical proofs
  • Direct proofs
  • Disproving by counterexample
  • Proof by contradiction
  • Induction over natural numbers
  <h4>Principle of Mathematical Induction</h4>
  <p>For each natural number \(n \in \mathbb{N}\), suppose that \(P(n)\) denotes a proposition which is either
    true
    or
    false. Let \(A={n \in \mathbb{N}:P(n)}\) is true. Suppose the following conditions hold: a) \(1 \in A\) and
    b) for each \(k \in \mathbb{N}\), if \(k \in A\)
    , then \(k+1 \in A\). Then \(A=\mathbb{N}\).</p>
  • Structural induction
  • Weak and strong induction (i.e., First and Second Principle of Induction)
  • Recursive mathematical definitions
  <h3>Basics of Counting</h3>
  Counting arguments
  o Set cardinality and counting
  o Sum and product rule
  o Inclusion-exclusion principle
  o Arithmetic and geometric progressions
  • The pigeonhole principle
  • Permutations and combinations
  o Basic definitions
  o Pascal’s identity
  o The binomial theorem
  • Solving recurrence relations (cross-reference: AL/Basic Analysis)
  o An example of a simple recurrence relation, such as Fibonacci numbers
  o Other examples, showing a variety of solutions
  • Basic modular arithmetic
  <h3>Graphs and Trees</h3>
  Trees
  o Properties
  o Traversal strategies
  • Undirected graphs
  • Directed graphs
  • Weighted graphs
  <h3>Discrete Probability</h3>
  Finite probability space, events
  • Axioms of probability and probability measures
  • Conditional probability, Bayes’ theorem
  • Independence
  • Integer random variables (Bernoulli, binomial)
  • Expectation, including Linearity of Expectation
  <h2>Graphics and Visualization (GV)</h2> !!
  Media applications including user interfaces, audio and video editing, game engines, cad, visualization,
  virtual reality
  • Digitization of analog data, resolution, and the limits of human perception, e.g., pixels for visual display,
  dots for laser printers, and samples for audio (HCI/Foundations)
  • Use of standard APIs for the construction of UIs and display of standard media formats (see HCI/GUI
  construction)
  • Standard media formats, including lossless and lossy formats
  <h3>Fundamental Concepts</h3>
  <h3>Basic Rendering</h3>
  <h3>Geometric Modeling</h3>
  <h3>Advanced Rendering</h3>
  <h3>Computer Animation</h3>
  <h3>Visualization</h3>
  Visualization of 2D/3D scalar fields: color mapping, isosurfaces
  • Direct volume data rendering: ray-casting, transfer functions, segmentation
  • Visualization of:
  o Vector fields and flow data
  o Time-varying data
  o High-dimensional data: dimension reduction, parallel coordinates,
  o Non-spatial data: multi-variate, tree/graph structured, text
  • Perceptual and cognitive foundations that drive visual abstractions
  • Visualization design
  • Evaluation of visualization methods
  • Applications of visualization
  <h2>Human-Computer Interaction (HCI)</h2>
  <h3>Foundations</h3>
  Contexts for HCI (anything with a user interface, e.g., webpage, business applications, mobile applications,
  and games)
  • Processes for user-centered development, e.g., early focus on users, empirical testing, iterative design
  • Different measures for evaluation, e.g., utility, efficiency, learnability, user satisfaction
  • Usability heuristics and the principles of usability testing
  • Physical capabilities that inform interaction design, e.g., color perception, ergonomics
  • Cognitive models that inform interaction design, e.g., attention, perception and recognition, movement, and
  memory; gulfs of expectation and execution
  • Social models that inform interaction design, e.g., culture, communication, networks and organizations
  • Principles of good design and good designers; engineering tradeoffs
  • Accessibility, e.g., interfaces for differently-abled populations (e.g., blind, motion-impaired)
  • Interfaces for differently-aged population groups (e.g., children, 80+)
  <h3>Designing Interaction</h3>
  Principles of graphical user interfaces (GUIs)
  • Elements of visual design (layout, color, fonts, labeling)
  • Task analysis, including qualitative aspects of generating task analytic models
  • Low-fidelity (paper) prototyping
  • Quantitative evaluation techniques, e.g., keystroke-level evaluation
  • Help and documentation
  • Handling human/system failure
  • User interface standards
  <h3>Programming Interactive Systems</h3>
  Software Architecture Patterns, e.g., Model-View controller; command objects, online, offline (cross
  reference PL/Event Driven and Reactive Programming, where MVC is used in the context of event-driven
  programming)
  • Interaction Design Patterns: visual hierarchy, navigational distance
  • Event management and user interaction
  • Geometry management (cross-reference GV/Geometric Modelling)
  • Choosing interaction styles and interaction techniques
  • Presenting information: navigation, representation, manipulation
  • Interface animation techniques (e.g., scene graphs)
  • Widget classes and libraries
  • Modern GUI libraries (e.g. iOS, Android, JavaFX) GUI builders and UI programming environments (crossreference
  PBD/Mobile Platforms)
  • Declarative Interface Specification: Stylesheets and DOMs
  • Data-driven applications (database-backed web pages)
  • Cross-platform design
  • Design for resource-constrained devices (e.g. small, mobile devices)
  <h3>User-Centered Design and Testing</h3>
  Approaches to, and characteristics of, the design process
  • Functionality and usability requirements (cross-reference to SE/Requirements Engineering)
  • Techniques for gathering requirements, e.g., interviews, surveys, ethnographic and contextual enquiry
  • Techniques and tools for the analysis and presentation of requirements, e.g., reports, personas
  • Prototyping techniques and tools, e.g., sketching, storyboards, low-fidelity prototyping, wireframes
  • Evaluation without users, using both qualitative and quantitative techniques, e.g., walkthroughs, GOMS,
  expert-based analysis, heuristics, guidelines, and standards
  • Evaluation with users, e.g., observation, think-aloud, interview, survey, experiment
  • Challenges to effective evaluation, e.g., sampling, generalization
  • Reporting the results of evaluations
  • Internationalization, designing for users from other cultures, cross-cultural
  <h3>New Interactive Technologies</h3>
  Choosing interaction styles and interaction techniques
  • Representing information to users: navigation, representation, manipulation
  • Approaches to design, implementation and evaluation of non-mouse interaction
  o Touch and multi-touch interfaces
  o Shared, embodied, and large interfaces
  o New input modalities (such as sensor and location data)
  o New Windows, e.g., iPhone, Android
  o Speech recognition and natural language processing (cross reference IS/Natural Language
  Processing)
  o Wearable and tangible interfaces
  o Persuasive interaction and emotion
  o Ubiquitous and context-aware interaction technologies (Ubicomp)
  o Bayesian inference (e.g. predictive text, guided pointing)
  o Ambient/peripheral display and interaction
  <h3>Collaboration and Communication</h3>
  <h3>Statistical Methods for HCI</h3>
  <h3>Human Factors and Security</h3>
  <h3>Design-Oriented HCI</h3>
  <h3>Mixed, Augmented and Virtual Reality</h3>
  Output
  o Sound
  o Stereoscopic display
  o Force feedback simulation, haptic devices
  • User input
  o Viewer and object tracking
  o Pose and gesture recognition
  o Accelerometers
  o Fiducial markers
  o User interface issues
  • Physical modelling and rendering
  o Physical simulation: collision detection & response, animation
  o Visibility computation
  o Time-critical rendering, multiple levels of details (LOD)
  • System architectures
  o Game engines
  o Mobile augmented reality
  o Flight simulators
  o CAVEs
  o Medical imaging
  • Networking
  o p2p, client-server, dead reckoning, encryption, synchronization
  o Distributed collaboration
  <h2>Information Assurance and Security (IAS)</h2>
  <h3>Foundational Concepts in Security</h3>
  <h3>Principles of Secure Design</h3>
  <h3>Defensive Programming</h3>
  <h3>Threats and Attacks</h3>
  <h3>Network Security</h3>
  <h3>Cryptography</h3>
  <h3>Web Security</h3>
  <h3>Platform Security</h3>
  <h3>Security Policy and Governance</h3>
  <h3>Digital Forensics</h3>
  <h3>Secure Software Engineering</h3>
  <h2>Information Management (IM)</h2>
  <h3>Information Management Concepts</h3>
  Information systems as socio-technical systems
  • Basic information storage and retrieval (IS&R) concepts
  • Information capture and representation
  • Supporting human needs: searching, retrieving, linking, browsing, navigating
  Approaches to and evolution of database systems
  • Components of database systems
  Design of core DBMS functions (e.g., query mechanisms, transaction management, buffer management,
  access methods)
  • Database architecture and data independence
  • Use of a declarative query language
  • Systems supporting structured and/or stream content
  <h3>Database Systems</h3>
  <h3>Data Modeling</h3>
  Data modeling
  • Conceptual models (e.g., entity-relationship, UML diagrams)
  • Spreadsheet models
  • Relational data models
  • Object-oriented models (cross-reference PL/Object-Oriented Programming)
  • Semi-structured data model (expressed using DTD or XML Schema, for example)
  <h3>Indexing</h3>
  The impact of indices on query performance
  • The basic structure of an index
  • Keeping a buffer of data in memory
  • Creating indexes with SQL
  • Indexing text
  • Indexing the web (e.g., web crawling)
  <h3>Relational Databases</h3>
  Mapping conceptual schema to a relational schema
  • Entity and referential integrity
  • Relational algebra and relational calculus
  • Relational Database design
  • Functional dependency
  • Decomposition of a schema; lossless-join and dependency-preservation properties of a decomposition
  • Candidate keys, superkeys, and closure of a set of attributes
  • Normal forms (BCNF)
  • Multi-valued dependency (4NF)
  • Join dependency (PJNF, 5NF)
  • Representation theory
  <h3>Query Languages</h3>
  Overview of database languages
  • SQL (data definition, query formulation, update sublanguage, constraints, integrity)
  • Selections
  • Projections
  • Select-project-join
  • Aggregates and group-by
  • Subqueries
  • QBE and 4th-generation environments
  • Different ways to invoke non-procedural queries in conventional languages
  • Introduction to other major query languages (e.g., XPATH, SPARQL)
  • Stored procedures
  <h3>Transaction Processing</h3>
  Transactions
  • Failure and recovery
  • Concurrency control
  • Interaction of transaction management with storage, especially buffering
  <h3>Distributed Databases</h3>
  <h3>Physical Database Design</h3>
  <h3>Data Mining</h3>
  Uses of data mining
  • Data mining algorithms
  • Associative and sequential patterns
  • Data clustering
  • Market basket analysis
  • Data cleaning
  • Data visualization (cross-reference GV/Visualization and CN/Interactive Visualization)
  <h3>Information Storage and Retrieval</h3>
  Documents, electronic publishing, markup, and markup languages
  • Tries, inverted files, PAT trees, signature files, indexing
  • Morphological analysis, stemming, phrases, stop lists
  • Term frequency distributions, uncertainty, fuzziness, weighting
  • Vector space, probabilistic, logical, and advanced models
  • Information needs, relevance, evaluation, effectiveness
  • Thesauri, ontologies, classification and categorization, metadata
  • Bibliographic information, bibliometrics, citations
  • Routing and (community) filtering
  • Multimedia search, information seeking behavior, user modeling, feedback
  • Information summarization and visualization
  • Faceted search (e.g., using citations, keywords, classification schemes)
  • Digital libraries
  • Digitization, storage, interchange, digital objects, composites, and packages
  • Metadata and cataloging
  • Naming, repositories, archives
  • Archiving and preservation, integrity
  • Spaces (conceptual, geographical, 2/3D, VR)
  • Architectures (agents, buses, wrappers/mediators), interoperability
  • Services (searching, linking, browsing, and so forth)
  • Intellectual property rights management, privacy, and protection (watermarking)
  <h3>Multimedia Systems</h3>
  • Input and output devices, device drivers, control signals and protocols, DSPs
  • Standards (e.g., audio, graphics, video)
  • Applications, media editors, authoring systems, and authoring
  • Streams/structures, capture/represent/transform, spaces/domains, compression/coding
  • Content-based analysis, indexing, and retrieval of audio, images, animation, and video
  Presentation, rendering, synchronization, multi-modal integration/interfaces
  • Real-time delivery, quality of service (including performance), capacity planning, audio/video
  conferencing, video-on-demand
  <h2>Intelligent Systems (IS)</h2>
  <h3>Fundamental Issues</h3>
  <h3>Basic Search Strategies</h3>
  <h3>Basic Knowledge Representation and Reasoning</h3>
  Review of propositional and predicate logic (cross-reference DS/Basic Logic)
  • Resolution and theorem proving (propositional logic only)
  • Forward chaining, backward chaining
  • Review of probabilistic reasoning, Bayes theorem (cross-reference with DS/Discrete Probability)
  <h3>Basic Machine Learning</h3>
  <h3>Advanced Search</h3>
  <h3>Advanced Representation and Reasoning</h3>
  <h3>Reasoning Under Uncertainty</h3>
  Review of basic probability (cross-reference DS/Discrete Probability)
  • Random variables and probability distributions
  o Axioms of probability
  o Probabilistic inference
  o Bayes’ Rule
  • Conditional Independence
  • Knowledge representations
  o Bayesian Networks
   Exact inference and its complexity
   Randomized sampling (Monte Carlo) methods (e.g. Gibbs sampling)
  o Markov Networks
  o Relational probability models
  o Hidden Markov Models
  • Decision Theory
  o Preferences and utility functions
  o Maximizing expected utility
  <h3>Agents</h3>
  Definitions of agents
  • Agent architectures (e.g., reactive, layered, cognitive)
  • Agent theory
  • Rationality, game theory
  o Decision-theoretic agents
  o Markov decision processes (MDP)
  • Software agents, personal assistants, and information access
  o Collaborative agents
  o Information-gathering agents
  o Believable agents (synthetic characters, modeling emotions in agents)
  • Learning agents
  • Multi-agent systems
  o Collaborating agents
  o Agent teams
  o Competitive agents (e.g., auctions, voting)
  o Swarm systems and biologically inspired models
  <h3>Natural Language Processing</h3>
  Deterministic and stochastic grammars
  • Parsing algorithms
  o CFGs and chart parsers (e.g. CYK)
  o Probabilistic CFGs and weighted CYK
  • Representing meaning / Semantics
  o Logic-based knowledge representations
  o Semantic roles
  o Temporal representations
  o Beliefs, desires, and intentions
  • Corpus-based methods
  • N-grams and HMMs
  • Smoothing and backoff
  Examples of use: POS tagging and morphology
  • Information retrieval (Cross-reference IM/Information Storage and Retrieval)
  o Vector space model
   TF & IDF
  o Precision and recall
  • Information extraction
  • Language translation
  • Text classification, categorization
  o Bag of words model
  <h3>Advanced Machine Learning</h3>
  Definition and examples of broad variety of machine learning tasks
  • General statistical-based learning, parameter estimation (maximum likelihood)
  • Inductive logic programming (ILP)
  • Supervised learning
  o Learning decision trees
  o Learning neural networks
  o Support vector machines (SVMs)
  • Ensembles
  • Nearest-neighbor algorithms
  • Unsupervised Learning and clustering
  o EM
  o K-means
  o Self-organizing maps
  • Semi-supervised learning
  • Learning graphical models (Cross-reference IS/Reasoning under Uncertainty)
  • Performance evaluation (such as cross-validation, area under ROC curve)
  • Learning theory
  • The problem of overfitting, the curse of dimensionality
  • Reinforcement learning
  o Exploration vs. exploitation trade-off
  o Markov decision processes
  o Value and policy iteration
  • Application of Machine Learning algorithms to Data Mining (cross-reference IM/Data Mining)
  <h3>Robotics</h3>
  Overview: problems and progress
  o State-of-the-art robot systems, including their sensors and an overview of their sensor processing
  o Robot control architectures, e.g., deliberative vs. reactive control and Braitenberg vehicles
  o World modeling and world models
  o Inherent uncertainty in sensing and in control
  • Configuration space and environmental maps
  • Interpreting uncertain sensor data
  • Localizing and mapping
  • Navigation and control
  • Motion planning
  • Multiple-robot coordination
  <h3>Perception and Computer Vision</h3>
  Computer vision
  o Image acquisition, representation, processing and properties
  o Shape representation, object recognition and segmentation
  o Motion analysis
  • Audio and speech recognition
  • Modularity in recognition
  • Approaches to pattern recognition (cross-reference IS/Advanced Machine Learning)
  o Classification algorithms and measures of classification quality
  o Statistical techniques
  <h2>Networking and Communication (NC)</h2>
  <h3>Introduction</h3>
  Organization of the Internet (Internet Service Providers, Content Providers, etc.)
  • Switching techniques (e.g., circuit, packet)
  • Physical pieces of a network, including hosts, routers, switches, ISPs, wireless, LAN, access point, and
  firewalls
  • Layering principles (encapsulation, multiplexing)
  • Roles of the different layers (application, transport, network, datalink, physical)
  <h3>Networked Applications</h3>
  Naming and address schemes (DNS, IP addresses, Uniform Resource Identifiers, etc.)
  • Distributed applications (client/server, peer-to-peer, cloud, etc.)
  • HTTP as an application layer protocol
  • Multiplexing with TCP and UDP
  • Socket APIs
  <h3>Reliable Data Delivery</h3>
  <h3>Routing and Forwarding</h3>
  <h3>Local Area Networks</h3>
  <h3>Resource Allocation</h3>
  <h3>Mobility</h3>
  <h3>Social Networking</h3>
  <h2>Operating Systems (OS)</h2>
  <h3>Overview of Operating Systems</h3>
  <h3>Operating System Principles</h3>
  <h3>Concurrency</h3>
  <h3>Scheduling and Dispatch</h3>
  <h3>Memory Management</h3>
  <h3>Security and Protection</h3>
  <h3>Virtual Machines</h3>
  <h3>Device Management</h3>
  <h3>File Systems</h3>
  <h3>Real Time and Embedded Systems</h3>
  <h3>Fault Tolerance</h3>
  <h3>System Performance Evaluation</h3>
  <h2>Platform-Based Development (PBD)</h2>
  <h3>Introduction</h3>
  <h3>Web Platforms</h3>
  <h3>Mobile Platforms</h3>
  <h3>Industrial Platforms</h3>
  <h3>Game Platforms</h3>
  <h2>Parallel and Distributed Computing (PD)</h2>
  <h3>Parallelism Fundamentals</h3>
  Multiple simultaneous computations
  • Goals of parallelism (e.g., throughput) versus concurrency (e.g., controlling access to shared resources)
  • Parallelism, communication, and coordination
  o Programming constructs for coordinating multiple simultaneous computations
  o Need for synchronization
  • Programming errors not found in sequential programming
  o Data races (simultaneous read/write or write/write of shared state)
  o Higher-level races (interleavings violating program intention, undesired non-determinism)
  o Lack of liveness/progress (deadlock, starvation)
  <h3>Parallel Decomposition</h3>
  <h3>Communication and Coordination</h3>
  <h3>Parallel Algorithms, Analysis, and Programming</h3>
  <h3>Parallel Architecture</h3>
  <h3>Parallel Performance</h3>
  <h3>Distributed Systems</h3>
  <h3>Cloud Computing</h3>
  <h3>Formal Models and Semantics</h3>
  <h2>Programmign Languages (PL)</h2>
  <h3>Object-Oriented Programming</h3>
  <h3>Functional Programming</h3>
  <h3>Event-Driven and Reactive Programming</h3>
  <h3>Basic Type Systems</h3>
  A type as a set of values together with a set of operations
  o Primitive types (e.g., numbers, Booleans)
  o Compound types built from other types (e.g., records, unions, arrays, lists, functions, references)
  • Association of types to variables, arguments, results, and fields
  • Type safety and errors caused by using values inconsistently given their intended types
  • Goals and limitations of static typing
  o Eliminating some classes of errors without running the program
  o Undecidability means static analysis must conservatively approximate program behavior
  <h3>Program Representation</h3>
  Programs that take (other) programs as input such as interpreters, compilers, type-checkers, documentation
  generators
  • Abstract syntax trees; contrast with concrete syntax
  • Data structures to represent code for execution, translation, or transmission
  <h3>Language Translation and Execution</h3>
  <h3>Syntax Analysis</h3>
  <h3>Compiler Semantic Analysis</h3>
  <h3>Code Generation</h3>
  <h3>Runtime Systems</h3>
  <h3>Static Analysis</h3>
  <h3>Advanced Programming Constructs</h3>
  <h3>Concurrency and Parallelism</h3>
  <h3>Type Systems</h3>
  <h3>Formal Semantics</h3>
  <h3>Language Pragmatics</h3>
  <h3>Logic Programming</h3>
  <h2>Software Development Fundamentals (SDF)</h2>
  <h3>Algorithms and Design</h3>
  <h3>Fundamental Programming Concepts</h3>
  Basic syntax and semantics of a higher-level language
  • Variables and primitive data types (e.g., numbers, characters, Booleans)
  • Expressions and assignments
  • Simple I/O including file I/O
  • Conditional and iterative control structures
  • Functions and parameter passing
  • The concept of recursion
  <h3>Fundamental Data Structures</h3>
  Arrays
  • Records/structs (heterogeneous aggregates)
  • Strings and string processing
  • Abstract data types and their implementation
  o Stacks
  o Queues
  o Priority queues
  o Sets
  o Maps
  • References and aliasing
  • Linked lists
  • Strategies for choosing the appropriate data structure
  <h3>Development Methods</h3>
  <h2>Software Engineering (SE)</h2>
  <h3>Software Processes</h3>
  <h3>Software Project Management</h3>
  <h3>Tools and Environments</h3>
  Software configuration management and version control
  • Release management
  • Requirements analysis and design modeling tools
  • Testing tools including static and dynamic analysis tools
  • Programming environments that automate parts of program construction processes (e.g., automated builds)
  o Continuous integration
  • Tool integration concepts and mechanisms
  <h3>Requirements Engineering</h3>
  <h3>Software Design</h3>
  System design principles: levels of abstraction (architectural design and detailed design), separation of
  concerns, information hiding, coupling and cohesion, re-use of standard structures
  • Design Paradigms such as structured design (top-down functional decomposition), object-oriented analysis
  and design, event driven design, component-level design, data-structured centered, aspect oriented,
  function oriented, service oriented
  • Structural and behavioral models of software designs
  • Design patterns
  <h3>Software Construction</h3>
  <h3>Software Verification and Validation</h3>
  <h3>Software Evolution</h3>
  <h3>Software Reliability</h3>
  <h3>Formal Methods</h3>
  <h2>Systems Fundamentals (SF)</h2>
  <h3>Computational Paradigms</h3>
  Basic building blocks and components of a computer (gates, flip-flops, registers, interconnections;
  Datapath + Control + Memory)
  • Hardware as a computational paradigm: Fundamental logic building blocks; Logic expressions,
  minimization, sum of product forms
  • Application-level sequential processing: single thread
  • Simple application-level parallel processing: request level (web services/client-server/distributed), single
  thread per server, multiple threads with multiple servers
  • Basic concept of pipelining, overlapped processing stages
  • Basic concept of scaling: going faster vs. handling larger problems
  <h3>Cross-Layer Communications</h3>
  <h3>State and State Machines</h3>
  <h3>Parallelism</h3>
  Sequential vs. parallel processing
  • Parallel programming vs. concurrent programming
  • Request parallelism vs. Task parallelism
  • Client-Server/Web Services, Thread (Fork-Join), Pipelining
  • Multicore architectures and hardware support for synchronization
  <h3>Evaluation</h3>
  <h3>Resource Allocation and Scheduling</h3>
  Kinds of resources (e.g., processor share, memory, disk, net bandwidth)
  • Kinds of scheduling (e.g., first-come, priority)
  • Advantages of fair scheduling, preemptive scheduling
  <h3>Proximity</h3>
  Speed of light and computers (one foot per nanosecond vs. one GHz clocks)
  • Latencies in computer systems: memory vs. disk latencies vs. across the network memory
  • Caches and the effects of spatial and temporal locality on performance in processors and systems
  • Caches and cache coherency in databases, operating systems, distributed systems, and computer
  architecture
  • Introduction into the processor memory hierarchy and the formula for average memory access time
  <h3>Virtualization and Isolation</h3>
  <h3>Reliability through Reduncancy</h3>
  <h3>Quantitative Evaluation</h3>
  <h2>Social Issues and Professional Practice (SP)</h2>
  <h3>Social Context</h3>
  Social implications of computing in a networked world (cross-reference HCI/Foundations/social models;
  IAS/Fundamental Concepts/social issues)
  • Impact of social media on individualism, collectivism and culture
  Growth and control of the Internet (cross-reference NC/Introduction/organization of the Internet)
  • Often referred to as the digital divide, differences in access to digital technology resources and its
  resulting
  ramifications for gender, class, ethnicity, geography, and/or underdeveloped countries.
  • Accessibility issues, including legal requirements
  • Context-aware computing (cross-reference HCI/Design for non-mouse interfaces/ ubiquitous and contextaware)
  <h3>Analytical Tools</h3>
  <h3>Professional Ethics</h3>
  <h3>Intellectual Property</h3>
  Philosophical foundations of intellectual property
  • Intellectual property rights (cross-reference IM/Information Storage and Retrieval/intellectual property and
  protection)
  • Intangible digital intellectual property (IDIP)
  • Legal foundations for intellectual property protection
  • Digital rights management
  • Copyrights, patents, trade secrets, trademarks
  • Plagiarism
  <h3>Privacy and Civil Liberties</h3>
  <h3>Professional Communication</h3>
  <h3>Sustainability</h3>
  <h3>History</h3>
  <h3>Economics of Computing</h3>
  <h3>Security Policies, Laws and Computer Crimes</h3>


  <h4>Principal Component Analysis</h4>
  <p>PCA is a dimensionality-reduction method used to reduce the dimensionality of large data sets, by
    transforming
    a
    large set of variables into a smaller one that still contains most of the information in the large set. The
    goal
    is
    to keep as much accuracy as possible while allowing for simplicity.</p>
  <p>The first step of PCA is standardization, which standardizes the range of continuous initial variables so
    that
    each
    one contributes equally to the analysis. If this is not performed, differences in the larger ranges will
    dominate
    over those with small ranges. This can be done mathematically by finding the z-score:</p>
  $$z=\frac{\text{value}-\text{mean}}{\text{standard deviation}}$$
  <p>The next step is covariance matrix computation. This aims to understand how the variables of the input data
    set
    are
    varying from the mean with respect to each other, to see if there is any relationship between them. The
    covariance
    matrix is a \(p\times p\) symmetric matrix (where p is the number of dimensions) that has as entries the
    covariances
    associated with all possible pairs of the initial variables. For example, a data set with 3 dimensions (say x,
    y,
    and z) would have a covariance matrix in this form:</p>
  $$ \left[
  \begin{array}{c}
  Cov(x,x)&Cov(x,y)&Cov(x,z)\\
  Cov(y,x)&Cov(y,y)&Cov(y,z)\\
  Cov(z,x)&Cov(z,y)&Cov(z,z)
  \end{array}
  \right] $$
  <p>Since the covariance of a variable with itself is its variance, the mani diagonal (top left to bottom right)
    is
    just the variances of each initial variable. And since the covariance is commutative (Cov(a,b,)=Cov(b,a)), the
    entries of the covariance matrix are symmetric with respect to the main diagonal.</p>
  <p>The next step is to compute the eigenvectors and eigenvalues of the covariance matrix to identify the
    principal
    components. Principal components are new variables that are constructed as linear combinations or mixtures of
    initial variables, but done in a way such that most of the information is compressed into the first
    components.
    So,
    10-dimensional data still gives you 10 principal components, but tries to put the maximum possible information
    into
    the first component, then the maximum remaining information in the second, and so on. Thus, you can discard
    components with low information and consider remaining components as your new variables. Geometrically
    speaking,
    principal components represent the directions of the data that explain a maximal amount of variance. Simply
    put,
    think of principal components as new axes that provide the best angle to see and evaluate the data, so that
    differences between observations are better visible. They are lines that maximize the variance (average of
    squared
    distances from the projected points to the origin).</p>
  <img src="{{url_for('static',filename='img/computer_science/PCA.gif')}}" style="width: 70%" alt="Builtin.com">
  <p>Now let's go back to eigenvectors and eigenvalues. Every eigenvector has an eigenvalue, and their number is
    equal
    to the number of dimensions of the data. For example, for a 3-dimensional data set, there are 3 variables and
    thus 3
    eigenvectors with 3 corresponding eigenvalues. The eigenvectors of the Covariance matrix are the directions of
    the
    axes where there is the most variance and that we call Principal Components. And eigenvalues are simply the
    coefficients attached to eigenvactors, which give the amount of variance carried in each Principal Component.
  </p>
  <p>Next, we create the feature vector. After we have ranked the principal components in order of significance,
    we
    choose which components we wish to discard, and form the remaining ones a matrix of vectors that we call the
    feature
    vector. This makes it the first step in dimensionality reduction, because we have removed principal
    components.
  </p>
  <p>Finally, we recast the data along the principal components axes. We use the feature vector to reorient the
    data
    from the original axes to the one represented by the principal components. This can be done by multiplying the
    transpose of the original data set by the transpose of the feature vector.</p>
  <h4>k-means clustering</h4>
  <p>k-means clustering is a method of vector quantization, originally from signal processing, that aims to
    partitian n
    observations into k clusters in whcih each observation belongs to the cluster with the nearest mean.</p>
  <h4>Uniform Manifold Approximation and Projection for Dimension Reduction</h4>
  <p>UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension
    reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic
    topology.
    The
    result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive
    with
    t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time
    performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a
    general purpose dimension reduction technique for machine learning.</p>

  <h3>MongoDB-Express-Angular-NodeJS (MEAN) Stack</h3>
  <h4>Angular</h4>
  <p>Angular is a client-side (browser) framework which allows you to build Single-Page-Applications (SPA). It
    creates
    "mobile app"-like websites because there is no refreshing of the page.</p>
  <h4>NodeJS</h4>
  <p>NodeJS is a server-side library. It listens to requests and sends responses.</p>
  <h4>Express</h4>
  <p>Express is a Node framework which simplifies writing server-side code and logic.</p>
  <h4>mongoDB</h4>
  <p>mongoDB is a noSQL database which stores "documents" in collections" instead of "records" in "tables" as in
    SQL. It
    stores application data (users, products, etc.). It is more flexible in terms of structure compared to SQL.
  </p>
  <p>First you need to download and install NodeJS. Then install the Angular CLI. Then create a new Angular
    project
    (ng
    new). 'ng serve' will serve the project.</p>
  <p>Each component needs logic (ts file) and template (html file).</p>
</div>
{% endblock %}